{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge mlxtend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_datareader as pdr\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from bayes_opt import UtilityFunction\n",
    "import warnings\n",
    "import ta\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import numpy as np\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # ปิด SettingWithCopyWarning\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicator\n",
    "# Basic stats\n",
    "# time series\n",
    "# observation\n",
    "# set index , gold , another industrial index ,oil price ,s and p\n",
    "# เงินการท่องเที่ยว\n",
    "# อัตราแลกเปลี่ยน\n",
    "# เงินเฟ้อ\n",
    "# Tourism index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price and Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ERW.BK_Open</th>\n",
       "      <th>ERW.BK_High</th>\n",
       "      <th>ERW.BK_Low</th>\n",
       "      <th>ERW.BK_Close</th>\n",
       "      <th>ERW.BK_Adj Close</th>\n",
       "      <th>ERW.BK_Volume</th>\n",
       "      <th>Gold_Open</th>\n",
       "      <th>Gold_High</th>\n",
       "      <th>Gold_Low</th>\n",
       "      <th>Gold_Close</th>\n",
       "      <th>Gold_Adj Close</th>\n",
       "      <th>Gold_Volume</th>\n",
       "      <th>Oil_Open</th>\n",
       "      <th>Oil_High</th>\n",
       "      <th>Oil_Low</th>\n",
       "      <th>Oil_Close</th>\n",
       "      <th>Oil_Adj Close</th>\n",
       "      <th>Oil_Volume</th>\n",
       "      <th>SET_Open</th>\n",
       "      <th>SET_High</th>\n",
       "      <th>SET_Low</th>\n",
       "      <th>SET_Close</th>\n",
       "      <th>SET_Adj Close</th>\n",
       "      <th>SET_Volume</th>\n",
       "      <th>DJI_Open</th>\n",
       "      <th>DJI_High</th>\n",
       "      <th>DJI_Low</th>\n",
       "      <th>DJI_Close</th>\n",
       "      <th>DJI_Adj Close</th>\n",
       "      <th>DJI_Volume</th>\n",
       "      <th>SP500_Open</th>\n",
       "      <th>SP500_High</th>\n",
       "      <th>SP500_Low</th>\n",
       "      <th>SP500_Close</th>\n",
       "      <th>SP500_Adj Close</th>\n",
       "      <th>SP500_Volume</th>\n",
       "      <th>SET50_Open</th>\n",
       "      <th>SET50_High</th>\n",
       "      <th>SET50_Low</th>\n",
       "      <th>SET50_Close</th>\n",
       "      <th>SET50_Adj Close</th>\n",
       "      <th>SET50_Volume</th>\n",
       "      <th>SET100_Open</th>\n",
       "      <th>SET100_High</th>\n",
       "      <th>SET100_Low</th>\n",
       "      <th>SET100_Close</th>\n",
       "      <th>SET100_Adj Close</th>\n",
       "      <th>SET100_Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-11-09 10:00:00</th>\n",
       "      <td>5.20</td>\n",
       "      <td>5.20</td>\n",
       "      <td>5.15</td>\n",
       "      <td>5.15</td>\n",
       "      <td>5.15</td>\n",
       "      <td>17900.0</td>\n",
       "      <td>1963.300049</td>\n",
       "      <td>1966.400024</td>\n",
       "      <td>1960.400024</td>\n",
       "      <td>1965.000000</td>\n",
       "      <td>1965.000000</td>\n",
       "      <td>6778.0</td>\n",
       "      <td>76.260002</td>\n",
       "      <td>76.629997</td>\n",
       "      <td>76.139999</td>\n",
       "      <td>76.620003</td>\n",
       "      <td>76.620003</td>\n",
       "      <td>9060.0</td>\n",
       "      <td>1408.530029</td>\n",
       "      <td>1408.530029</td>\n",
       "      <td>1400.790039</td>\n",
       "      <td>1402.660034</td>\n",
       "      <td>1402.660034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34105.480469</td>\n",
       "      <td>34129.738281</td>\n",
       "      <td>34020.199219</td>\n",
       "      <td>34038.589844</td>\n",
       "      <td>34038.589844</td>\n",
       "      <td>12683606.0</td>\n",
       "      <td>4388.319824</td>\n",
       "      <td>4391.080078</td>\n",
       "      <td>4374.479980</td>\n",
       "      <td>4378.359863</td>\n",
       "      <td>4378.359863</td>\n",
       "      <td>87995219.0</td>\n",
       "      <td>870.520020</td>\n",
       "      <td>870.520020</td>\n",
       "      <td>865.000000</td>\n",
       "      <td>866.729980</td>\n",
       "      <td>866.729980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1929.150024</td>\n",
       "      <td>1929.150024</td>\n",
       "      <td>1917.310059</td>\n",
       "      <td>1920.780029</td>\n",
       "      <td>1920.780029</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-09 10:15:00</th>\n",
       "      <td>5.15</td>\n",
       "      <td>5.20</td>\n",
       "      <td>5.15</td>\n",
       "      <td>5.15</td>\n",
       "      <td>5.15</td>\n",
       "      <td>907200.0</td>\n",
       "      <td>1965.000000</td>\n",
       "      <td>1966.199951</td>\n",
       "      <td>1962.800049</td>\n",
       "      <td>1963.000000</td>\n",
       "      <td>1963.000000</td>\n",
       "      <td>3746.0</td>\n",
       "      <td>76.620003</td>\n",
       "      <td>76.919998</td>\n",
       "      <td>76.470001</td>\n",
       "      <td>76.870003</td>\n",
       "      <td>76.870003</td>\n",
       "      <td>8944.0</td>\n",
       "      <td>1402.380005</td>\n",
       "      <td>1404.319946</td>\n",
       "      <td>1400.829956</td>\n",
       "      <td>1401.630005</td>\n",
       "      <td>1401.630005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34038.761719</td>\n",
       "      <td>34041.511719</td>\n",
       "      <td>33999.289062</td>\n",
       "      <td>34012.191406</td>\n",
       "      <td>34012.191406</td>\n",
       "      <td>9524776.0</td>\n",
       "      <td>4378.379883</td>\n",
       "      <td>4379.290039</td>\n",
       "      <td>4373.470215</td>\n",
       "      <td>4374.209961</td>\n",
       "      <td>4374.209961</td>\n",
       "      <td>71084667.0</td>\n",
       "      <td>866.489990</td>\n",
       "      <td>868.020020</td>\n",
       "      <td>865.530029</td>\n",
       "      <td>866.200012</td>\n",
       "      <td>866.200012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1920.180054</td>\n",
       "      <td>1923.300049</td>\n",
       "      <td>1917.979980</td>\n",
       "      <td>1919.280029</td>\n",
       "      <td>1919.280029</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-09 10:30:00</th>\n",
       "      <td>5.10</td>\n",
       "      <td>5.15</td>\n",
       "      <td>5.10</td>\n",
       "      <td>5.10</td>\n",
       "      <td>5.10</td>\n",
       "      <td>80800.0</td>\n",
       "      <td>1963.000000</td>\n",
       "      <td>1968.800049</td>\n",
       "      <td>1962.400024</td>\n",
       "      <td>1968.699951</td>\n",
       "      <td>1968.699951</td>\n",
       "      <td>6664.0</td>\n",
       "      <td>76.879997</td>\n",
       "      <td>77.070000</td>\n",
       "      <td>76.860001</td>\n",
       "      <td>76.959999</td>\n",
       "      <td>76.959999</td>\n",
       "      <td>11158.0</td>\n",
       "      <td>1401.520020</td>\n",
       "      <td>1401.880005</td>\n",
       "      <td>1394.569946</td>\n",
       "      <td>1394.569946</td>\n",
       "      <td>1394.569946</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34011.988281</td>\n",
       "      <td>34025.078125</td>\n",
       "      <td>33983.250000</td>\n",
       "      <td>34011.839844</td>\n",
       "      <td>34011.839844</td>\n",
       "      <td>9093214.0</td>\n",
       "      <td>4374.330078</td>\n",
       "      <td>4376.279785</td>\n",
       "      <td>4370.560059</td>\n",
       "      <td>4375.959961</td>\n",
       "      <td>4375.959961</td>\n",
       "      <td>71871797.0</td>\n",
       "      <td>866.049988</td>\n",
       "      <td>866.390015</td>\n",
       "      <td>861.750000</td>\n",
       "      <td>862.030029</td>\n",
       "      <td>862.030029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1918.989990</td>\n",
       "      <td>1919.530029</td>\n",
       "      <td>1909.089966</td>\n",
       "      <td>1909.479980</td>\n",
       "      <td>1909.479980</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-09 10:45:00</th>\n",
       "      <td>5.10</td>\n",
       "      <td>5.15</td>\n",
       "      <td>5.10</td>\n",
       "      <td>5.10</td>\n",
       "      <td>5.10</td>\n",
       "      <td>327800.0</td>\n",
       "      <td>1968.699951</td>\n",
       "      <td>1970.400024</td>\n",
       "      <td>1966.800049</td>\n",
       "      <td>1968.199951</td>\n",
       "      <td>1968.199951</td>\n",
       "      <td>5895.0</td>\n",
       "      <td>76.949997</td>\n",
       "      <td>77.160004</td>\n",
       "      <td>76.629997</td>\n",
       "      <td>76.800003</td>\n",
       "      <td>76.800003</td>\n",
       "      <td>11649.0</td>\n",
       "      <td>1394.609985</td>\n",
       "      <td>1396.640015</td>\n",
       "      <td>1394.030029</td>\n",
       "      <td>1396.030029</td>\n",
       "      <td>1396.030029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34011.441406</td>\n",
       "      <td>34062.410156</td>\n",
       "      <td>34003.101562</td>\n",
       "      <td>34056.828125</td>\n",
       "      <td>34056.828125</td>\n",
       "      <td>9787072.0</td>\n",
       "      <td>4375.950195</td>\n",
       "      <td>4379.569824</td>\n",
       "      <td>4373.279785</td>\n",
       "      <td>4379.569824</td>\n",
       "      <td>4379.569824</td>\n",
       "      <td>67262530.0</td>\n",
       "      <td>861.830017</td>\n",
       "      <td>863.969971</td>\n",
       "      <td>861.640015</td>\n",
       "      <td>863.700012</td>\n",
       "      <td>863.700012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1909.130005</td>\n",
       "      <td>1913.300049</td>\n",
       "      <td>1908.670044</td>\n",
       "      <td>1912.869995</td>\n",
       "      <td>1912.869995</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-09 11:00:00</th>\n",
       "      <td>5.10</td>\n",
       "      <td>5.10</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.05</td>\n",
       "      <td>1715700.0</td>\n",
       "      <td>1968.099976</td>\n",
       "      <td>1968.599976</td>\n",
       "      <td>1965.900024</td>\n",
       "      <td>1966.400024</td>\n",
       "      <td>1966.400024</td>\n",
       "      <td>4926.0</td>\n",
       "      <td>76.790001</td>\n",
       "      <td>76.800003</td>\n",
       "      <td>76.099998</td>\n",
       "      <td>76.139999</td>\n",
       "      <td>76.139999</td>\n",
       "      <td>20637.0</td>\n",
       "      <td>1397.020020</td>\n",
       "      <td>1397.020020</td>\n",
       "      <td>1392.520020</td>\n",
       "      <td>1394.689941</td>\n",
       "      <td>1394.689941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34054.019531</td>\n",
       "      <td>34121.250000</td>\n",
       "      <td>34054.019531</td>\n",
       "      <td>34101.031250</td>\n",
       "      <td>34101.031250</td>\n",
       "      <td>8539581.0</td>\n",
       "      <td>4379.370117</td>\n",
       "      <td>4387.060059</td>\n",
       "      <td>4379.330078</td>\n",
       "      <td>4385.350098</td>\n",
       "      <td>4385.350098</td>\n",
       "      <td>60593421.0</td>\n",
       "      <td>863.650024</td>\n",
       "      <td>864.479980</td>\n",
       "      <td>860.840027</td>\n",
       "      <td>862.809998</td>\n",
       "      <td>862.809998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1914.229980</td>\n",
       "      <td>1914.229980</td>\n",
       "      <td>1906.810059</td>\n",
       "      <td>1910.699951</td>\n",
       "      <td>1910.699951</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ERW.BK_Open  ERW.BK_High  ERW.BK_Low  ERW.BK_Close  \\\n",
       "Datetime                                                                  \n",
       "2023-11-09 10:00:00         5.20         5.20        5.15          5.15   \n",
       "2023-11-09 10:15:00         5.15         5.20        5.15          5.15   \n",
       "2023-11-09 10:30:00         5.10         5.15        5.10          5.10   \n",
       "2023-11-09 10:45:00         5.10         5.15        5.10          5.10   \n",
       "2023-11-09 11:00:00         5.10         5.10        5.05          5.05   \n",
       "\n",
       "                     ERW.BK_Adj Close  ERW.BK_Volume    Gold_Open  \\\n",
       "Datetime                                                            \n",
       "2023-11-09 10:00:00              5.15        17900.0  1963.300049   \n",
       "2023-11-09 10:15:00              5.15       907200.0  1965.000000   \n",
       "2023-11-09 10:30:00              5.10        80800.0  1963.000000   \n",
       "2023-11-09 10:45:00              5.10       327800.0  1968.699951   \n",
       "2023-11-09 11:00:00              5.05      1715700.0  1968.099976   \n",
       "\n",
       "                       Gold_High     Gold_Low   Gold_Close  Gold_Adj Close  \\\n",
       "Datetime                                                                     \n",
       "2023-11-09 10:00:00  1966.400024  1960.400024  1965.000000     1965.000000   \n",
       "2023-11-09 10:15:00  1966.199951  1962.800049  1963.000000     1963.000000   \n",
       "2023-11-09 10:30:00  1968.800049  1962.400024  1968.699951     1968.699951   \n",
       "2023-11-09 10:45:00  1970.400024  1966.800049  1968.199951     1968.199951   \n",
       "2023-11-09 11:00:00  1968.599976  1965.900024  1966.400024     1966.400024   \n",
       "\n",
       "                     Gold_Volume   Oil_Open   Oil_High    Oil_Low  Oil_Close  \\\n",
       "Datetime                                                                       \n",
       "2023-11-09 10:00:00       6778.0  76.260002  76.629997  76.139999  76.620003   \n",
       "2023-11-09 10:15:00       3746.0  76.620003  76.919998  76.470001  76.870003   \n",
       "2023-11-09 10:30:00       6664.0  76.879997  77.070000  76.860001  76.959999   \n",
       "2023-11-09 10:45:00       5895.0  76.949997  77.160004  76.629997  76.800003   \n",
       "2023-11-09 11:00:00       4926.0  76.790001  76.800003  76.099998  76.139999   \n",
       "\n",
       "                     Oil_Adj Close  Oil_Volume     SET_Open     SET_High  \\\n",
       "Datetime                                                                   \n",
       "2023-11-09 10:00:00      76.620003      9060.0  1408.530029  1408.530029   \n",
       "2023-11-09 10:15:00      76.870003      8944.0  1402.380005  1404.319946   \n",
       "2023-11-09 10:30:00      76.959999     11158.0  1401.520020  1401.880005   \n",
       "2023-11-09 10:45:00      76.800003     11649.0  1394.609985  1396.640015   \n",
       "2023-11-09 11:00:00      76.139999     20637.0  1397.020020  1397.020020   \n",
       "\n",
       "                         SET_Low    SET_Close  SET_Adj Close  SET_Volume  \\\n",
       "Datetime                                                                   \n",
       "2023-11-09 10:00:00  1400.790039  1402.660034    1402.660034         0.0   \n",
       "2023-11-09 10:15:00  1400.829956  1401.630005    1401.630005         0.0   \n",
       "2023-11-09 10:30:00  1394.569946  1394.569946    1394.569946         0.0   \n",
       "2023-11-09 10:45:00  1394.030029  1396.030029    1396.030029         0.0   \n",
       "2023-11-09 11:00:00  1392.520020  1394.689941    1394.689941         0.0   \n",
       "\n",
       "                         DJI_Open      DJI_High       DJI_Low     DJI_Close  \\\n",
       "Datetime                                                                      \n",
       "2023-11-09 10:00:00  34105.480469  34129.738281  34020.199219  34038.589844   \n",
       "2023-11-09 10:15:00  34038.761719  34041.511719  33999.289062  34012.191406   \n",
       "2023-11-09 10:30:00  34011.988281  34025.078125  33983.250000  34011.839844   \n",
       "2023-11-09 10:45:00  34011.441406  34062.410156  34003.101562  34056.828125   \n",
       "2023-11-09 11:00:00  34054.019531  34121.250000  34054.019531  34101.031250   \n",
       "\n",
       "                     DJI_Adj Close  DJI_Volume   SP500_Open   SP500_High  \\\n",
       "Datetime                                                                   \n",
       "2023-11-09 10:00:00   34038.589844  12683606.0  4388.319824  4391.080078   \n",
       "2023-11-09 10:15:00   34012.191406   9524776.0  4378.379883  4379.290039   \n",
       "2023-11-09 10:30:00   34011.839844   9093214.0  4374.330078  4376.279785   \n",
       "2023-11-09 10:45:00   34056.828125   9787072.0  4375.950195  4379.569824   \n",
       "2023-11-09 11:00:00   34101.031250   8539581.0  4379.370117  4387.060059   \n",
       "\n",
       "                       SP500_Low  SP500_Close  SP500_Adj Close  SP500_Volume  \\\n",
       "Datetime                                                                       \n",
       "2023-11-09 10:00:00  4374.479980  4378.359863      4378.359863    87995219.0   \n",
       "2023-11-09 10:15:00  4373.470215  4374.209961      4374.209961    71084667.0   \n",
       "2023-11-09 10:30:00  4370.560059  4375.959961      4375.959961    71871797.0   \n",
       "2023-11-09 10:45:00  4373.279785  4379.569824      4379.569824    67262530.0   \n",
       "2023-11-09 11:00:00  4379.330078  4385.350098      4385.350098    60593421.0   \n",
       "\n",
       "                     SET50_Open  SET50_High   SET50_Low  SET50_Close  \\\n",
       "Datetime                                                               \n",
       "2023-11-09 10:00:00  870.520020  870.520020  865.000000   866.729980   \n",
       "2023-11-09 10:15:00  866.489990  868.020020  865.530029   866.200012   \n",
       "2023-11-09 10:30:00  866.049988  866.390015  861.750000   862.030029   \n",
       "2023-11-09 10:45:00  861.830017  863.969971  861.640015   863.700012   \n",
       "2023-11-09 11:00:00  863.650024  864.479980  860.840027   862.809998   \n",
       "\n",
       "                     SET50_Adj Close  SET50_Volume  SET100_Open  SET100_High  \\\n",
       "Datetime                                                                       \n",
       "2023-11-09 10:00:00       866.729980           0.0  1929.150024  1929.150024   \n",
       "2023-11-09 10:15:00       866.200012           0.0  1920.180054  1923.300049   \n",
       "2023-11-09 10:30:00       862.030029           0.0  1918.989990  1919.530029   \n",
       "2023-11-09 10:45:00       863.700012           0.0  1909.130005  1913.300049   \n",
       "2023-11-09 11:00:00       862.809998           0.0  1914.229980  1914.229980   \n",
       "\n",
       "                      SET100_Low  SET100_Close  SET100_Adj Close  \\\n",
       "Datetime                                                           \n",
       "2023-11-09 10:00:00  1917.310059   1920.780029       1920.780029   \n",
       "2023-11-09 10:15:00  1917.979980   1919.280029       1919.280029   \n",
       "2023-11-09 10:30:00  1909.089966   1909.479980       1909.479980   \n",
       "2023-11-09 10:45:00  1908.670044   1912.869995       1912.869995   \n",
       "2023-11-09 11:00:00  1906.810059   1910.699951       1910.699951   \n",
       "\n",
       "                     SET100_Volume  \n",
       "Datetime                            \n",
       "2023-11-09 10:00:00            0.0  \n",
       "2023-11-09 10:15:00            0.0  \n",
       "2023-11-09 10:30:00            0.0  \n",
       "2023-11-09 10:45:00            0.0  \n",
       "2023-11-09 11:00:00            0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import timedelta, datetime\n",
    "import yfinance as yf  # Don't forget to import yfinance\n",
    "\n",
    "# Stock Name\n",
    "stock_name = 'ERW.BK'\n",
    "\n",
    "# ตั้งค่าวันที่\n",
    "cur_date = datetime.now().strftime('%Y-%m-%d')\n",
    "date_60_days_ago = (datetime.now() - timedelta(days=59)).strftime('%Y-%m-%d')\n",
    "\n",
    "# ดึงข้อมูล ERW.BK\n",
    "df = yf.download(stock_name, start=date_60_days_ago, end=cur_date, interval='15m')\n",
    "\n",
    "# ดึงข้อมูลสินค้าโภคภัณฑ์และดัชนีตลาดหลักทรัพย์\n",
    "# ตัวอย่าง: ทองคำ (Gold), น้ำมัน (Crude Oil), SET Index, Dow Jones, S&P 500\n",
    "# โปรดแทนที่ 'SYMBOL' ด้วยตัวย่อของแต่ละสินค้าหรือดัชนี\n",
    "df_gold = yf.download('GC=F', start=date_60_days_ago, end=cur_date, interval='15m')\n",
    "df_oil = yf.download('CL=F', start=date_60_days_ago, end=cur_date, interval='15m')\n",
    "df_set = yf.download('^SET.BK', start=date_60_days_ago, end=cur_date, interval='15m')\n",
    "df_dji = yf.download('^DJI', start=date_60_days_ago, end=cur_date, interval='15m')\n",
    "df_sp500 = yf.download('^GSPC', start=date_60_days_ago, end=cur_date, interval='15m')\n",
    "\n",
    "# เพิ่มคำนำหน้าในแต่ละ feature ของ DataFrame\n",
    "df = df.add_prefix(f'{stock_name}_')\n",
    "df_gold = df_gold.add_prefix('Gold_')\n",
    "df_oil = df_oil.add_prefix('Oil_')\n",
    "df_set = df_set.add_prefix('SET_')\n",
    "df_dji = df_dji.add_prefix('DJI_')\n",
    "df_sp500 = df_sp500.add_prefix('SP500_')\n",
    "\n",
    "# ดึงข้อมูลดัชนี SET50\n",
    "df_set50 = yf.download('^SET50.BK', start=date_60_days_ago, end=cur_date, interval='15m')\n",
    "\n",
    "# ดึงข้อมูลดัชนี SET100\n",
    "df_set100 = yf.download('^SET100.BK', start=date_60_days_ago, end=cur_date, interval='15m')\n",
    "\n",
    "# เพิ่มคำนำหน้าในแต่ละ feature ของ DataFrame ของดัชนี SET50 และ SET100\n",
    "df_set50 = df_set50.add_prefix('SET50_')\n",
    "df_set100 = df_set100.add_prefix('SET100_')\n",
    "\n",
    "# รวม DataFrame ทั้งหมดเข้าด้วยกัน โดยใช้วันที่เป็น index\n",
    "df_combined = pd.concat([df, df_gold, df_oil, df_set, df_dji, df_sp500, df_set50, df_set100], axis=1)\n",
    "\n",
    "# เลือกเฉพาะแถวที่อยู่ใน df_erw\n",
    "df_selected = df_combined[df_combined[f'{stock_name}_Close'].notnull()]\n",
    "df = df_selected\n",
    "\n",
    "# แสดงผล DataFrame ที่เลือก\n",
    "display(df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Data Daily\n",
    "df.to_csv(stock_name + cur_date +'_'+ date_60_days_ago+'.csv')\n",
    "\n",
    "# last_dt_prefer = datetime.date(2024, 1, 6) # Adjust this\n",
    "# last_dt_prefer_str = last_dt_prefer.strftime('%Y-%m-%d')\n",
    "# date_60_days_ago_prefer = (last_dt_prefer - timedelta(days=59)).strftime('%Y-%m-%d')\n",
    "# df = pd.read_csv(stock_name + last_dt_prefer_str +'_'+ date_60_days_ago_prefer+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ERW.BK_Open</th>\n",
       "      <th>ERW.BK_High</th>\n",
       "      <th>ERW.BK_Low</th>\n",
       "      <th>ERW.BK_Close</th>\n",
       "      <th>ERW.BK_Adj Close</th>\n",
       "      <th>ERW.BK_Volume</th>\n",
       "      <th>Gold_Open</th>\n",
       "      <th>Gold_High</th>\n",
       "      <th>Gold_Low</th>\n",
       "      <th>Gold_Close</th>\n",
       "      <th>Gold_Adj Close</th>\n",
       "      <th>Gold_Volume</th>\n",
       "      <th>Oil_Open</th>\n",
       "      <th>Oil_High</th>\n",
       "      <th>Oil_Low</th>\n",
       "      <th>Oil_Close</th>\n",
       "      <th>Oil_Adj Close</th>\n",
       "      <th>Oil_Volume</th>\n",
       "      <th>SET_Open</th>\n",
       "      <th>SET_High</th>\n",
       "      <th>SET_Low</th>\n",
       "      <th>SET_Close</th>\n",
       "      <th>SET_Adj Close</th>\n",
       "      <th>SET_Volume</th>\n",
       "      <th>DJI_Open</th>\n",
       "      <th>DJI_High</th>\n",
       "      <th>DJI_Low</th>\n",
       "      <th>DJI_Close</th>\n",
       "      <th>DJI_Adj Close</th>\n",
       "      <th>DJI_Volume</th>\n",
       "      <th>SP500_Open</th>\n",
       "      <th>SP500_High</th>\n",
       "      <th>SP500_Low</th>\n",
       "      <th>SP500_Close</th>\n",
       "      <th>SP500_Adj Close</th>\n",
       "      <th>SP500_Volume</th>\n",
       "      <th>SET50_Open</th>\n",
       "      <th>SET50_High</th>\n",
       "      <th>SET50_Low</th>\n",
       "      <th>SET50_Close</th>\n",
       "      <th>SET50_Adj Close</th>\n",
       "      <th>SET50_Volume</th>\n",
       "      <th>SET100_Open</th>\n",
       "      <th>SET100_High</th>\n",
       "      <th>SET100_Low</th>\n",
       "      <th>SET100_Close</th>\n",
       "      <th>SET100_Adj Close</th>\n",
       "      <th>SET100_Volume</th>\n",
       "      <th>SMA_5</th>\n",
       "      <th>EMA_5</th>\n",
       "      <th>Stoch_%K_5</th>\n",
       "      <th>Stoch_%D_5</th>\n",
       "      <th>RSI_5</th>\n",
       "      <th>WPR_5</th>\n",
       "      <th>ATR_5</th>\n",
       "      <th>CCI_5</th>\n",
       "      <th>Bollinger_mavg_5</th>\n",
       "      <th>Bollinger_hband_5</th>\n",
       "      <th>Bollinger_lband_5</th>\n",
       "      <th>SMA_10</th>\n",
       "      <th>EMA_10</th>\n",
       "      <th>Stoch_%K_10</th>\n",
       "      <th>Stoch_%D_10</th>\n",
       "      <th>RSI_10</th>\n",
       "      <th>WPR_10</th>\n",
       "      <th>ATR_10</th>\n",
       "      <th>CCI_10</th>\n",
       "      <th>Bollinger_mavg_10</th>\n",
       "      <th>Bollinger_hband_10</th>\n",
       "      <th>Bollinger_lband_10</th>\n",
       "      <th>SMA_15</th>\n",
       "      <th>EMA_15</th>\n",
       "      <th>Stoch_%K_15</th>\n",
       "      <th>Stoch_%D_15</th>\n",
       "      <th>RSI_15</th>\n",
       "      <th>WPR_15</th>\n",
       "      <th>ATR_15</th>\n",
       "      <th>CCI_15</th>\n",
       "      <th>Bollinger_mavg_15</th>\n",
       "      <th>Bollinger_hband_15</th>\n",
       "      <th>Bollinger_lband_15</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Stoch_%K_20</th>\n",
       "      <th>Stoch_%D_20</th>\n",
       "      <th>RSI_20</th>\n",
       "      <th>WPR_20</th>\n",
       "      <th>ATR_20</th>\n",
       "      <th>CCI_20</th>\n",
       "      <th>Bollinger_mavg_20</th>\n",
       "      <th>Bollinger_hband_20</th>\n",
       "      <th>Bollinger_lband_20</th>\n",
       "      <th>SMA_25</th>\n",
       "      <th>EMA_25</th>\n",
       "      <th>Stoch_%K_25</th>\n",
       "      <th>Stoch_%D_25</th>\n",
       "      <th>RSI_25</th>\n",
       "      <th>WPR_25</th>\n",
       "      <th>ATR_25</th>\n",
       "      <th>CCI_25</th>\n",
       "      <th>Bollinger_mavg_25</th>\n",
       "      <th>Bollinger_hband_25</th>\n",
       "      <th>Bollinger_lband_25</th>\n",
       "      <th>MACD</th>\n",
       "      <th>MACD_signal</th>\n",
       "      <th>MACD_diff</th>\n",
       "      <th>Momentum</th>\n",
       "      <th>ROC</th>\n",
       "      <th>OBV</th>\n",
       "      <th>Close_pct_change</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>day_of_week</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-11-10 15:15:00</th>\n",
       "      <td>5.05</td>\n",
       "      <td>5.10</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.10</td>\n",
       "      <td>5.10</td>\n",
       "      <td>1611000.0</td>\n",
       "      <td>1940.199951</td>\n",
       "      <td>1940.800049</td>\n",
       "      <td>1939.699951</td>\n",
       "      <td>1939.900024</td>\n",
       "      <td>1939.900024</td>\n",
       "      <td>614.0</td>\n",
       "      <td>77.279999</td>\n",
       "      <td>77.339996</td>\n",
       "      <td>77.250000</td>\n",
       "      <td>77.269997</td>\n",
       "      <td>77.269997</td>\n",
       "      <td>1152.0</td>\n",
       "      <td>1398.459961</td>\n",
       "      <td>1399.099976</td>\n",
       "      <td>1396.969971</td>\n",
       "      <td>1397.910034</td>\n",
       "      <td>1397.910034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34220.378906</td>\n",
       "      <td>34267.671875</td>\n",
       "      <td>34204.640625</td>\n",
       "      <td>34267.671875</td>\n",
       "      <td>34267.671875</td>\n",
       "      <td>9220593.0</td>\n",
       "      <td>4407.600098</td>\n",
       "      <td>4412.910156</td>\n",
       "      <td>4405.569824</td>\n",
       "      <td>4412.649902</td>\n",
       "      <td>4412.649902</td>\n",
       "      <td>68946000.0</td>\n",
       "      <td>865.340027</td>\n",
       "      <td>866.260010</td>\n",
       "      <td>864.510010</td>\n",
       "      <td>865.010010</td>\n",
       "      <td>865.010010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1915.579956</td>\n",
       "      <td>1916.780029</td>\n",
       "      <td>1913.260010</td>\n",
       "      <td>1914.729980</td>\n",
       "      <td>1914.729980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.09</td>\n",
       "      <td>5.090739</td>\n",
       "      <td>66.666561</td>\n",
       "      <td>66.666561</td>\n",
       "      <td>59.187542</td>\n",
       "      <td>-33.333439</td>\n",
       "      <td>0.059311</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>5.09</td>\n",
       "      <td>5.187980</td>\n",
       "      <td>4.992020</td>\n",
       "      <td>5.065</td>\n",
       "      <td>5.074884</td>\n",
       "      <td>70.588136</td>\n",
       "      <td>70.588136</td>\n",
       "      <td>55.707271</td>\n",
       "      <td>-29.411864</td>\n",
       "      <td>0.055659</td>\n",
       "      <td>4.772068e+01</td>\n",
       "      <td>5.065</td>\n",
       "      <td>5.15500</td>\n",
       "      <td>4.97500</td>\n",
       "      <td>5.046667</td>\n",
       "      <td>5.068554</td>\n",
       "      <td>70.588136</td>\n",
       "      <td>70.588136</td>\n",
       "      <td>53.463683</td>\n",
       "      <td>-29.411864</td>\n",
       "      <td>0.054533</td>\n",
       "      <td>81.800369</td>\n",
       "      <td>5.046667</td>\n",
       "      <td>5.139523</td>\n",
       "      <td>4.953811</td>\n",
       "      <td>5.0550</td>\n",
       "      <td>5.067603</td>\n",
       "      <td>70.588136</td>\n",
       "      <td>70.588136</td>\n",
       "      <td>52.150059</td>\n",
       "      <td>-29.411864</td>\n",
       "      <td>0.054404</td>\n",
       "      <td>62.164963</td>\n",
       "      <td>5.0550</td>\n",
       "      <td>5.143882</td>\n",
       "      <td>4.966118</td>\n",
       "      <td>5.058</td>\n",
       "      <td>5.069267</td>\n",
       "      <td>70.588136</td>\n",
       "      <td>70.588136</td>\n",
       "      <td>51.307980</td>\n",
       "      <td>-29.411864</td>\n",
       "      <td>0.054561</td>\n",
       "      <td>55.414885</td>\n",
       "      <td>5.058</td>\n",
       "      <td>5.141331</td>\n",
       "      <td>4.974669</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>-0.010398</td>\n",
       "      <td>0.012072</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>10774104.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-10 15:30:00</th>\n",
       "      <td>5.10</td>\n",
       "      <td>5.10</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.05</td>\n",
       "      <td>53800.0</td>\n",
       "      <td>1940.000000</td>\n",
       "      <td>1940.000000</td>\n",
       "      <td>1939.500000</td>\n",
       "      <td>1939.800049</td>\n",
       "      <td>1939.800049</td>\n",
       "      <td>960.0</td>\n",
       "      <td>77.269997</td>\n",
       "      <td>77.300003</td>\n",
       "      <td>77.260002</td>\n",
       "      <td>77.290001</td>\n",
       "      <td>77.290001</td>\n",
       "      <td>2039.0</td>\n",
       "      <td>1397.670044</td>\n",
       "      <td>1399.439941</td>\n",
       "      <td>1396.930054</td>\n",
       "      <td>1398.689941</td>\n",
       "      <td>1398.689941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34268.128906</td>\n",
       "      <td>34290.019531</td>\n",
       "      <td>34261.578125</td>\n",
       "      <td>34283.429688</td>\n",
       "      <td>34283.429688</td>\n",
       "      <td>10761736.0</td>\n",
       "      <td>4412.700195</td>\n",
       "      <td>4414.939941</td>\n",
       "      <td>4411.359863</td>\n",
       "      <td>4414.689941</td>\n",
       "      <td>4414.689941</td>\n",
       "      <td>90390000.0</td>\n",
       "      <td>865.010010</td>\n",
       "      <td>866.320007</td>\n",
       "      <td>864.309998</td>\n",
       "      <td>866.320007</td>\n",
       "      <td>866.320007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1914.229980</td>\n",
       "      <td>1917.280029</td>\n",
       "      <td>1913.359985</td>\n",
       "      <td>1916.459961</td>\n",
       "      <td>1916.459961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.10</td>\n",
       "      <td>5.077159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.444374</td>\n",
       "      <td>42.199251</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.057449</td>\n",
       "      <td>-95.237852</td>\n",
       "      <td>5.10</td>\n",
       "      <td>5.163245</td>\n",
       "      <td>5.036755</td>\n",
       "      <td>5.070</td>\n",
       "      <td>5.070360</td>\n",
       "      <td>33.333439</td>\n",
       "      <td>58.169904</td>\n",
       "      <td>47.381537</td>\n",
       "      <td>-66.666561</td>\n",
       "      <td>0.055093</td>\n",
       "      <td>2.220451e-12</td>\n",
       "      <td>5.070</td>\n",
       "      <td>5.15000</td>\n",
       "      <td>4.99000</td>\n",
       "      <td>5.050000</td>\n",
       "      <td>5.066235</td>\n",
       "      <td>41.176553</td>\n",
       "      <td>60.784275</td>\n",
       "      <td>47.670444</td>\n",
       "      <td>-58.823447</td>\n",
       "      <td>0.054231</td>\n",
       "      <td>39.225430</td>\n",
       "      <td>5.050000</td>\n",
       "      <td>5.139443</td>\n",
       "      <td>4.960557</td>\n",
       "      <td>5.0525</td>\n",
       "      <td>5.065926</td>\n",
       "      <td>41.176553</td>\n",
       "      <td>60.784275</td>\n",
       "      <td>47.465935</td>\n",
       "      <td>-58.823447</td>\n",
       "      <td>0.054184</td>\n",
       "      <td>29.081599</td>\n",
       "      <td>5.0525</td>\n",
       "      <td>5.138958</td>\n",
       "      <td>4.966042</td>\n",
       "      <td>5.058</td>\n",
       "      <td>5.067785</td>\n",
       "      <td>41.176553</td>\n",
       "      <td>60.784275</td>\n",
       "      <td>47.228213</td>\n",
       "      <td>-58.823447</td>\n",
       "      <td>0.054379</td>\n",
       "      <td>17.768995</td>\n",
       "      <td>5.058</td>\n",
       "      <td>5.141331</td>\n",
       "      <td>4.974669</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>-0.008350</td>\n",
       "      <td>0.008190</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.019417</td>\n",
       "      <td>10720304.0</td>\n",
       "      <td>-0.009804</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-10 15:45:00</th>\n",
       "      <td>5.05</td>\n",
       "      <td>5.10</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.05</td>\n",
       "      <td>457201.0</td>\n",
       "      <td>1939.900024</td>\n",
       "      <td>1940.300049</td>\n",
       "      <td>1939.500000</td>\n",
       "      <td>1940.199951</td>\n",
       "      <td>1940.199951</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>77.290001</td>\n",
       "      <td>77.339996</td>\n",
       "      <td>77.220001</td>\n",
       "      <td>77.250000</td>\n",
       "      <td>77.250000</td>\n",
       "      <td>1239.0</td>\n",
       "      <td>1398.250000</td>\n",
       "      <td>1399.500000</td>\n",
       "      <td>1393.390015</td>\n",
       "      <td>1393.390015</td>\n",
       "      <td>1393.390015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34283.671875</td>\n",
       "      <td>34310.359375</td>\n",
       "      <td>34271.988281</td>\n",
       "      <td>34286.519531</td>\n",
       "      <td>34286.519531</td>\n",
       "      <td>33535611.0</td>\n",
       "      <td>4414.729980</td>\n",
       "      <td>4418.029785</td>\n",
       "      <td>4413.629883</td>\n",
       "      <td>4415.890137</td>\n",
       "      <td>4415.890137</td>\n",
       "      <td>257745000.0</td>\n",
       "      <td>865.729980</td>\n",
       "      <td>866.919983</td>\n",
       "      <td>862.599976</td>\n",
       "      <td>862.770020</td>\n",
       "      <td>862.770020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1915.239990</td>\n",
       "      <td>1918.099976</td>\n",
       "      <td>1908.359985</td>\n",
       "      <td>1908.660034</td>\n",
       "      <td>1908.660034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.08</td>\n",
       "      <td>5.068106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.222187</td>\n",
       "      <td>42.199251</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.055959</td>\n",
       "      <td>-83.333174</td>\n",
       "      <td>5.08</td>\n",
       "      <td>5.128990</td>\n",
       "      <td>5.031011</td>\n",
       "      <td>5.070</td>\n",
       "      <td>5.066658</td>\n",
       "      <td>33.333439</td>\n",
       "      <td>45.751672</td>\n",
       "      <td>47.381537</td>\n",
       "      <td>-66.666561</td>\n",
       "      <td>0.054584</td>\n",
       "      <td>-9.259272e+00</td>\n",
       "      <td>5.070</td>\n",
       "      <td>5.15000</td>\n",
       "      <td>4.99000</td>\n",
       "      <td>5.050000</td>\n",
       "      <td>5.064205</td>\n",
       "      <td>41.176553</td>\n",
       "      <td>50.980414</td>\n",
       "      <td>47.670444</td>\n",
       "      <td>-58.823447</td>\n",
       "      <td>0.053949</td>\n",
       "      <td>33.433193</td>\n",
       "      <td>5.050000</td>\n",
       "      <td>5.139443</td>\n",
       "      <td>4.960557</td>\n",
       "      <td>5.0525</td>\n",
       "      <td>5.064410</td>\n",
       "      <td>41.176553</td>\n",
       "      <td>50.980414</td>\n",
       "      <td>47.465935</td>\n",
       "      <td>-58.823447</td>\n",
       "      <td>0.053975</td>\n",
       "      <td>31.890736</td>\n",
       "      <td>5.0525</td>\n",
       "      <td>5.138958</td>\n",
       "      <td>4.966042</td>\n",
       "      <td>5.058</td>\n",
       "      <td>5.066417</td>\n",
       "      <td>41.176553</td>\n",
       "      <td>50.980414</td>\n",
       "      <td>47.228213</td>\n",
       "      <td>-58.823447</td>\n",
       "      <td>0.054203</td>\n",
       "      <td>16.328068</td>\n",
       "      <td>5.058</td>\n",
       "      <td>5.141331</td>\n",
       "      <td>4.974669</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.006999</td>\n",
       "      <td>0.005403</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.009804</td>\n",
       "      <td>11177505.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-13 10:00:00</th>\n",
       "      <td>5.05</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>829400.0</td>\n",
       "      <td>1936.599976</td>\n",
       "      <td>1940.500000</td>\n",
       "      <td>1935.599976</td>\n",
       "      <td>1939.900024</td>\n",
       "      <td>1939.900024</td>\n",
       "      <td>7686.0</td>\n",
       "      <td>77.870003</td>\n",
       "      <td>78.070000</td>\n",
       "      <td>77.699997</td>\n",
       "      <td>77.809998</td>\n",
       "      <td>77.809998</td>\n",
       "      <td>8148.0</td>\n",
       "      <td>1391.790039</td>\n",
       "      <td>1392.010010</td>\n",
       "      <td>1382.880005</td>\n",
       "      <td>1383.880005</td>\n",
       "      <td>1383.880005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34253.511719</td>\n",
       "      <td>34276.269531</td>\n",
       "      <td>34227.371094</td>\n",
       "      <td>34259.648438</td>\n",
       "      <td>34259.648438</td>\n",
       "      <td>10927936.0</td>\n",
       "      <td>4397.970215</td>\n",
       "      <td>4400.870117</td>\n",
       "      <td>4395.160156</td>\n",
       "      <td>4400.680176</td>\n",
       "      <td>4400.680176</td>\n",
       "      <td>83108630.0</td>\n",
       "      <td>861.619995</td>\n",
       "      <td>861.840027</td>\n",
       "      <td>855.820007</td>\n",
       "      <td>856.909973</td>\n",
       "      <td>856.909973</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1906.130005</td>\n",
       "      <td>1906.479980</td>\n",
       "      <td>1892.530029</td>\n",
       "      <td>1894.469971</td>\n",
       "      <td>1894.469971</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.03</td>\n",
       "      <td>5.031291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.666746</td>\n",
       "      <td>36.260298</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.053051</td>\n",
       "      <td>-72.916729</td>\n",
       "      <td>5.03</td>\n",
       "      <td>5.078990</td>\n",
       "      <td>4.981010</td>\n",
       "      <td>5.060</td>\n",
       "      <td>5.043947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.111146</td>\n",
       "      <td>41.972457</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.053342</td>\n",
       "      <td>-9.183677e+01</td>\n",
       "      <td>5.060</td>\n",
       "      <td>5.15798</td>\n",
       "      <td>4.96202</td>\n",
       "      <td>5.053333</td>\n",
       "      <td>5.048481</td>\n",
       "      <td>11.764689</td>\n",
       "      <td>21.568644</td>\n",
       "      <td>43.555608</td>\n",
       "      <td>-88.235311</td>\n",
       "      <td>0.053211</td>\n",
       "      <td>-89.172153</td>\n",
       "      <td>5.053333</td>\n",
       "      <td>5.138708</td>\n",
       "      <td>4.967958</td>\n",
       "      <td>5.0425</td>\n",
       "      <td>5.052012</td>\n",
       "      <td>11.764689</td>\n",
       "      <td>21.568644</td>\n",
       "      <td>44.077134</td>\n",
       "      <td>-88.235311</td>\n",
       "      <td>0.053408</td>\n",
       "      <td>-64.869118</td>\n",
       "      <td>5.0425</td>\n",
       "      <td>5.127794</td>\n",
       "      <td>4.957206</td>\n",
       "      <td>5.050</td>\n",
       "      <td>5.055789</td>\n",
       "      <td>11.764689</td>\n",
       "      <td>21.568644</td>\n",
       "      <td>44.272180</td>\n",
       "      <td>-88.235311</td>\n",
       "      <td>0.053719</td>\n",
       "      <td>-78.976136</td>\n",
       "      <td>5.050</td>\n",
       "      <td>5.134853</td>\n",
       "      <td>4.965147</td>\n",
       "      <td>-0.010475</td>\n",
       "      <td>-0.007594</td>\n",
       "      <td>-0.002881</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.009901</td>\n",
       "      <td>10558605.0</td>\n",
       "      <td>-0.009901</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-13 10:15:00</th>\n",
       "      <td>5.05</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>30354.0</td>\n",
       "      <td>1939.800049</td>\n",
       "      <td>1940.800049</td>\n",
       "      <td>1938.599976</td>\n",
       "      <td>1939.400024</td>\n",
       "      <td>1939.400024</td>\n",
       "      <td>4225.0</td>\n",
       "      <td>77.800003</td>\n",
       "      <td>78.059998</td>\n",
       "      <td>77.769997</td>\n",
       "      <td>77.900002</td>\n",
       "      <td>77.900002</td>\n",
       "      <td>7363.0</td>\n",
       "      <td>1384.270020</td>\n",
       "      <td>1386.260010</td>\n",
       "      <td>1381.650024</td>\n",
       "      <td>1381.849976</td>\n",
       "      <td>1381.849976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34259.421875</td>\n",
       "      <td>34259.421875</td>\n",
       "      <td>34220.730469</td>\n",
       "      <td>34241.609375</td>\n",
       "      <td>34241.609375</td>\n",
       "      <td>9230512.0</td>\n",
       "      <td>4400.740234</td>\n",
       "      <td>4401.319824</td>\n",
       "      <td>4396.810059</td>\n",
       "      <td>4400.970215</td>\n",
       "      <td>4400.970215</td>\n",
       "      <td>68535169.0</td>\n",
       "      <td>857.289978</td>\n",
       "      <td>858.729980</td>\n",
       "      <td>855.349976</td>\n",
       "      <td>855.619995</td>\n",
       "      <td>855.619995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1895.189941</td>\n",
       "      <td>1898.449951</td>\n",
       "      <td>1891.079956</td>\n",
       "      <td>1891.510010</td>\n",
       "      <td>1891.510010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.02</td>\n",
       "      <td>5.020861</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.666746</td>\n",
       "      <td>36.260298</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.052441</td>\n",
       "      <td>-55.555556</td>\n",
       "      <td>5.02</td>\n",
       "      <td>5.068990</td>\n",
       "      <td>4.971010</td>\n",
       "      <td>5.060</td>\n",
       "      <td>5.035957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.111146</td>\n",
       "      <td>41.972457</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.053008</td>\n",
       "      <td>-9.183677e+01</td>\n",
       "      <td>5.060</td>\n",
       "      <td>5.15798</td>\n",
       "      <td>4.96202</td>\n",
       "      <td>5.053333</td>\n",
       "      <td>5.042421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.647081</td>\n",
       "      <td>43.555608</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.052997</td>\n",
       "      <td>-91.398031</td>\n",
       "      <td>5.053333</td>\n",
       "      <td>5.138708</td>\n",
       "      <td>4.967958</td>\n",
       "      <td>5.0425</td>\n",
       "      <td>5.047059</td>\n",
       "      <td>11.764689</td>\n",
       "      <td>21.568644</td>\n",
       "      <td>44.077134</td>\n",
       "      <td>-88.235311</td>\n",
       "      <td>0.053237</td>\n",
       "      <td>-64.869118</td>\n",
       "      <td>5.0425</td>\n",
       "      <td>5.127794</td>\n",
       "      <td>4.957206</td>\n",
       "      <td>5.046</td>\n",
       "      <td>5.051497</td>\n",
       "      <td>11.764689</td>\n",
       "      <td>21.568644</td>\n",
       "      <td>44.272180</td>\n",
       "      <td>-88.235311</td>\n",
       "      <td>0.053570</td>\n",
       "      <td>-73.243401</td>\n",
       "      <td>5.046</td>\n",
       "      <td>5.130475</td>\n",
       "      <td>4.961525</td>\n",
       "      <td>-0.013377</td>\n",
       "      <td>-0.008751</td>\n",
       "      <td>-0.004626</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.009901</td>\n",
       "      <td>10588959.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ERW.BK_Open  ERW.BK_High  ERW.BK_Low  ERW.BK_Close  \\\n",
       "Datetime                                                                  \n",
       "2023-11-10 15:15:00         5.05         5.10        5.05          5.10   \n",
       "2023-11-10 15:30:00         5.10         5.10        5.05          5.05   \n",
       "2023-11-10 15:45:00         5.05         5.10        5.05          5.05   \n",
       "2023-11-13 10:00:00         5.05         5.05        5.00          5.00   \n",
       "2023-11-13 10:15:00         5.05         5.05        5.00          5.00   \n",
       "\n",
       "                     ERW.BK_Adj Close  ERW.BK_Volume    Gold_Open  \\\n",
       "Datetime                                                            \n",
       "2023-11-10 15:15:00              5.10      1611000.0  1940.199951   \n",
       "2023-11-10 15:30:00              5.05        53800.0  1940.000000   \n",
       "2023-11-10 15:45:00              5.05       457201.0  1939.900024   \n",
       "2023-11-13 10:00:00              5.00       829400.0  1936.599976   \n",
       "2023-11-13 10:15:00              5.00        30354.0  1939.800049   \n",
       "\n",
       "                       Gold_High     Gold_Low   Gold_Close  Gold_Adj Close  \\\n",
       "Datetime                                                                     \n",
       "2023-11-10 15:15:00  1940.800049  1939.699951  1939.900024     1939.900024   \n",
       "2023-11-10 15:30:00  1940.000000  1939.500000  1939.800049     1939.800049   \n",
       "2023-11-10 15:45:00  1940.300049  1939.500000  1940.199951     1940.199951   \n",
       "2023-11-13 10:00:00  1940.500000  1935.599976  1939.900024     1939.900024   \n",
       "2023-11-13 10:15:00  1940.800049  1938.599976  1939.400024     1939.400024   \n",
       "\n",
       "                     Gold_Volume   Oil_Open   Oil_High    Oil_Low  Oil_Close  \\\n",
       "Datetime                                                                       \n",
       "2023-11-10 15:15:00        614.0  77.279999  77.339996  77.250000  77.269997   \n",
       "2023-11-10 15:30:00        960.0  77.269997  77.300003  77.260002  77.290001   \n",
       "2023-11-10 15:45:00       1045.0  77.290001  77.339996  77.220001  77.250000   \n",
       "2023-11-13 10:00:00       7686.0  77.870003  78.070000  77.699997  77.809998   \n",
       "2023-11-13 10:15:00       4225.0  77.800003  78.059998  77.769997  77.900002   \n",
       "\n",
       "                     Oil_Adj Close  Oil_Volume     SET_Open     SET_High  \\\n",
       "Datetime                                                                   \n",
       "2023-11-10 15:15:00      77.269997      1152.0  1398.459961  1399.099976   \n",
       "2023-11-10 15:30:00      77.290001      2039.0  1397.670044  1399.439941   \n",
       "2023-11-10 15:45:00      77.250000      1239.0  1398.250000  1399.500000   \n",
       "2023-11-13 10:00:00      77.809998      8148.0  1391.790039  1392.010010   \n",
       "2023-11-13 10:15:00      77.900002      7363.0  1384.270020  1386.260010   \n",
       "\n",
       "                         SET_Low    SET_Close  SET_Adj Close  SET_Volume  \\\n",
       "Datetime                                                                   \n",
       "2023-11-10 15:15:00  1396.969971  1397.910034    1397.910034         0.0   \n",
       "2023-11-10 15:30:00  1396.930054  1398.689941    1398.689941         0.0   \n",
       "2023-11-10 15:45:00  1393.390015  1393.390015    1393.390015         0.0   \n",
       "2023-11-13 10:00:00  1382.880005  1383.880005    1383.880005         0.0   \n",
       "2023-11-13 10:15:00  1381.650024  1381.849976    1381.849976         0.0   \n",
       "\n",
       "                         DJI_Open      DJI_High       DJI_Low     DJI_Close  \\\n",
       "Datetime                                                                      \n",
       "2023-11-10 15:15:00  34220.378906  34267.671875  34204.640625  34267.671875   \n",
       "2023-11-10 15:30:00  34268.128906  34290.019531  34261.578125  34283.429688   \n",
       "2023-11-10 15:45:00  34283.671875  34310.359375  34271.988281  34286.519531   \n",
       "2023-11-13 10:00:00  34253.511719  34276.269531  34227.371094  34259.648438   \n",
       "2023-11-13 10:15:00  34259.421875  34259.421875  34220.730469  34241.609375   \n",
       "\n",
       "                     DJI_Adj Close  DJI_Volume   SP500_Open   SP500_High  \\\n",
       "Datetime                                                                   \n",
       "2023-11-10 15:15:00   34267.671875   9220593.0  4407.600098  4412.910156   \n",
       "2023-11-10 15:30:00   34283.429688  10761736.0  4412.700195  4414.939941   \n",
       "2023-11-10 15:45:00   34286.519531  33535611.0  4414.729980  4418.029785   \n",
       "2023-11-13 10:00:00   34259.648438  10927936.0  4397.970215  4400.870117   \n",
       "2023-11-13 10:15:00   34241.609375   9230512.0  4400.740234  4401.319824   \n",
       "\n",
       "                       SP500_Low  SP500_Close  SP500_Adj Close  SP500_Volume  \\\n",
       "Datetime                                                                       \n",
       "2023-11-10 15:15:00  4405.569824  4412.649902      4412.649902    68946000.0   \n",
       "2023-11-10 15:30:00  4411.359863  4414.689941      4414.689941    90390000.0   \n",
       "2023-11-10 15:45:00  4413.629883  4415.890137      4415.890137   257745000.0   \n",
       "2023-11-13 10:00:00  4395.160156  4400.680176      4400.680176    83108630.0   \n",
       "2023-11-13 10:15:00  4396.810059  4400.970215      4400.970215    68535169.0   \n",
       "\n",
       "                     SET50_Open  SET50_High   SET50_Low  SET50_Close  \\\n",
       "Datetime                                                               \n",
       "2023-11-10 15:15:00  865.340027  866.260010  864.510010   865.010010   \n",
       "2023-11-10 15:30:00  865.010010  866.320007  864.309998   866.320007   \n",
       "2023-11-10 15:45:00  865.729980  866.919983  862.599976   862.770020   \n",
       "2023-11-13 10:00:00  861.619995  861.840027  855.820007   856.909973   \n",
       "2023-11-13 10:15:00  857.289978  858.729980  855.349976   855.619995   \n",
       "\n",
       "                     SET50_Adj Close  SET50_Volume  SET100_Open  SET100_High  \\\n",
       "Datetime                                                                       \n",
       "2023-11-10 15:15:00       865.010010           0.0  1915.579956  1916.780029   \n",
       "2023-11-10 15:30:00       866.320007           0.0  1914.229980  1917.280029   \n",
       "2023-11-10 15:45:00       862.770020           0.0  1915.239990  1918.099976   \n",
       "2023-11-13 10:00:00       856.909973           0.0  1906.130005  1906.479980   \n",
       "2023-11-13 10:15:00       855.619995           0.0  1895.189941  1898.449951   \n",
       "\n",
       "                      SET100_Low  SET100_Close  SET100_Adj Close  \\\n",
       "Datetime                                                           \n",
       "2023-11-10 15:15:00  1913.260010   1914.729980       1914.729980   \n",
       "2023-11-10 15:30:00  1913.359985   1916.459961       1916.459961   \n",
       "2023-11-10 15:45:00  1908.359985   1908.660034       1908.660034   \n",
       "2023-11-13 10:00:00  1892.530029   1894.469971       1894.469971   \n",
       "2023-11-13 10:15:00  1891.079956   1891.510010       1891.510010   \n",
       "\n",
       "                     SET100_Volume  SMA_5     EMA_5  Stoch_%K_5  Stoch_%D_5  \\\n",
       "Datetime                                                                      \n",
       "2023-11-10 15:15:00            0.0   5.09  5.090739   66.666561   66.666561   \n",
       "2023-11-10 15:30:00            0.0   5.10  5.077159    0.000000   44.444374   \n",
       "2023-11-10 15:45:00            0.0   5.08  5.068106    0.000000   22.222187   \n",
       "2023-11-13 10:00:00            0.0   5.03  5.031291    0.000000   16.666746   \n",
       "2023-11-13 10:15:00            0.0   5.02  5.020861    0.000000   16.666746   \n",
       "\n",
       "                         RSI_5       WPR_5     ATR_5      CCI_5  \\\n",
       "Datetime                                                          \n",
       "2023-11-10 15:15:00  59.187542  -33.333439  0.059311  -0.000079   \n",
       "2023-11-10 15:30:00  42.199251 -100.000000  0.057449 -95.237852   \n",
       "2023-11-10 15:45:00  42.199251 -100.000000  0.055959 -83.333174   \n",
       "2023-11-13 10:00:00  36.260298 -100.000000  0.053051 -72.916729   \n",
       "2023-11-13 10:15:00  36.260298 -100.000000  0.052441 -55.555556   \n",
       "\n",
       "                     Bollinger_mavg_5  Bollinger_hband_5  Bollinger_lband_5  \\\n",
       "Datetime                                                                      \n",
       "2023-11-10 15:15:00              5.09           5.187980           4.992020   \n",
       "2023-11-10 15:30:00              5.10           5.163245           5.036755   \n",
       "2023-11-10 15:45:00              5.08           5.128990           5.031011   \n",
       "2023-11-13 10:00:00              5.03           5.078990           4.981010   \n",
       "2023-11-13 10:15:00              5.02           5.068990           4.971010   \n",
       "\n",
       "                     SMA_10    EMA_10  Stoch_%K_10  Stoch_%D_10     RSI_10  \\\n",
       "Datetime                                                                     \n",
       "2023-11-10 15:15:00   5.065  5.074884    70.588136    70.588136  55.707271   \n",
       "2023-11-10 15:30:00   5.070  5.070360    33.333439    58.169904  47.381537   \n",
       "2023-11-10 15:45:00   5.070  5.066658    33.333439    45.751672  47.381537   \n",
       "2023-11-13 10:00:00   5.060  5.043947     0.000000    11.111146  41.972457   \n",
       "2023-11-13 10:15:00   5.060  5.035957     0.000000    11.111146  41.972457   \n",
       "\n",
       "                         WPR_10    ATR_10        CCI_10  Bollinger_mavg_10  \\\n",
       "Datetime                                                                     \n",
       "2023-11-10 15:15:00  -29.411864  0.055659  4.772068e+01              5.065   \n",
       "2023-11-10 15:30:00  -66.666561  0.055093  2.220451e-12              5.070   \n",
       "2023-11-10 15:45:00  -66.666561  0.054584 -9.259272e+00              5.070   \n",
       "2023-11-13 10:00:00 -100.000000  0.053342 -9.183677e+01              5.060   \n",
       "2023-11-13 10:15:00 -100.000000  0.053008 -9.183677e+01              5.060   \n",
       "\n",
       "                     Bollinger_hband_10  Bollinger_lband_10    SMA_15  \\\n",
       "Datetime                                                                \n",
       "2023-11-10 15:15:00             5.15500             4.97500  5.046667   \n",
       "2023-11-10 15:30:00             5.15000             4.99000  5.050000   \n",
       "2023-11-10 15:45:00             5.15000             4.99000  5.050000   \n",
       "2023-11-13 10:00:00             5.15798             4.96202  5.053333   \n",
       "2023-11-13 10:15:00             5.15798             4.96202  5.053333   \n",
       "\n",
       "                       EMA_15  Stoch_%K_15  Stoch_%D_15     RSI_15  \\\n",
       "Datetime                                                             \n",
       "2023-11-10 15:15:00  5.068554    70.588136    70.588136  53.463683   \n",
       "2023-11-10 15:30:00  5.066235    41.176553    60.784275  47.670444   \n",
       "2023-11-10 15:45:00  5.064205    41.176553    50.980414  47.670444   \n",
       "2023-11-13 10:00:00  5.048481    11.764689    21.568644  43.555608   \n",
       "2023-11-13 10:15:00  5.042421     0.000000    17.647081  43.555608   \n",
       "\n",
       "                         WPR_15    ATR_15     CCI_15  Bollinger_mavg_15  \\\n",
       "Datetime                                                                  \n",
       "2023-11-10 15:15:00  -29.411864  0.054533  81.800369           5.046667   \n",
       "2023-11-10 15:30:00  -58.823447  0.054231  39.225430           5.050000   \n",
       "2023-11-10 15:45:00  -58.823447  0.053949  33.433193           5.050000   \n",
       "2023-11-13 10:00:00  -88.235311  0.053211 -89.172153           5.053333   \n",
       "2023-11-13 10:15:00 -100.000000  0.052997 -91.398031           5.053333   \n",
       "\n",
       "                     Bollinger_hband_15  Bollinger_lband_15  SMA_20    EMA_20  \\\n",
       "Datetime                                                                        \n",
       "2023-11-10 15:15:00            5.139523            4.953811  5.0550  5.067603   \n",
       "2023-11-10 15:30:00            5.139443            4.960557  5.0525  5.065926   \n",
       "2023-11-10 15:45:00            5.139443            4.960557  5.0525  5.064410   \n",
       "2023-11-13 10:00:00            5.138708            4.967958  5.0425  5.052012   \n",
       "2023-11-13 10:15:00            5.138708            4.967958  5.0425  5.047059   \n",
       "\n",
       "                     Stoch_%K_20  Stoch_%D_20     RSI_20     WPR_20    ATR_20  \\\n",
       "Datetime                                                                        \n",
       "2023-11-10 15:15:00    70.588136    70.588136  52.150059 -29.411864  0.054404   \n",
       "2023-11-10 15:30:00    41.176553    60.784275  47.465935 -58.823447  0.054184   \n",
       "2023-11-10 15:45:00    41.176553    50.980414  47.465935 -58.823447  0.053975   \n",
       "2023-11-13 10:00:00    11.764689    21.568644  44.077134 -88.235311  0.053408   \n",
       "2023-11-13 10:15:00    11.764689    21.568644  44.077134 -88.235311  0.053237   \n",
       "\n",
       "                        CCI_20  Bollinger_mavg_20  Bollinger_hband_20  \\\n",
       "Datetime                                                                \n",
       "2023-11-10 15:15:00  62.164963             5.0550            5.143882   \n",
       "2023-11-10 15:30:00  29.081599             5.0525            5.138958   \n",
       "2023-11-10 15:45:00  31.890736             5.0525            5.138958   \n",
       "2023-11-13 10:00:00 -64.869118             5.0425            5.127794   \n",
       "2023-11-13 10:15:00 -64.869118             5.0425            5.127794   \n",
       "\n",
       "                     Bollinger_lband_20  SMA_25    EMA_25  Stoch_%K_25  \\\n",
       "Datetime                                                                 \n",
       "2023-11-10 15:15:00            4.966118   5.058  5.069267    70.588136   \n",
       "2023-11-10 15:30:00            4.966042   5.058  5.067785    41.176553   \n",
       "2023-11-10 15:45:00            4.966042   5.058  5.066417    41.176553   \n",
       "2023-11-13 10:00:00            4.957206   5.050  5.055789    11.764689   \n",
       "2023-11-13 10:15:00            4.957206   5.046  5.051497    11.764689   \n",
       "\n",
       "                     Stoch_%D_25     RSI_25     WPR_25    ATR_25     CCI_25  \\\n",
       "Datetime                                                                      \n",
       "2023-11-10 15:15:00    70.588136  51.307980 -29.411864  0.054561  55.414885   \n",
       "2023-11-10 15:30:00    60.784275  47.228213 -58.823447  0.054379  17.768995   \n",
       "2023-11-10 15:45:00    50.980414  47.228213 -58.823447  0.054203  16.328068   \n",
       "2023-11-13 10:00:00    21.568644  44.272180 -88.235311  0.053719 -78.976136   \n",
       "2023-11-13 10:15:00    21.568644  44.272180 -88.235311  0.053570 -73.243401   \n",
       "\n",
       "                     Bollinger_mavg_25  Bollinger_hband_25  \\\n",
       "Datetime                                                     \n",
       "2023-11-10 15:15:00              5.058            5.141331   \n",
       "2023-11-10 15:30:00              5.058            5.141331   \n",
       "2023-11-10 15:45:00              5.058            5.141331   \n",
       "2023-11-13 10:00:00              5.050            5.134853   \n",
       "2023-11-13 10:15:00              5.046            5.130475   \n",
       "\n",
       "                     Bollinger_lband_25      MACD  MACD_signal  MACD_diff  \\\n",
       "Datetime                                                                    \n",
       "2023-11-10 15:15:00            4.974669  0.001675    -0.010398   0.012072   \n",
       "2023-11-10 15:30:00            4.974669 -0.000160    -0.008350   0.008190   \n",
       "2023-11-10 15:45:00            4.974669 -0.001596    -0.006999   0.005403   \n",
       "2023-11-13 10:00:00            4.965147 -0.010475    -0.007594  -0.002881   \n",
       "2023-11-13 10:15:00            4.961525 -0.013377    -0.008751  -0.004626   \n",
       "\n",
       "                     Momentum       ROC         OBV  Close_pct_change  \\\n",
       "Datetime                                                                \n",
       "2023-11-10 15:15:00      0.10  0.020000  10774104.0          0.000000   \n",
       "2023-11-10 15:30:00     -0.10 -0.019417  10720304.0         -0.009804   \n",
       "2023-11-10 15:45:00     -0.05 -0.009804  11177505.0          0.000000   \n",
       "2023-11-13 10:00:00     -0.05 -0.009901  10558605.0         -0.009901   \n",
       "2023-11-13 10:15:00     -0.05 -0.009901  10588959.0          0.000000   \n",
       "\n",
       "                     hour_of_day  day_of_week  \n",
       "Datetime                                       \n",
       "2023-11-10 15:15:00           15            4  \n",
       "2023-11-10 15:30:00           15            4  \n",
       "2023-11-10 15:45:00           15            4  \n",
       "2023-11-13 10:00:00           10            0  \n",
       "2023-11-13 10:15:00           10            0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERW.BK_Open         531\n",
      "ERW.BK_High         531\n",
      "ERW.BK_Low          531\n",
      "ERW.BK_Close        531\n",
      "ERW.BK_Adj Close    531\n",
      "                   ... \n",
      "ROC                 531\n",
      "OBV                 531\n",
      "Close_pct_change    531\n",
      "hour_of_day         531\n",
      "day_of_week         531\n",
      "Length: 112, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ta\n",
    "\n",
    "def calculate_indicators(df, windows, stock_name):\n",
    "    # กำหนดคอลัมน์ที่จำเป็น\n",
    "    global close_col\n",
    "    close_col = f'{stock_name}_Close'\n",
    "    low_col = f'{stock_name}_Low'\n",
    "    high_col = f'{stock_name}_High'\n",
    "    volume_col = f'{stock_name}_Volume'\n",
    "\n",
    "    # ตัวชี้วัดทางเทคนิค\n",
    "    for window in windows:\n",
    "        # Simple and Exponential Moving Averages\n",
    "        df[f'SMA_{window}'] = ta.trend.sma_indicator(df[close_col], window=window)\n",
    "        df[f'EMA_{window}'] = ta.trend.ema_indicator(df[close_col], window=window)\n",
    "        \n",
    "        # Stochastic Oscillator\n",
    "        stoch = ta.momentum.StochasticOscillator(df[high_col], df[low_col], df[close_col], window=window, smooth_window=3)\n",
    "        df[f'Stoch_%K_{window}'] = stoch.stoch()\n",
    "        df[f'Stoch_%D_{window}'] = stoch.stoch_signal()\n",
    "\n",
    "        # Relative Strength Index\n",
    "        df[f'RSI_{window}'] = ta.momentum.rsi(df[close_col], window=window)\n",
    "\n",
    "        # Williams %R\n",
    "        df[f'WPR_{window}'] = ta.momentum.williams_r(df[high_col], df[low_col], df[close_col], lbp=window)\n",
    "\n",
    "        # Average True Range\n",
    "        df[f'ATR_{window}'] = ta.volatility.average_true_range(df[high_col], df[low_col], df[close_col], window=window)\n",
    "\n",
    "        # Commodity Channel Index\n",
    "        df[f'CCI_{window}'] = ta.trend.cci(df[high_col], df[low_col], df[close_col], window=window)\n",
    "\n",
    "        # Bollinger Bands\n",
    "        bollinger = ta.volatility.BollingerBands(df[close_col], window=window, window_dev=2)\n",
    "        df[f'Bollinger_mavg_{window}'] = bollinger.bollinger_mavg()\n",
    "        df[f'Bollinger_hband_{window}'] = bollinger.bollinger_hband()\n",
    "        df[f'Bollinger_lband_{window}'] = bollinger.bollinger_lband()\n",
    "\n",
    "    # ตัวชี้วัด MACD\n",
    "    macd = ta.trend.MACD(df[close_col], window_slow=26, window_fast=12, window_sign=9)\n",
    "    df['MACD'] = macd.macd()\n",
    "    df['MACD_signal'] = macd.macd_signal()\n",
    "    df['MACD_diff'] = macd.macd_diff()\n",
    "\n",
    "\n",
    "    # ตัวชี้วัดเพิ่มเติม\n",
    "    df['Momentum'] = df[close_col].diff(4)\n",
    "    df['ROC'] = df[close_col].pct_change(periods=4)\n",
    "    df['OBV'] = ta.volume.on_balance_volume(df[close_col], df[volume_col])\n",
    "    df['Close_pct_change'] = df[close_col].pct_change()\n",
    "\n",
    "    # Time-based Features\n",
    "    df['hour_of_day'] = df.index.hour\n",
    "    df['day_of_week'] = df.index.dayofweek\n",
    "\n",
    "    # ลบข้อมูล NaN\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "# กำหนด windows size\n",
    "windows = [5, 10, 15, 20, 25]\n",
    "\n",
    "# ใช้งานฟังก์ชันกับ DataFrame\n",
    "calculate_indicators(df, windows, stock_name)\n",
    "\n",
    "# แสดงผล DataFrame\n",
    "display(df.head(5))\n",
    "print(df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ERW.BK_Open</th>\n",
       "      <th>ERW.BK_High</th>\n",
       "      <th>ERW.BK_Low</th>\n",
       "      <th>ERW.BK_Close</th>\n",
       "      <th>ERW.BK_Adj Close</th>\n",
       "      <th>ERW.BK_Volume</th>\n",
       "      <th>Gold_Open</th>\n",
       "      <th>Gold_High</th>\n",
       "      <th>Gold_Low</th>\n",
       "      <th>Gold_Close</th>\n",
       "      <th>Gold_Adj Close</th>\n",
       "      <th>Gold_Volume</th>\n",
       "      <th>Oil_Open</th>\n",
       "      <th>Oil_High</th>\n",
       "      <th>Oil_Low</th>\n",
       "      <th>Oil_Close</th>\n",
       "      <th>Oil_Adj Close</th>\n",
       "      <th>Oil_Volume</th>\n",
       "      <th>SET_Open</th>\n",
       "      <th>SET_High</th>\n",
       "      <th>SET_Low</th>\n",
       "      <th>SET_Close</th>\n",
       "      <th>SET_Adj Close</th>\n",
       "      <th>SET_Volume</th>\n",
       "      <th>DJI_Open</th>\n",
       "      <th>DJI_High</th>\n",
       "      <th>DJI_Low</th>\n",
       "      <th>DJI_Close</th>\n",
       "      <th>DJI_Adj Close</th>\n",
       "      <th>DJI_Volume</th>\n",
       "      <th>SP500_Open</th>\n",
       "      <th>SP500_High</th>\n",
       "      <th>SP500_Low</th>\n",
       "      <th>SP500_Close</th>\n",
       "      <th>SP500_Adj Close</th>\n",
       "      <th>SP500_Volume</th>\n",
       "      <th>SET50_Open</th>\n",
       "      <th>SET50_High</th>\n",
       "      <th>SET50_Low</th>\n",
       "      <th>SET50_Close</th>\n",
       "      <th>SET50_Adj Close</th>\n",
       "      <th>SET50_Volume</th>\n",
       "      <th>SET100_Open</th>\n",
       "      <th>SET100_High</th>\n",
       "      <th>SET100_Low</th>\n",
       "      <th>SET100_Close</th>\n",
       "      <th>SET100_Adj Close</th>\n",
       "      <th>SET100_Volume</th>\n",
       "      <th>SMA_5</th>\n",
       "      <th>EMA_5</th>\n",
       "      <th>Stoch_%K_5</th>\n",
       "      <th>Stoch_%D_5</th>\n",
       "      <th>RSI_5</th>\n",
       "      <th>WPR_5</th>\n",
       "      <th>ATR_5</th>\n",
       "      <th>CCI_5</th>\n",
       "      <th>Bollinger_mavg_5</th>\n",
       "      <th>Bollinger_hband_5</th>\n",
       "      <th>Bollinger_lband_5</th>\n",
       "      <th>SMA_10</th>\n",
       "      <th>EMA_10</th>\n",
       "      <th>Stoch_%K_10</th>\n",
       "      <th>Stoch_%D_10</th>\n",
       "      <th>RSI_10</th>\n",
       "      <th>WPR_10</th>\n",
       "      <th>ATR_10</th>\n",
       "      <th>CCI_10</th>\n",
       "      <th>Bollinger_mavg_10</th>\n",
       "      <th>Bollinger_hband_10</th>\n",
       "      <th>Bollinger_lband_10</th>\n",
       "      <th>SMA_15</th>\n",
       "      <th>EMA_15</th>\n",
       "      <th>Stoch_%K_15</th>\n",
       "      <th>Stoch_%D_15</th>\n",
       "      <th>RSI_15</th>\n",
       "      <th>WPR_15</th>\n",
       "      <th>ATR_15</th>\n",
       "      <th>CCI_15</th>\n",
       "      <th>Bollinger_mavg_15</th>\n",
       "      <th>Bollinger_hband_15</th>\n",
       "      <th>Bollinger_lband_15</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Stoch_%K_20</th>\n",
       "      <th>Stoch_%D_20</th>\n",
       "      <th>RSI_20</th>\n",
       "      <th>WPR_20</th>\n",
       "      <th>ATR_20</th>\n",
       "      <th>CCI_20</th>\n",
       "      <th>Bollinger_mavg_20</th>\n",
       "      <th>Bollinger_hband_20</th>\n",
       "      <th>Bollinger_lband_20</th>\n",
       "      <th>SMA_25</th>\n",
       "      <th>EMA_25</th>\n",
       "      <th>Stoch_%K_25</th>\n",
       "      <th>Stoch_%D_25</th>\n",
       "      <th>RSI_25</th>\n",
       "      <th>WPR_25</th>\n",
       "      <th>ATR_25</th>\n",
       "      <th>CCI_25</th>\n",
       "      <th>Bollinger_mavg_25</th>\n",
       "      <th>Bollinger_hband_25</th>\n",
       "      <th>Bollinger_lband_25</th>\n",
       "      <th>MACD</th>\n",
       "      <th>MACD_signal</th>\n",
       "      <th>MACD_diff</th>\n",
       "      <th>Momentum</th>\n",
       "      <th>ROC</th>\n",
       "      <th>OBV</th>\n",
       "      <th>Close_pct_change</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-01-05 12:15:00</th>\n",
       "      <td>5.20</td>\n",
       "      <td>5.25</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.25</td>\n",
       "      <td>5.25</td>\n",
       "      <td>270600.0</td>\n",
       "      <td>2050.500000</td>\n",
       "      <td>2052.300049</td>\n",
       "      <td>2047.800049</td>\n",
       "      <td>2048.300049</td>\n",
       "      <td>2048.300049</td>\n",
       "      <td>3184.0</td>\n",
       "      <td>73.839996</td>\n",
       "      <td>73.930000</td>\n",
       "      <td>73.639999</td>\n",
       "      <td>73.660004</td>\n",
       "      <td>73.660004</td>\n",
       "      <td>4888.0</td>\n",
       "      <td>1431.290039</td>\n",
       "      <td>1431.469971</td>\n",
       "      <td>1429.359985</td>\n",
       "      <td>1430.560059</td>\n",
       "      <td>1430.560059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37375.929688</td>\n",
       "      <td>37385.261719</td>\n",
       "      <td>37347.480469</td>\n",
       "      <td>37350.468750</td>\n",
       "      <td>37350.468750</td>\n",
       "      <td>6434746.0</td>\n",
       "      <td>4695.419922</td>\n",
       "      <td>4696.839844</td>\n",
       "      <td>4690.100098</td>\n",
       "      <td>4690.700195</td>\n",
       "      <td>4690.700195</td>\n",
       "      <td>45079284.0</td>\n",
       "      <td>877.049988</td>\n",
       "      <td>877.150024</td>\n",
       "      <td>875.289978</td>\n",
       "      <td>876.789978</td>\n",
       "      <td>876.789978</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1945.989990</td>\n",
       "      <td>1947.319946</td>\n",
       "      <td>1943.250000</td>\n",
       "      <td>1945.589966</td>\n",
       "      <td>1945.589966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.24</td>\n",
       "      <td>5.248453</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>48.060458</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>0.068006</td>\n",
       "      <td>-41.666667</td>\n",
       "      <td>5.24</td>\n",
       "      <td>5.28000</td>\n",
       "      <td>5.20000</td>\n",
       "      <td>5.250</td>\n",
       "      <td>5.253827</td>\n",
       "      <td>50.0</td>\n",
       "      <td>38.888960</td>\n",
       "      <td>47.807249</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>0.067443</td>\n",
       "      <td>-83.333333</td>\n",
       "      <td>5.250</td>\n",
       "      <td>5.294722</td>\n",
       "      <td>5.205278</td>\n",
       "      <td>5.270000</td>\n",
       "      <td>5.257645</td>\n",
       "      <td>33.333439</td>\n",
       "      <td>33.333439</td>\n",
       "      <td>48.621594</td>\n",
       "      <td>-66.666561</td>\n",
       "      <td>0.066056</td>\n",
       "      <td>-100.694465</td>\n",
       "      <td>5.270000</td>\n",
       "      <td>5.341181</td>\n",
       "      <td>5.198819</td>\n",
       "      <td>5.2675</td>\n",
       "      <td>5.259378</td>\n",
       "      <td>33.333439</td>\n",
       "      <td>33.333439</td>\n",
       "      <td>49.175232</td>\n",
       "      <td>-66.666561</td>\n",
       "      <td>0.064787</td>\n",
       "      <td>-79.096051</td>\n",
       "      <td>5.2675</td>\n",
       "      <td>5.340129</td>\n",
       "      <td>5.194871</td>\n",
       "      <td>5.264</td>\n",
       "      <td>5.259965</td>\n",
       "      <td>33.333439</td>\n",
       "      <td>33.333439</td>\n",
       "      <td>49.570490</td>\n",
       "      <td>-66.666561</td>\n",
       "      <td>0.063506</td>\n",
       "      <td>-73.302486</td>\n",
       "      <td>5.264</td>\n",
       "      <td>5.330453</td>\n",
       "      <td>5.197547</td>\n",
       "      <td>-0.004313</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>-0.004430</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>215753307.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-05 14:15:00</th>\n",
       "      <td>5.25</td>\n",
       "      <td>5.25</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.20</td>\n",
       "      <td>5.20</td>\n",
       "      <td>590800.0</td>\n",
       "      <td>2056.100098</td>\n",
       "      <td>2056.199951</td>\n",
       "      <td>2054.100098</td>\n",
       "      <td>2054.300049</td>\n",
       "      <td>2054.300049</td>\n",
       "      <td>908.0</td>\n",
       "      <td>73.900002</td>\n",
       "      <td>74.010002</td>\n",
       "      <td>73.750000</td>\n",
       "      <td>73.760002</td>\n",
       "      <td>73.760002</td>\n",
       "      <td>15922.0</td>\n",
       "      <td>1430.869995</td>\n",
       "      <td>1430.869995</td>\n",
       "      <td>1430.770020</td>\n",
       "      <td>1430.770020</td>\n",
       "      <td>1430.770020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37506.800781</td>\n",
       "      <td>37511.488281</td>\n",
       "      <td>37467.289062</td>\n",
       "      <td>37474.859375</td>\n",
       "      <td>37474.859375</td>\n",
       "      <td>11272802.0</td>\n",
       "      <td>4704.069824</td>\n",
       "      <td>4705.069824</td>\n",
       "      <td>4698.129883</td>\n",
       "      <td>4699.270020</td>\n",
       "      <td>4699.270020</td>\n",
       "      <td>51791000.0</td>\n",
       "      <td>876.840027</td>\n",
       "      <td>876.840027</td>\n",
       "      <td>876.760010</td>\n",
       "      <td>876.760010</td>\n",
       "      <td>876.760010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1946.180054</td>\n",
       "      <td>1946.180054</td>\n",
       "      <td>1946.010010</td>\n",
       "      <td>1946.010010</td>\n",
       "      <td>1946.010010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.24</td>\n",
       "      <td>5.232302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>28.125136</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>0.064405</td>\n",
       "      <td>-166.666667</td>\n",
       "      <td>5.24</td>\n",
       "      <td>5.28000</td>\n",
       "      <td>5.20000</td>\n",
       "      <td>5.245</td>\n",
       "      <td>5.244040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.777813</td>\n",
       "      <td>38.561730</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>0.065699</td>\n",
       "      <td>-125.925926</td>\n",
       "      <td>5.245</td>\n",
       "      <td>5.298852</td>\n",
       "      <td>5.191148</td>\n",
       "      <td>5.263333</td>\n",
       "      <td>5.250440</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.222293</td>\n",
       "      <td>42.692269</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.064986</td>\n",
       "      <td>-127.451029</td>\n",
       "      <td>5.263333</td>\n",
       "      <td>5.340506</td>\n",
       "      <td>5.186161</td>\n",
       "      <td>5.2650</td>\n",
       "      <td>5.253723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.222293</td>\n",
       "      <td>44.826938</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.064048</td>\n",
       "      <td>-117.647108</td>\n",
       "      <td>5.2650</td>\n",
       "      <td>5.343103</td>\n",
       "      <td>5.186897</td>\n",
       "      <td>5.262</td>\n",
       "      <td>5.255352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.222293</td>\n",
       "      <td>46.119618</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.062966</td>\n",
       "      <td>-111.967756</td>\n",
       "      <td>5.262</td>\n",
       "      <td>5.332880</td>\n",
       "      <td>5.191120</td>\n",
       "      <td>-0.008435</td>\n",
       "      <td>-0.001594</td>\n",
       "      <td>-0.006841</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.009524</td>\n",
       "      <td>215162507.0</td>\n",
       "      <td>-0.009524</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-05 14:30:00</th>\n",
       "      <td>5.20</td>\n",
       "      <td>5.25</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.20</td>\n",
       "      <td>5.20</td>\n",
       "      <td>1276230.0</td>\n",
       "      <td>2054.199951</td>\n",
       "      <td>2055.199951</td>\n",
       "      <td>2053.199951</td>\n",
       "      <td>2053.500000</td>\n",
       "      <td>2053.500000</td>\n",
       "      <td>714.0</td>\n",
       "      <td>73.760002</td>\n",
       "      <td>73.790001</td>\n",
       "      <td>73.660004</td>\n",
       "      <td>73.779999</td>\n",
       "      <td>73.779999</td>\n",
       "      <td>4509.0</td>\n",
       "      <td>1431.699951</td>\n",
       "      <td>1431.699951</td>\n",
       "      <td>1429.949951</td>\n",
       "      <td>1429.949951</td>\n",
       "      <td>1429.949951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37474.410156</td>\n",
       "      <td>37500.070312</td>\n",
       "      <td>37440.960938</td>\n",
       "      <td>37448.769531</td>\n",
       "      <td>37448.769531</td>\n",
       "      <td>6859581.0</td>\n",
       "      <td>4699.270020</td>\n",
       "      <td>4701.819824</td>\n",
       "      <td>4693.850098</td>\n",
       "      <td>4694.580078</td>\n",
       "      <td>4694.580078</td>\n",
       "      <td>48581000.0</td>\n",
       "      <td>877.539978</td>\n",
       "      <td>877.539978</td>\n",
       "      <td>875.760010</td>\n",
       "      <td>876.210022</td>\n",
       "      <td>876.210022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1947.640015</td>\n",
       "      <td>1947.640015</td>\n",
       "      <td>1944.449951</td>\n",
       "      <td>1944.449951</td>\n",
       "      <td>1944.449951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.23</td>\n",
       "      <td>5.221534</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>28.125136</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>0.061524</td>\n",
       "      <td>-83.333333</td>\n",
       "      <td>5.23</td>\n",
       "      <td>5.27899</td>\n",
       "      <td>5.18101</td>\n",
       "      <td>5.235</td>\n",
       "      <td>5.236033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>38.561730</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>0.064129</td>\n",
       "      <td>-106.060606</td>\n",
       "      <td>5.235</td>\n",
       "      <td>5.280826</td>\n",
       "      <td>5.189174</td>\n",
       "      <td>5.256667</td>\n",
       "      <td>5.244135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.111146</td>\n",
       "      <td>42.692269</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.063987</td>\n",
       "      <td>-111.842155</td>\n",
       "      <td>5.256667</td>\n",
       "      <td>5.337220</td>\n",
       "      <td>5.176113</td>\n",
       "      <td>5.2650</td>\n",
       "      <td>5.248607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.111146</td>\n",
       "      <td>44.826938</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.063345</td>\n",
       "      <td>-117.647108</td>\n",
       "      <td>5.2650</td>\n",
       "      <td>5.343103</td>\n",
       "      <td>5.186897</td>\n",
       "      <td>5.260</td>\n",
       "      <td>5.251094</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.111146</td>\n",
       "      <td>46.119618</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.062447</td>\n",
       "      <td>-107.526937</td>\n",
       "      <td>5.260</td>\n",
       "      <td>5.334833</td>\n",
       "      <td>5.185167</td>\n",
       "      <td>-0.011569</td>\n",
       "      <td>-0.003589</td>\n",
       "      <td>-0.007980</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.009524</td>\n",
       "      <td>216438737.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-05 14:45:00</th>\n",
       "      <td>5.20</td>\n",
       "      <td>5.25</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.20</td>\n",
       "      <td>5.20</td>\n",
       "      <td>115400.0</td>\n",
       "      <td>2053.500000</td>\n",
       "      <td>2053.500000</td>\n",
       "      <td>2051.800049</td>\n",
       "      <td>2052.600098</td>\n",
       "      <td>2052.600098</td>\n",
       "      <td>1084.0</td>\n",
       "      <td>73.769997</td>\n",
       "      <td>73.809998</td>\n",
       "      <td>73.660004</td>\n",
       "      <td>73.699997</td>\n",
       "      <td>73.699997</td>\n",
       "      <td>1792.0</td>\n",
       "      <td>1430.770020</td>\n",
       "      <td>1431.290039</td>\n",
       "      <td>1427.969971</td>\n",
       "      <td>1428.560059</td>\n",
       "      <td>1428.560059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37449.289062</td>\n",
       "      <td>37452.988281</td>\n",
       "      <td>37377.230469</td>\n",
       "      <td>37386.449219</td>\n",
       "      <td>37386.449219</td>\n",
       "      <td>7070748.0</td>\n",
       "      <td>4694.640137</td>\n",
       "      <td>4695.399902</td>\n",
       "      <td>4684.069824</td>\n",
       "      <td>4685.120117</td>\n",
       "      <td>4685.120117</td>\n",
       "      <td>53007000.0</td>\n",
       "      <td>877.159973</td>\n",
       "      <td>877.520020</td>\n",
       "      <td>874.200012</td>\n",
       "      <td>875.460022</td>\n",
       "      <td>875.460022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1945.939941</td>\n",
       "      <td>1946.819946</td>\n",
       "      <td>1941.170044</td>\n",
       "      <td>1942.750000</td>\n",
       "      <td>1942.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.22</td>\n",
       "      <td>5.214356</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.125136</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>0.059219</td>\n",
       "      <td>-55.555556</td>\n",
       "      <td>5.22</td>\n",
       "      <td>5.26899</td>\n",
       "      <td>5.17101</td>\n",
       "      <td>5.230</td>\n",
       "      <td>5.229481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.561730</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>0.062716</td>\n",
       "      <td>-99.099099</td>\n",
       "      <td>5.230</td>\n",
       "      <td>5.278990</td>\n",
       "      <td>5.181010</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>5.238618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.692269</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.063054</td>\n",
       "      <td>-100.000025</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>5.331650</td>\n",
       "      <td>5.168350</td>\n",
       "      <td>5.2625</td>\n",
       "      <td>5.243978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.826938</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.062678</td>\n",
       "      <td>-111.461660</td>\n",
       "      <td>5.2625</td>\n",
       "      <td>5.345416</td>\n",
       "      <td>5.179584</td>\n",
       "      <td>5.258</td>\n",
       "      <td>5.247164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46.119618</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.061949</td>\n",
       "      <td>-99.247135</td>\n",
       "      <td>5.258</td>\n",
       "      <td>5.336384</td>\n",
       "      <td>5.179616</td>\n",
       "      <td>-0.013892</td>\n",
       "      <td>-0.005649</td>\n",
       "      <td>-0.008242</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.009524</td>\n",
       "      <td>216554137.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-05 15:00:00</th>\n",
       "      <td>5.20</td>\n",
       "      <td>5.25</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.20</td>\n",
       "      <td>5.20</td>\n",
       "      <td>168500.0</td>\n",
       "      <td>2052.600098</td>\n",
       "      <td>2052.600098</td>\n",
       "      <td>2050.699951</td>\n",
       "      <td>2051.600098</td>\n",
       "      <td>2051.600098</td>\n",
       "      <td>870.0</td>\n",
       "      <td>73.699997</td>\n",
       "      <td>73.760002</td>\n",
       "      <td>73.680000</td>\n",
       "      <td>73.739998</td>\n",
       "      <td>73.739998</td>\n",
       "      <td>982.0</td>\n",
       "      <td>1428.920044</td>\n",
       "      <td>1430.239990</td>\n",
       "      <td>1428.089966</td>\n",
       "      <td>1429.239990</td>\n",
       "      <td>1429.239990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37386.281250</td>\n",
       "      <td>37427.039062</td>\n",
       "      <td>37380.628906</td>\n",
       "      <td>37415.089844</td>\n",
       "      <td>37415.089844</td>\n",
       "      <td>7259176.0</td>\n",
       "      <td>4685.109863</td>\n",
       "      <td>4689.899902</td>\n",
       "      <td>4684.180176</td>\n",
       "      <td>4688.799805</td>\n",
       "      <td>4688.799805</td>\n",
       "      <td>54990000.0</td>\n",
       "      <td>875.700012</td>\n",
       "      <td>876.770020</td>\n",
       "      <td>874.820007</td>\n",
       "      <td>875.679993</td>\n",
       "      <td>875.679993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1941.839966</td>\n",
       "      <td>1945.109985</td>\n",
       "      <td>1941.579956</td>\n",
       "      <td>1943.760010</td>\n",
       "      <td>1943.760010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.21</td>\n",
       "      <td>5.209571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.125136</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>0.057375</td>\n",
       "      <td>-41.666667</td>\n",
       "      <td>5.21</td>\n",
       "      <td>5.25000</td>\n",
       "      <td>5.17000</td>\n",
       "      <td>5.225</td>\n",
       "      <td>5.224121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.561730</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>0.061444</td>\n",
       "      <td>-83.333333</td>\n",
       "      <td>5.225</td>\n",
       "      <td>5.275000</td>\n",
       "      <td>5.175000</td>\n",
       "      <td>5.243333</td>\n",
       "      <td>5.233791</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.692269</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.062184</td>\n",
       "      <td>-87.837864</td>\n",
       "      <td>5.243333</td>\n",
       "      <td>5.323887</td>\n",
       "      <td>5.162779</td>\n",
       "      <td>5.2575</td>\n",
       "      <td>5.239789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.826938</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.062044</td>\n",
       "      <td>-98.989936</td>\n",
       "      <td>5.2575</td>\n",
       "      <td>5.342794</td>\n",
       "      <td>5.172206</td>\n",
       "      <td>5.256</td>\n",
       "      <td>5.243536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46.119618</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.061471</td>\n",
       "      <td>-91.683072</td>\n",
       "      <td>5.256</td>\n",
       "      <td>5.337585</td>\n",
       "      <td>5.174415</td>\n",
       "      <td>-0.015553</td>\n",
       "      <td>-0.007630</td>\n",
       "      <td>-0.007923</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.009524</td>\n",
       "      <td>216722637.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ERW.BK_Open  ERW.BK_High  ERW.BK_Low  ERW.BK_Close  \\\n",
       "Datetime                                                                  \n",
       "2024-01-05 12:15:00         5.20         5.25         5.2          5.25   \n",
       "2024-01-05 14:15:00         5.25         5.25         5.2          5.20   \n",
       "2024-01-05 14:30:00         5.20         5.25         5.2          5.20   \n",
       "2024-01-05 14:45:00         5.20         5.25         5.2          5.20   \n",
       "2024-01-05 15:00:00         5.20         5.25         5.2          5.20   \n",
       "\n",
       "                     ERW.BK_Adj Close  ERW.BK_Volume    Gold_Open  \\\n",
       "Datetime                                                            \n",
       "2024-01-05 12:15:00              5.25       270600.0  2050.500000   \n",
       "2024-01-05 14:15:00              5.20       590800.0  2056.100098   \n",
       "2024-01-05 14:30:00              5.20      1276230.0  2054.199951   \n",
       "2024-01-05 14:45:00              5.20       115400.0  2053.500000   \n",
       "2024-01-05 15:00:00              5.20       168500.0  2052.600098   \n",
       "\n",
       "                       Gold_High     Gold_Low   Gold_Close  Gold_Adj Close  \\\n",
       "Datetime                                                                     \n",
       "2024-01-05 12:15:00  2052.300049  2047.800049  2048.300049     2048.300049   \n",
       "2024-01-05 14:15:00  2056.199951  2054.100098  2054.300049     2054.300049   \n",
       "2024-01-05 14:30:00  2055.199951  2053.199951  2053.500000     2053.500000   \n",
       "2024-01-05 14:45:00  2053.500000  2051.800049  2052.600098     2052.600098   \n",
       "2024-01-05 15:00:00  2052.600098  2050.699951  2051.600098     2051.600098   \n",
       "\n",
       "                     Gold_Volume   Oil_Open   Oil_High    Oil_Low  Oil_Close  \\\n",
       "Datetime                                                                       \n",
       "2024-01-05 12:15:00       3184.0  73.839996  73.930000  73.639999  73.660004   \n",
       "2024-01-05 14:15:00        908.0  73.900002  74.010002  73.750000  73.760002   \n",
       "2024-01-05 14:30:00        714.0  73.760002  73.790001  73.660004  73.779999   \n",
       "2024-01-05 14:45:00       1084.0  73.769997  73.809998  73.660004  73.699997   \n",
       "2024-01-05 15:00:00        870.0  73.699997  73.760002  73.680000  73.739998   \n",
       "\n",
       "                     Oil_Adj Close  Oil_Volume     SET_Open     SET_High  \\\n",
       "Datetime                                                                   \n",
       "2024-01-05 12:15:00      73.660004      4888.0  1431.290039  1431.469971   \n",
       "2024-01-05 14:15:00      73.760002     15922.0  1430.869995  1430.869995   \n",
       "2024-01-05 14:30:00      73.779999      4509.0  1431.699951  1431.699951   \n",
       "2024-01-05 14:45:00      73.699997      1792.0  1430.770020  1431.290039   \n",
       "2024-01-05 15:00:00      73.739998       982.0  1428.920044  1430.239990   \n",
       "\n",
       "                         SET_Low    SET_Close  SET_Adj Close  SET_Volume  \\\n",
       "Datetime                                                                   \n",
       "2024-01-05 12:15:00  1429.359985  1430.560059    1430.560059         0.0   \n",
       "2024-01-05 14:15:00  1430.770020  1430.770020    1430.770020         0.0   \n",
       "2024-01-05 14:30:00  1429.949951  1429.949951    1429.949951         0.0   \n",
       "2024-01-05 14:45:00  1427.969971  1428.560059    1428.560059         0.0   \n",
       "2024-01-05 15:00:00  1428.089966  1429.239990    1429.239990         0.0   \n",
       "\n",
       "                         DJI_Open      DJI_High       DJI_Low     DJI_Close  \\\n",
       "Datetime                                                                      \n",
       "2024-01-05 12:15:00  37375.929688  37385.261719  37347.480469  37350.468750   \n",
       "2024-01-05 14:15:00  37506.800781  37511.488281  37467.289062  37474.859375   \n",
       "2024-01-05 14:30:00  37474.410156  37500.070312  37440.960938  37448.769531   \n",
       "2024-01-05 14:45:00  37449.289062  37452.988281  37377.230469  37386.449219   \n",
       "2024-01-05 15:00:00  37386.281250  37427.039062  37380.628906  37415.089844   \n",
       "\n",
       "                     DJI_Adj Close  DJI_Volume   SP500_Open   SP500_High  \\\n",
       "Datetime                                                                   \n",
       "2024-01-05 12:15:00   37350.468750   6434746.0  4695.419922  4696.839844   \n",
       "2024-01-05 14:15:00   37474.859375  11272802.0  4704.069824  4705.069824   \n",
       "2024-01-05 14:30:00   37448.769531   6859581.0  4699.270020  4701.819824   \n",
       "2024-01-05 14:45:00   37386.449219   7070748.0  4694.640137  4695.399902   \n",
       "2024-01-05 15:00:00   37415.089844   7259176.0  4685.109863  4689.899902   \n",
       "\n",
       "                       SP500_Low  SP500_Close  SP500_Adj Close  SP500_Volume  \\\n",
       "Datetime                                                                       \n",
       "2024-01-05 12:15:00  4690.100098  4690.700195      4690.700195    45079284.0   \n",
       "2024-01-05 14:15:00  4698.129883  4699.270020      4699.270020    51791000.0   \n",
       "2024-01-05 14:30:00  4693.850098  4694.580078      4694.580078    48581000.0   \n",
       "2024-01-05 14:45:00  4684.069824  4685.120117      4685.120117    53007000.0   \n",
       "2024-01-05 15:00:00  4684.180176  4688.799805      4688.799805    54990000.0   \n",
       "\n",
       "                     SET50_Open  SET50_High   SET50_Low  SET50_Close  \\\n",
       "Datetime                                                               \n",
       "2024-01-05 12:15:00  877.049988  877.150024  875.289978   876.789978   \n",
       "2024-01-05 14:15:00  876.840027  876.840027  876.760010   876.760010   \n",
       "2024-01-05 14:30:00  877.539978  877.539978  875.760010   876.210022   \n",
       "2024-01-05 14:45:00  877.159973  877.520020  874.200012   875.460022   \n",
       "2024-01-05 15:00:00  875.700012  876.770020  874.820007   875.679993   \n",
       "\n",
       "                     SET50_Adj Close  SET50_Volume  SET100_Open  SET100_High  \\\n",
       "Datetime                                                                       \n",
       "2024-01-05 12:15:00       876.789978           0.0  1945.989990  1947.319946   \n",
       "2024-01-05 14:15:00       876.760010           0.0  1946.180054  1946.180054   \n",
       "2024-01-05 14:30:00       876.210022           0.0  1947.640015  1947.640015   \n",
       "2024-01-05 14:45:00       875.460022           0.0  1945.939941  1946.819946   \n",
       "2024-01-05 15:00:00       875.679993           0.0  1941.839966  1945.109985   \n",
       "\n",
       "                      SET100_Low  SET100_Close  SET100_Adj Close  \\\n",
       "Datetime                                                           \n",
       "2024-01-05 12:15:00  1943.250000   1945.589966       1945.589966   \n",
       "2024-01-05 14:15:00  1946.010010   1946.010010       1946.010010   \n",
       "2024-01-05 14:30:00  1944.449951   1944.449951       1944.449951   \n",
       "2024-01-05 14:45:00  1941.170044   1942.750000       1942.750000   \n",
       "2024-01-05 15:00:00  1941.579956   1943.760010       1943.760010   \n",
       "\n",
       "                     SET100_Volume  SMA_5     EMA_5  Stoch_%K_5  Stoch_%D_5  \\\n",
       "Datetime                                                                      \n",
       "2024-01-05 12:15:00            0.0   5.24  5.248453        50.0   50.000000   \n",
       "2024-01-05 14:15:00            0.0   5.24  5.232302         0.0   33.333333   \n",
       "2024-01-05 14:30:00            0.0   5.23  5.221534         0.0   16.666667   \n",
       "2024-01-05 14:45:00            0.0   5.22  5.214356         0.0    0.000000   \n",
       "2024-01-05 15:00:00            0.0   5.21  5.209571         0.0    0.000000   \n",
       "\n",
       "                         RSI_5  WPR_5     ATR_5       CCI_5  Bollinger_mavg_5  \\\n",
       "Datetime                                                                        \n",
       "2024-01-05 12:15:00  48.060458  -50.0  0.068006  -41.666667              5.24   \n",
       "2024-01-05 14:15:00  28.125136 -100.0  0.064405 -166.666667              5.24   \n",
       "2024-01-05 14:30:00  28.125136 -100.0  0.061524  -83.333333              5.23   \n",
       "2024-01-05 14:45:00  28.125136 -100.0  0.059219  -55.555556              5.22   \n",
       "2024-01-05 15:00:00  28.125136 -100.0  0.057375  -41.666667              5.21   \n",
       "\n",
       "                     Bollinger_hband_5  Bollinger_lband_5  SMA_10    EMA_10  \\\n",
       "Datetime                                                                      \n",
       "2024-01-05 12:15:00            5.28000            5.20000   5.250  5.253827   \n",
       "2024-01-05 14:15:00            5.28000            5.20000   5.245  5.244040   \n",
       "2024-01-05 14:30:00            5.27899            5.18101   5.235  5.236033   \n",
       "2024-01-05 14:45:00            5.26899            5.17101   5.230  5.229481   \n",
       "2024-01-05 15:00:00            5.25000            5.17000   5.225  5.224121   \n",
       "\n",
       "                     Stoch_%K_10  Stoch_%D_10     RSI_10  WPR_10    ATR_10  \\\n",
       "Datetime                                                                     \n",
       "2024-01-05 12:15:00         50.0    38.888960  47.807249   -50.0  0.067443   \n",
       "2024-01-05 14:15:00          0.0    27.777813  38.561730  -100.0  0.065699   \n",
       "2024-01-05 14:30:00          0.0    16.666667  38.561730  -100.0  0.064129   \n",
       "2024-01-05 14:45:00          0.0     0.000000  38.561730  -100.0  0.062716   \n",
       "2024-01-05 15:00:00          0.0     0.000000  38.561730  -100.0  0.061444   \n",
       "\n",
       "                         CCI_10  Bollinger_mavg_10  Bollinger_hband_10  \\\n",
       "Datetime                                                                 \n",
       "2024-01-05 12:15:00  -83.333333              5.250            5.294722   \n",
       "2024-01-05 14:15:00 -125.925926              5.245            5.298852   \n",
       "2024-01-05 14:30:00 -106.060606              5.235            5.280826   \n",
       "2024-01-05 14:45:00  -99.099099              5.230            5.278990   \n",
       "2024-01-05 15:00:00  -83.333333              5.225            5.275000   \n",
       "\n",
       "                     Bollinger_lband_10    SMA_15    EMA_15  Stoch_%K_15  \\\n",
       "Datetime                                                                   \n",
       "2024-01-05 12:15:00            5.205278  5.270000  5.257645    33.333439   \n",
       "2024-01-05 14:15:00            5.191148  5.263333  5.250440     0.000000   \n",
       "2024-01-05 14:30:00            5.189174  5.256667  5.244135     0.000000   \n",
       "2024-01-05 14:45:00            5.181010  5.250000  5.238618     0.000000   \n",
       "2024-01-05 15:00:00            5.175000  5.243333  5.233791     0.000000   \n",
       "\n",
       "                     Stoch_%D_15     RSI_15      WPR_15    ATR_15      CCI_15  \\\n",
       "Datetime                                                                        \n",
       "2024-01-05 12:15:00    33.333439  48.621594  -66.666561  0.066056 -100.694465   \n",
       "2024-01-05 14:15:00    22.222293  42.692269 -100.000000  0.064986 -127.451029   \n",
       "2024-01-05 14:30:00    11.111146  42.692269 -100.000000  0.063987 -111.842155   \n",
       "2024-01-05 14:45:00     0.000000  42.692269 -100.000000  0.063054 -100.000025   \n",
       "2024-01-05 15:00:00     0.000000  42.692269 -100.000000  0.062184  -87.837864   \n",
       "\n",
       "                     Bollinger_mavg_15  Bollinger_hband_15  \\\n",
       "Datetime                                                     \n",
       "2024-01-05 12:15:00           5.270000            5.341181   \n",
       "2024-01-05 14:15:00           5.263333            5.340506   \n",
       "2024-01-05 14:30:00           5.256667            5.337220   \n",
       "2024-01-05 14:45:00           5.250000            5.331650   \n",
       "2024-01-05 15:00:00           5.243333            5.323887   \n",
       "\n",
       "                     Bollinger_lband_15  SMA_20    EMA_20  Stoch_%K_20  \\\n",
       "Datetime                                                                 \n",
       "2024-01-05 12:15:00            5.198819  5.2675  5.259378    33.333439   \n",
       "2024-01-05 14:15:00            5.186161  5.2650  5.253723     0.000000   \n",
       "2024-01-05 14:30:00            5.176113  5.2650  5.248607     0.000000   \n",
       "2024-01-05 14:45:00            5.168350  5.2625  5.243978     0.000000   \n",
       "2024-01-05 15:00:00            5.162779  5.2575  5.239789     0.000000   \n",
       "\n",
       "                     Stoch_%D_20     RSI_20      WPR_20    ATR_20      CCI_20  \\\n",
       "Datetime                                                                        \n",
       "2024-01-05 12:15:00    33.333439  49.175232  -66.666561  0.064787  -79.096051   \n",
       "2024-01-05 14:15:00    22.222293  44.826938 -100.000000  0.064048 -117.647108   \n",
       "2024-01-05 14:30:00    11.111146  44.826938 -100.000000  0.063345 -117.647108   \n",
       "2024-01-05 14:45:00     0.000000  44.826938 -100.000000  0.062678 -111.461660   \n",
       "2024-01-05 15:00:00     0.000000  44.826938 -100.000000  0.062044  -98.989936   \n",
       "\n",
       "                     Bollinger_mavg_20  Bollinger_hband_20  \\\n",
       "Datetime                                                     \n",
       "2024-01-05 12:15:00             5.2675            5.340129   \n",
       "2024-01-05 14:15:00             5.2650            5.343103   \n",
       "2024-01-05 14:30:00             5.2650            5.343103   \n",
       "2024-01-05 14:45:00             5.2625            5.345416   \n",
       "2024-01-05 15:00:00             5.2575            5.342794   \n",
       "\n",
       "                     Bollinger_lband_20  SMA_25    EMA_25  Stoch_%K_25  \\\n",
       "Datetime                                                                 \n",
       "2024-01-05 12:15:00            5.194871   5.264  5.259965    33.333439   \n",
       "2024-01-05 14:15:00            5.186897   5.262  5.255352     0.000000   \n",
       "2024-01-05 14:30:00            5.186897   5.260  5.251094     0.000000   \n",
       "2024-01-05 14:45:00            5.179584   5.258  5.247164     0.000000   \n",
       "2024-01-05 15:00:00            5.172206   5.256  5.243536     0.000000   \n",
       "\n",
       "                     Stoch_%D_25     RSI_25      WPR_25    ATR_25      CCI_25  \\\n",
       "Datetime                                                                        \n",
       "2024-01-05 12:15:00    33.333439  49.570490  -66.666561  0.063506  -73.302486   \n",
       "2024-01-05 14:15:00    22.222293  46.119618 -100.000000  0.062966 -111.967756   \n",
       "2024-01-05 14:30:00    11.111146  46.119618 -100.000000  0.062447 -107.526937   \n",
       "2024-01-05 14:45:00     0.000000  46.119618 -100.000000  0.061949  -99.247135   \n",
       "2024-01-05 15:00:00     0.000000  46.119618 -100.000000  0.061471  -91.683072   \n",
       "\n",
       "                     Bollinger_mavg_25  Bollinger_hband_25  \\\n",
       "Datetime                                                     \n",
       "2024-01-05 12:15:00              5.264            5.330453   \n",
       "2024-01-05 14:15:00              5.262            5.332880   \n",
       "2024-01-05 14:30:00              5.260            5.334833   \n",
       "2024-01-05 14:45:00              5.258            5.336384   \n",
       "2024-01-05 15:00:00              5.256            5.337585   \n",
       "\n",
       "                     Bollinger_lband_25      MACD  MACD_signal  MACD_diff  \\\n",
       "Datetime                                                                    \n",
       "2024-01-05 12:15:00            5.197547 -0.004313     0.000117  -0.004430   \n",
       "2024-01-05 14:15:00            5.191120 -0.008435    -0.001594  -0.006841   \n",
       "2024-01-05 14:30:00            5.185167 -0.011569    -0.003589  -0.007980   \n",
       "2024-01-05 14:45:00            5.179616 -0.013892    -0.005649  -0.008242   \n",
       "2024-01-05 15:00:00            5.174415 -0.015553    -0.007630  -0.007923   \n",
       "\n",
       "                     Momentum       ROC          OBV  Close_pct_change  \\\n",
       "Datetime                                                                 \n",
       "2024-01-05 12:15:00      0.05  0.009615  215753307.0          0.000000   \n",
       "2024-01-05 14:15:00     -0.05 -0.009524  215162507.0         -0.009524   \n",
       "2024-01-05 14:30:00     -0.05 -0.009524  216438737.0          0.000000   \n",
       "2024-01-05 14:45:00     -0.05 -0.009524  216554137.0          0.000000   \n",
       "2024-01-05 15:00:00     -0.05 -0.009524  216722637.0          0.000000   \n",
       "\n",
       "                     hour_of_day  day_of_week  y  \n",
       "Datetime                                          \n",
       "2024-01-05 12:15:00           12            4 -1  \n",
       "2024-01-05 14:15:00           14            4 -1  \n",
       "2024-01-05 14:30:00           14            4 -1  \n",
       "2024-01-05 14:45:00           14            4 -1  \n",
       "2024-01-05 15:00:00           15            4 -1  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['y'] = np.where(df[f'{stock_name}_Close'].shift(-1) > df[f'{stock_name}_Close'],1,-1)\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SET_Volume', 'SET50_Volume', 'SET100_Volume']\n",
      "['RSI_20', 'SET_Open', 'DJI_Close', 'Gold_Adj Close', 'WPR_5', 'ATR_20', 'ROC', 'RSI_25', 'ERW.BK_Adj Close', 'Oil_Low', 'Gold_Open', 'SET100_High', 'ERW.BK_High', 'Stoch_%D_15', 'WPR_20', 'ATR_10', 'SMA_25', 'Gold_High', 'Stoch_%K_25', 'SP500_Volume', 'SET100_Adj Close', 'SP500_Close', 'ERW.BK_Close', 'RSI_10', 'SP500_Adj Close', 'Close_pct_change', 'day_of_week', 'SP500_High', 'SP500_Open', 'EMA_15', 'Stoch_%K_10', 'Bollinger_hband_25', 'SET_Adj Close', 'CCI_20', 'RSI_15', 'SET_Low', 'Bollinger_mavg_5', 'WPR_25', 'SMA_10', 'SET50_Close', 'ATR_5', 'Bollinger_mavg_10', 'Oil_Adj Close', 'Bollinger_hband_15', 'Bollinger_lband_15', 'Gold_Volume', 'SET100_Close', 'SMA_5', 'y', 'SET50_Open', 'Bollinger_lband_10', 'SET_High', 'SET50_Adj Close', 'DJI_Adj Close', 'ATR_15', 'Stoch_%D_20', 'SET50_Low', 'EMA_25', 'WPR_15', 'Stoch_%K_5', 'hour_of_day', 'Stoch_%D_10', 'Gold_Low', 'Bollinger_hband_20', 'CCI_5', 'EMA_5', 'OBV', 'RSI_5', 'CCI_10', 'MACD_signal', 'ERW.BK_Open', 'SMA_20', 'ERW.BK_Low', 'Bollinger_lband_5', 'Oil_Volume', 'DJI_Open', 'DJI_Volume', 'SMA_15', 'Bollinger_lband_20', 'WPR_10', 'ATR_25', 'CCI_15', 'DJI_High', 'Stoch_%K_15', 'EMA_10', 'Gold_Close', 'SP500_Low', 'SET50_High', 'Bollinger_mavg_25', 'DJI_Low', 'Stoch_%D_5', 'Bollinger_lband_25', 'MACD', 'Oil_Close', 'SET_Close', 'Momentum', 'EMA_20', 'Oil_High', 'Bollinger_hband_10', 'SET100_Open', 'MACD_diff', 'Oil_Open', 'CCI_25', 'Bollinger_mavg_20', 'SET100_Low', 'Bollinger_mavg_15', 'Stoch_%D_25', 'Stoch_%K_20', 'ERW.BK_Volume', 'Bollinger_hband_5']\n"
     ]
    }
   ],
   "source": [
    "# Filter columns that's too unique\n",
    "no_variation_cols = []\n",
    "for i in df.columns:\n",
    "    # print( i ,':',len(df[i].unique()) )\n",
    "    if (len(df[i].unique()) <= 4) & (i != 'y'):\n",
    "        no_variation_cols.append(i)\n",
    "print(no_variation_cols)\n",
    "\n",
    "selected_cols = list(set(df.columns) - set(no_variation_cols))\n",
    "df = df[selected_cols]\n",
    "df.head()\n",
    "print(selected_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n",
      "-1    422\n",
      " 1    109\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ERW.BK_Close</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-11-16 15:15:00</th>\n",
       "      <td>5.40</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-16 15:30:00</th>\n",
       "      <td>5.40</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-16 15:45:00</th>\n",
       "      <td>5.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-17 10:00:00</th>\n",
       "      <td>5.45</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-17 10:15:00</th>\n",
       "      <td>5.45</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-17 10:30:00</th>\n",
       "      <td>5.40</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-17 10:45:00</th>\n",
       "      <td>5.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ERW.BK_Close  y\n",
       "Datetime                            \n",
       "2023-11-16 15:15:00          5.40 -1\n",
       "2023-11-16 15:30:00          5.40 -1\n",
       "2023-11-16 15:45:00          5.40  1\n",
       "2023-11-17 10:00:00          5.45 -1\n",
       "2023-11-17 10:15:00          5.45 -1\n",
       "2023-11-17 10:30:00          5.40 -1\n",
       "2023-11-17 10:45:00          5.40  1"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df['y'].value_counts())\n",
    "\n",
    "# Check whether y column is correct or not\n",
    "rand_row = np.random.randint(1,100)\n",
    "df[[close_col,'y']].iloc[rand_row : rand_row +7 , :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF(Scale, Ft selection )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531 0\n"
     ]
    }
   ],
   "source": [
    "split_index = int( 1 * len(df))\n",
    "df_test = df.iloc[split_index:]\n",
    "df = df.iloc[:split_index]\n",
    "print(len(df) , len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RSI_20</th>\n",
       "      <th>SET_Open</th>\n",
       "      <th>DJI_Close</th>\n",
       "      <th>Gold_Adj Close</th>\n",
       "      <th>WPR_5</th>\n",
       "      <th>ATR_20</th>\n",
       "      <th>ROC</th>\n",
       "      <th>RSI_25</th>\n",
       "      <th>ERW.BK_Adj Close</th>\n",
       "      <th>Oil_Low</th>\n",
       "      <th>Gold_Open</th>\n",
       "      <th>SET100_High</th>\n",
       "      <th>ERW.BK_High</th>\n",
       "      <th>Stoch_%D_15</th>\n",
       "      <th>WPR_20</th>\n",
       "      <th>ATR_10</th>\n",
       "      <th>SMA_25</th>\n",
       "      <th>Gold_High</th>\n",
       "      <th>Stoch_%K_25</th>\n",
       "      <th>SP500_Volume</th>\n",
       "      <th>SET100_Adj Close</th>\n",
       "      <th>SP500_Close</th>\n",
       "      <th>ERW.BK_Close</th>\n",
       "      <th>RSI_10</th>\n",
       "      <th>SP500_Adj Close</th>\n",
       "      <th>Close_pct_change</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>SP500_High</th>\n",
       "      <th>SP500_Open</th>\n",
       "      <th>EMA_15</th>\n",
       "      <th>Stoch_%K_10</th>\n",
       "      <th>Bollinger_hband_25</th>\n",
       "      <th>SET_Adj Close</th>\n",
       "      <th>CCI_20</th>\n",
       "      <th>RSI_15</th>\n",
       "      <th>SET_Low</th>\n",
       "      <th>Bollinger_mavg_5</th>\n",
       "      <th>WPR_25</th>\n",
       "      <th>SMA_10</th>\n",
       "      <th>SET50_Close</th>\n",
       "      <th>ATR_5</th>\n",
       "      <th>Bollinger_mavg_10</th>\n",
       "      <th>Oil_Adj Close</th>\n",
       "      <th>Bollinger_hband_15</th>\n",
       "      <th>Bollinger_lband_15</th>\n",
       "      <th>Gold_Volume</th>\n",
       "      <th>SET100_Close</th>\n",
       "      <th>SMA_5</th>\n",
       "      <th>SET50_Open</th>\n",
       "      <th>Bollinger_lband_10</th>\n",
       "      <th>SET_High</th>\n",
       "      <th>SET50_Adj Close</th>\n",
       "      <th>DJI_Adj Close</th>\n",
       "      <th>ATR_15</th>\n",
       "      <th>Stoch_%D_20</th>\n",
       "      <th>SET50_Low</th>\n",
       "      <th>EMA_25</th>\n",
       "      <th>WPR_15</th>\n",
       "      <th>Stoch_%K_5</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>Stoch_%D_10</th>\n",
       "      <th>Gold_Low</th>\n",
       "      <th>Bollinger_hband_20</th>\n",
       "      <th>CCI_5</th>\n",
       "      <th>EMA_5</th>\n",
       "      <th>OBV</th>\n",
       "      <th>RSI_5</th>\n",
       "      <th>CCI_10</th>\n",
       "      <th>MACD_signal</th>\n",
       "      <th>ERW.BK_Open</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>ERW.BK_Low</th>\n",
       "      <th>Bollinger_lband_5</th>\n",
       "      <th>Oil_Volume</th>\n",
       "      <th>DJI_Open</th>\n",
       "      <th>DJI_Volume</th>\n",
       "      <th>SMA_15</th>\n",
       "      <th>Bollinger_lband_20</th>\n",
       "      <th>WPR_10</th>\n",
       "      <th>ATR_25</th>\n",
       "      <th>CCI_15</th>\n",
       "      <th>DJI_High</th>\n",
       "      <th>Stoch_%K_15</th>\n",
       "      <th>EMA_10</th>\n",
       "      <th>Gold_Close</th>\n",
       "      <th>SP500_Low</th>\n",
       "      <th>SET50_High</th>\n",
       "      <th>Bollinger_mavg_25</th>\n",
       "      <th>DJI_Low</th>\n",
       "      <th>Stoch_%D_5</th>\n",
       "      <th>Bollinger_lband_25</th>\n",
       "      <th>MACD</th>\n",
       "      <th>Oil_Close</th>\n",
       "      <th>SET_Close</th>\n",
       "      <th>Momentum</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>Oil_High</th>\n",
       "      <th>Bollinger_hband_10</th>\n",
       "      <th>SET100_Open</th>\n",
       "      <th>MACD_diff</th>\n",
       "      <th>Oil_Open</th>\n",
       "      <th>CCI_25</th>\n",
       "      <th>Bollinger_mavg_20</th>\n",
       "      <th>SET100_Low</th>\n",
       "      <th>Bollinger_mavg_15</th>\n",
       "      <th>Stoch_%D_25</th>\n",
       "      <th>Stoch_%K_20</th>\n",
       "      <th>ERW.BK_Volume</th>\n",
       "      <th>Bollinger_hband_5</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-11-10 15:15:00</th>\n",
       "      <td>0.110830</td>\n",
       "      <td>-0.020362</td>\n",
       "      <td>-0.919037</td>\n",
       "      <td>-1.770800</td>\n",
       "      <td>0.222224</td>\n",
       "      <td>-0.062453</td>\n",
       "      <td>1.054971</td>\n",
       "      <td>0.061302</td>\n",
       "      <td>-0.666666</td>\n",
       "      <td>0.856429</td>\n",
       "      <td>-1.764010</td>\n",
       "      <td>-0.082203</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.617635</td>\n",
       "      <td>0.617640</td>\n",
       "      <td>-0.026664</td>\n",
       "      <td>-0.973332</td>\n",
       "      <td>-1.796753</td>\n",
       "      <td>0.552629</td>\n",
       "      <td>0.263677</td>\n",
       "      <td>-0.075388</td>\n",
       "      <td>-1.005911</td>\n",
       "      <td>-0.666666</td>\n",
       "      <td>0.337707</td>\n",
       "      <td>-1.005911</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.020989</td>\n",
       "      <td>-1.025298</td>\n",
       "      <td>-0.914775</td>\n",
       "      <td>0.494113</td>\n",
       "      <td>-0.867709</td>\n",
       "      <td>-0.049760</td>\n",
       "      <td>0.422200</td>\n",
       "      <td>0.263879</td>\n",
       "      <td>-0.028633</td>\n",
       "      <td>-0.733332</td>\n",
       "      <td>0.552629</td>\n",
       "      <td>-0.903224</td>\n",
       "      <td>-0.040660</td>\n",
       "      <td>0.154376</td>\n",
       "      <td>-0.903224</td>\n",
       "      <td>0.834804</td>\n",
       "      <td>-0.827725</td>\n",
       "      <td>-1.089673</td>\n",
       "      <td>-0.516538</td>\n",
       "      <td>-0.075388</td>\n",
       "      <td>-0.733332</td>\n",
       "      <td>-0.035614</td>\n",
       "      <td>-1.067777</td>\n",
       "      <td>-0.055676</td>\n",
       "      <td>-0.040660</td>\n",
       "      <td>-0.919037</td>\n",
       "      <td>-0.049562</td>\n",
       "      <td>0.617641</td>\n",
       "      <td>-0.009006</td>\n",
       "      <td>-0.945674</td>\n",
       "      <td>0.617640</td>\n",
       "      <td>0.222224</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.529406</td>\n",
       "      <td>-1.814248</td>\n",
       "      <td>-0.836984</td>\n",
       "      <td>0.144490</td>\n",
       "      <td>-0.714325</td>\n",
       "      <td>-1.557053</td>\n",
       "      <td>0.378489</td>\n",
       "      <td>0.356885</td>\n",
       "      <td>-0.464162</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-0.983605</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-0.987373</td>\n",
       "      <td>-0.560295</td>\n",
       "      <td>-0.936588</td>\n",
       "      <td>0.430106</td>\n",
       "      <td>-1.021738</td>\n",
       "      <td>-0.958114</td>\n",
       "      <td>0.494113</td>\n",
       "      <td>-0.081234</td>\n",
       "      <td>0.549450</td>\n",
       "      <td>-0.920554</td>\n",
       "      <td>0.617640</td>\n",
       "      <td>-0.849909</td>\n",
       "      <td>-1.770800</td>\n",
       "      <td>-1.033988</td>\n",
       "      <td>-0.044376</td>\n",
       "      <td>-0.973332</td>\n",
       "      <td>-0.942608</td>\n",
       "      <td>0.379746</td>\n",
       "      <td>-0.898158</td>\n",
       "      <td>-0.031553</td>\n",
       "      <td>0.834804</td>\n",
       "      <td>-0.049760</td>\n",
       "      <td>1.000005</td>\n",
       "      <td>-0.918881</td>\n",
       "      <td>0.791615</td>\n",
       "      <td>-0.680257</td>\n",
       "      <td>-0.054431</td>\n",
       "      <td>1.270606</td>\n",
       "      <td>0.808355</td>\n",
       "      <td>0.352058</td>\n",
       "      <td>-0.983605</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-1.021738</td>\n",
       "      <td>0.617640</td>\n",
       "      <td>0.617640</td>\n",
       "      <td>1.675475</td>\n",
       "      <td>-0.440437</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-10 15:30:00</th>\n",
       "      <td>-0.351624</td>\n",
       "      <td>-0.047619</td>\n",
       "      <td>-0.911503</td>\n",
       "      <td>-1.772497</td>\n",
       "      <td>-0.666664</td>\n",
       "      <td>-0.077470</td>\n",
       "      <td>-1.024244</td>\n",
       "      <td>-0.429547</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>0.858927</td>\n",
       "      <td>-1.767405</td>\n",
       "      <td>-0.070220</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.323522</td>\n",
       "      <td>-0.264702</td>\n",
       "      <td>-0.060273</td>\n",
       "      <td>-0.973332</td>\n",
       "      <td>-1.810417</td>\n",
       "      <td>-0.236840</td>\n",
       "      <td>1.057486</td>\n",
       "      <td>-0.034505</td>\n",
       "      <td>-0.994092</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-0.220242</td>\n",
       "      <td>-0.994092</td>\n",
       "      <td>-0.009804</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.009347</td>\n",
       "      <td>-0.995876</td>\n",
       "      <td>-0.929731</td>\n",
       "      <td>-0.399996</td>\n",
       "      <td>-0.867709</td>\n",
       "      <td>-0.023178</td>\n",
       "      <td>0.209864</td>\n",
       "      <td>-0.213607</td>\n",
       "      <td>-0.030010</td>\n",
       "      <td>-0.666665</td>\n",
       "      <td>-0.236840</td>\n",
       "      <td>-0.870966</td>\n",
       "      <td>0.025922</td>\n",
       "      <td>0.055976</td>\n",
       "      <td>-0.870966</td>\n",
       "      <td>0.839849</td>\n",
       "      <td>-0.828205</td>\n",
       "      <td>-1.047384</td>\n",
       "      <td>-0.394792</td>\n",
       "      <td>-0.034505</td>\n",
       "      <td>-0.666665</td>\n",
       "      <td>-0.052168</td>\n",
       "      <td>-0.967777</td>\n",
       "      <td>-0.043697</td>\n",
       "      <td>0.025922</td>\n",
       "      <td>-0.911503</td>\n",
       "      <td>-0.067852</td>\n",
       "      <td>0.323526</td>\n",
       "      <td>-0.019015</td>\n",
       "      <td>-0.955782</td>\n",
       "      <td>-0.264702</td>\n",
       "      <td>-0.666664</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.210082</td>\n",
       "      <td>-1.817854</td>\n",
       "      <td>-0.867216</td>\n",
       "      <td>-0.516036</td>\n",
       "      <td>-0.800414</td>\n",
       "      <td>-1.557767</td>\n",
       "      <td>-0.243993</td>\n",
       "      <td>0.060823</td>\n",
       "      <td>-0.386506</td>\n",
       "      <td>-0.666666</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-0.707784</td>\n",
       "      <td>-0.420929</td>\n",
       "      <td>-0.913864</td>\n",
       "      <td>0.865141</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>-0.958572</td>\n",
       "      <td>-0.399996</td>\n",
       "      <td>-0.094826</td>\n",
       "      <td>0.279137</td>\n",
       "      <td>-0.909922</td>\n",
       "      <td>-0.264702</td>\n",
       "      <td>-0.878756</td>\n",
       "      <td>-1.772497</td>\n",
       "      <td>-1.000492</td>\n",
       "      <td>-0.041316</td>\n",
       "      <td>-0.973332</td>\n",
       "      <td>-0.915137</td>\n",
       "      <td>-0.126582</td>\n",
       "      <td>-0.898158</td>\n",
       "      <td>-0.094557</td>\n",
       "      <td>0.839849</td>\n",
       "      <td>-0.023178</td>\n",
       "      <td>-1.000005</td>\n",
       "      <td>-0.929962</td>\n",
       "      <td>0.781752</td>\n",
       "      <td>-0.711570</td>\n",
       "      <td>-0.086240</td>\n",
       "      <td>0.887726</td>\n",
       "      <td>0.805897</td>\n",
       "      <td>0.119089</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>-0.050304</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>0.323526</td>\n",
       "      <td>-0.264702</td>\n",
       "      <td>-0.168638</td>\n",
       "      <td>-0.599148</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-10 15:45:00</th>\n",
       "      <td>-0.351624</td>\n",
       "      <td>-0.027607</td>\n",
       "      <td>-0.910026</td>\n",
       "      <td>-1.765708</td>\n",
       "      <td>-0.666664</td>\n",
       "      <td>-0.091736</td>\n",
       "      <td>-0.517140</td>\n",
       "      <td>-0.429547</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>0.848939</td>\n",
       "      <td>-1.769102</td>\n",
       "      <td>-0.050569</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.029409</td>\n",
       "      <td>-0.264702</td>\n",
       "      <td>-0.090521</td>\n",
       "      <td>-0.973332</td>\n",
       "      <td>-1.805292</td>\n",
       "      <td>-0.236840</td>\n",
       "      <td>7.252600</td>\n",
       "      <td>-0.218835</td>\n",
       "      <td>-0.987138</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-0.220242</td>\n",
       "      <td>-0.987138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.991625</td>\n",
       "      <td>-0.984167</td>\n",
       "      <td>-0.942817</td>\n",
       "      <td>-0.399996</td>\n",
       "      <td>-0.867709</td>\n",
       "      <td>-0.203817</td>\n",
       "      <td>0.227894</td>\n",
       "      <td>-0.213607</td>\n",
       "      <td>-0.152123</td>\n",
       "      <td>-0.799998</td>\n",
       "      <td>-0.236840</td>\n",
       "      <td>-0.870966</td>\n",
       "      <td>-0.154510</td>\n",
       "      <td>-0.022744</td>\n",
       "      <td>-0.870966</td>\n",
       "      <td>0.829761</td>\n",
       "      <td>-0.828205</td>\n",
       "      <td>-1.047384</td>\n",
       "      <td>-0.364884</td>\n",
       "      <td>-0.218835</td>\n",
       "      <td>-0.799998</td>\n",
       "      <td>-0.016053</td>\n",
       "      <td>-0.967777</td>\n",
       "      <td>-0.041580</td>\n",
       "      <td>-0.154510</td>\n",
       "      <td>-0.910026</td>\n",
       "      <td>-0.084923</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>-0.104580</td>\n",
       "      <td>-0.965113</td>\n",
       "      <td>-0.264702</td>\n",
       "      <td>-0.666664</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.109242</td>\n",
       "      <td>-1.817854</td>\n",
       "      <td>-0.867216</td>\n",
       "      <td>-0.433470</td>\n",
       "      <td>-0.857807</td>\n",
       "      <td>-1.551704</td>\n",
       "      <td>-0.243993</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>-0.335272</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-0.743684</td>\n",
       "      <td>-0.546626</td>\n",
       "      <td>-0.906467</td>\n",
       "      <td>7.293764</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>-0.958572</td>\n",
       "      <td>-0.399996</td>\n",
       "      <td>-0.107874</td>\n",
       "      <td>0.242361</td>\n",
       "      <td>-0.900246</td>\n",
       "      <td>-0.264702</td>\n",
       "      <td>-0.902359</td>\n",
       "      <td>-1.765708</td>\n",
       "      <td>-0.987359</td>\n",
       "      <td>-0.010713</td>\n",
       "      <td>-0.973332</td>\n",
       "      <td>-0.910114</td>\n",
       "      <td>-0.632911</td>\n",
       "      <td>-0.898158</td>\n",
       "      <td>-0.143856</td>\n",
       "      <td>0.829761</td>\n",
       "      <td>-0.203817</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.939988</td>\n",
       "      <td>0.791615</td>\n",
       "      <td>-0.711570</td>\n",
       "      <td>-0.062442</td>\n",
       "      <td>0.612911</td>\n",
       "      <td>0.810812</td>\n",
       "      <td>0.110172</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>-0.166745</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>-0.264702</td>\n",
       "      <td>0.309089</td>\n",
       "      <td>-0.818958</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-13 10:00:00</th>\n",
       "      <td>-0.686194</td>\n",
       "      <td>-0.250518</td>\n",
       "      <td>-0.922874</td>\n",
       "      <td>-1.770800</td>\n",
       "      <td>-0.666664</td>\n",
       "      <td>-0.130391</td>\n",
       "      <td>-0.522265</td>\n",
       "      <td>-0.785197</td>\n",
       "      <td>-1.333331</td>\n",
       "      <td>0.968788</td>\n",
       "      <td>-1.825130</td>\n",
       "      <td>-0.329059</td>\n",
       "      <td>-1.333331</td>\n",
       "      <td>-0.852936</td>\n",
       "      <td>-1.147052</td>\n",
       "      <td>-0.164288</td>\n",
       "      <td>-1.026665</td>\n",
       "      <td>-1.801877</td>\n",
       "      <td>-1.026316</td>\n",
       "      <td>0.787946</td>\n",
       "      <td>-0.554179</td>\n",
       "      <td>-1.075261</td>\n",
       "      <td>-1.333331</td>\n",
       "      <td>-0.582732</td>\n",
       "      <td>-1.075261</td>\n",
       "      <td>-0.009901</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.090045</td>\n",
       "      <td>-1.080851</td>\n",
       "      <td>-1.044214</td>\n",
       "      <td>-1.199996</td>\n",
       "      <td>-0.906987</td>\n",
       "      <td>-0.527948</td>\n",
       "      <td>-0.393131</td>\n",
       "      <td>-0.552757</td>\n",
       "      <td>-0.514662</td>\n",
       "      <td>-1.133330</td>\n",
       "      <td>-1.026316</td>\n",
       "      <td>-0.935482</td>\n",
       "      <td>-0.452352</td>\n",
       "      <td>-0.176394</td>\n",
       "      <td>-0.935482</td>\n",
       "      <td>0.970996</td>\n",
       "      <td>-0.832616</td>\n",
       "      <td>-1.000991</td>\n",
       "      <td>1.971851</td>\n",
       "      <td>-0.554179</td>\n",
       "      <td>-1.133330</td>\n",
       "      <td>-0.222222</td>\n",
       "      <td>-1.154308</td>\n",
       "      <td>-0.305498</td>\n",
       "      <td>-0.452352</td>\n",
       "      <td>-0.922874</td>\n",
       "      <td>-0.129600</td>\n",
       "      <td>-0.852936</td>\n",
       "      <td>-0.443833</td>\n",
       "      <td>-1.037601</td>\n",
       "      <td>-1.147052</td>\n",
       "      <td>-0.666664</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-1.888188</td>\n",
       "      <td>-0.935766</td>\n",
       "      <td>-0.361227</td>\n",
       "      <td>-1.091203</td>\n",
       "      <td>-1.559911</td>\n",
       "      <td>-0.461607</td>\n",
       "      <td>-0.508939</td>\n",
       "      <td>-0.357831</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-1.065572</td>\n",
       "      <td>-1.333331</td>\n",
       "      <td>-1.056186</td>\n",
       "      <td>0.538927</td>\n",
       "      <td>-0.920820</td>\n",
       "      <td>0.912056</td>\n",
       "      <td>-0.978259</td>\n",
       "      <td>-1.011770</td>\n",
       "      <td>-1.199996</td>\n",
       "      <td>-0.143966</td>\n",
       "      <td>-0.536075</td>\n",
       "      <td>-0.916463</td>\n",
       "      <td>-1.147052</td>\n",
       "      <td>-1.047163</td>\n",
       "      <td>-1.770800</td>\n",
       "      <td>-1.094210</td>\n",
       "      <td>-0.269828</td>\n",
       "      <td>-1.026665</td>\n",
       "      <td>-0.931641</td>\n",
       "      <td>-0.759490</td>\n",
       "      <td>-0.956260</td>\n",
       "      <td>-0.448707</td>\n",
       "      <td>0.970996</td>\n",
       "      <td>-0.527948</td>\n",
       "      <td>-0.500005</td>\n",
       "      <td>-1.021933</td>\n",
       "      <td>0.971640</td>\n",
       "      <td>-0.661597</td>\n",
       "      <td>-0.277097</td>\n",
       "      <td>-0.204041</td>\n",
       "      <td>0.953319</td>\n",
       "      <td>-0.479612</td>\n",
       "      <td>-1.065572</td>\n",
       "      <td>-0.535398</td>\n",
       "      <td>-0.978259</td>\n",
       "      <td>-0.852935</td>\n",
       "      <td>-1.147052</td>\n",
       "      <td>0.749866</td>\n",
       "      <td>-1.139789</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-13 10:15:00</th>\n",
       "      <td>-0.686194</td>\n",
       "      <td>-0.510008</td>\n",
       "      <td>-0.931499</td>\n",
       "      <td>-1.779289</td>\n",
       "      <td>-0.666664</td>\n",
       "      <td>-0.142009</td>\n",
       "      <td>-0.522265</td>\n",
       "      <td>-0.785197</td>\n",
       "      <td>-1.333331</td>\n",
       "      <td>0.986266</td>\n",
       "      <td>-1.770800</td>\n",
       "      <td>-0.521510</td>\n",
       "      <td>-1.333331</td>\n",
       "      <td>-0.970582</td>\n",
       "      <td>-1.147052</td>\n",
       "      <td>-0.184131</td>\n",
       "      <td>-1.053332</td>\n",
       "      <td>-1.796753</td>\n",
       "      <td>-1.026316</td>\n",
       "      <td>0.248469</td>\n",
       "      <td>-0.624129</td>\n",
       "      <td>-1.073580</td>\n",
       "      <td>-1.333331</td>\n",
       "      <td>-0.582732</td>\n",
       "      <td>-1.073580</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.087466</td>\n",
       "      <td>-1.064871</td>\n",
       "      <td>-1.083293</td>\n",
       "      <td>-1.199996</td>\n",
       "      <td>-0.933533</td>\n",
       "      <td>-0.597138</td>\n",
       "      <td>-0.393131</td>\n",
       "      <td>-0.552757</td>\n",
       "      <td>-0.557090</td>\n",
       "      <td>-1.199997</td>\n",
       "      <td>-1.026316</td>\n",
       "      <td>-0.935482</td>\n",
       "      <td>-0.517917</td>\n",
       "      <td>-0.208635</td>\n",
       "      <td>-0.935482</td>\n",
       "      <td>0.993695</td>\n",
       "      <td>-0.832616</td>\n",
       "      <td>-1.000991</td>\n",
       "      <td>0.754046</td>\n",
       "      <td>-0.624129</td>\n",
       "      <td>-1.199997</td>\n",
       "      <td>-0.439429</td>\n",
       "      <td>-1.154308</td>\n",
       "      <td>-0.508106</td>\n",
       "      <td>-0.517917</td>\n",
       "      <td>-0.931499</td>\n",
       "      <td>-0.142553</td>\n",
       "      <td>-0.852936</td>\n",
       "      <td>-0.467352</td>\n",
       "      <td>-1.066871</td>\n",
       "      <td>-1.499990</td>\n",
       "      <td>-0.666664</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-1.834086</td>\n",
       "      <td>-0.935766</td>\n",
       "      <td>-0.240817</td>\n",
       "      <td>-1.157326</td>\n",
       "      <td>-1.559508</td>\n",
       "      <td>-0.461607</td>\n",
       "      <td>-0.508939</td>\n",
       "      <td>-0.401695</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-1.065572</td>\n",
       "      <td>-1.333331</td>\n",
       "      <td>-1.118687</td>\n",
       "      <td>0.415586</td>\n",
       "      <td>-0.918008</td>\n",
       "      <td>0.432906</td>\n",
       "      <td>-0.978259</td>\n",
       "      <td>-1.011770</td>\n",
       "      <td>-1.199996</td>\n",
       "      <td>-0.155048</td>\n",
       "      <td>-0.550208</td>\n",
       "      <td>-0.924478</td>\n",
       "      <td>-1.499990</td>\n",
       "      <td>-1.098111</td>\n",
       "      <td>-1.779289</td>\n",
       "      <td>-1.084665</td>\n",
       "      <td>-0.428464</td>\n",
       "      <td>-1.053332</td>\n",
       "      <td>-0.934845</td>\n",
       "      <td>-0.759490</td>\n",
       "      <td>-0.978360</td>\n",
       "      <td>-0.548342</td>\n",
       "      <td>0.993695</td>\n",
       "      <td>-0.597138</td>\n",
       "      <td>-0.500005</td>\n",
       "      <td>-1.054676</td>\n",
       "      <td>0.969174</td>\n",
       "      <td>-0.661597</td>\n",
       "      <td>-0.534874</td>\n",
       "      <td>-0.376163</td>\n",
       "      <td>0.936120</td>\n",
       "      <td>-0.444135</td>\n",
       "      <td>-1.065572</td>\n",
       "      <td>-0.569168</td>\n",
       "      <td>-0.978259</td>\n",
       "      <td>-0.852935</td>\n",
       "      <td>-1.147052</td>\n",
       "      <td>-0.196404</td>\n",
       "      <td>-1.203956</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       RSI_20  SET_Open  DJI_Close  Gold_Adj Close     WPR_5  \\\n",
       "Datetime                                                                       \n",
       "2023-11-10 15:15:00  0.110830 -0.020362  -0.919037       -1.770800  0.222224   \n",
       "2023-11-10 15:30:00 -0.351624 -0.047619  -0.911503       -1.772497 -0.666664   \n",
       "2023-11-10 15:45:00 -0.351624 -0.027607  -0.910026       -1.765708 -0.666664   \n",
       "2023-11-13 10:00:00 -0.686194 -0.250518  -0.922874       -1.770800 -0.666664   \n",
       "2023-11-13 10:15:00 -0.686194 -0.510008  -0.931499       -1.779289 -0.666664   \n",
       "\n",
       "                       ATR_20       ROC    RSI_25  ERW.BK_Adj Close   Oil_Low  \\\n",
       "Datetime                                                                        \n",
       "2023-11-10 15:15:00 -0.062453  1.054971  0.061302         -0.666666  0.856429   \n",
       "2023-11-10 15:30:00 -0.077470 -1.024244 -0.429547         -0.999997  0.858927   \n",
       "2023-11-10 15:45:00 -0.091736 -0.517140 -0.429547         -0.999997  0.848939   \n",
       "2023-11-13 10:00:00 -0.130391 -0.522265 -0.785197         -1.333331  0.968788   \n",
       "2023-11-13 10:15:00 -0.142009 -0.522265 -0.785197         -1.333331  0.986266   \n",
       "\n",
       "                     Gold_Open  SET100_High  ERW.BK_High  Stoch_%D_15  \\\n",
       "Datetime                                                                \n",
       "2023-11-10 15:15:00  -1.764010    -0.082203    -1.000000     0.617635   \n",
       "2023-11-10 15:30:00  -1.767405    -0.070220    -1.000000     0.323522   \n",
       "2023-11-10 15:45:00  -1.769102    -0.050569    -1.000000     0.029409   \n",
       "2023-11-13 10:00:00  -1.825130    -0.329059    -1.333331    -0.852936   \n",
       "2023-11-13 10:15:00  -1.770800    -0.521510    -1.333331    -0.970582   \n",
       "\n",
       "                       WPR_20    ATR_10    SMA_25  Gold_High  Stoch_%K_25  \\\n",
       "Datetime                                                                    \n",
       "2023-11-10 15:15:00  0.617640 -0.026664 -0.973332  -1.796753     0.552629   \n",
       "2023-11-10 15:30:00 -0.264702 -0.060273 -0.973332  -1.810417    -0.236840   \n",
       "2023-11-10 15:45:00 -0.264702 -0.090521 -0.973332  -1.805292    -0.236840   \n",
       "2023-11-13 10:00:00 -1.147052 -0.164288 -1.026665  -1.801877    -1.026316   \n",
       "2023-11-13 10:15:00 -1.147052 -0.184131 -1.053332  -1.796753    -1.026316   \n",
       "\n",
       "                     SP500_Volume  SET100_Adj Close  SP500_Close  \\\n",
       "Datetime                                                           \n",
       "2023-11-10 15:15:00      0.263677         -0.075388    -1.005911   \n",
       "2023-11-10 15:30:00      1.057486         -0.034505    -0.994092   \n",
       "2023-11-10 15:45:00      7.252600         -0.218835    -0.987138   \n",
       "2023-11-13 10:00:00      0.787946         -0.554179    -1.075261   \n",
       "2023-11-13 10:15:00      0.248469         -0.624129    -1.073580   \n",
       "\n",
       "                     ERW.BK_Close    RSI_10  SP500_Adj Close  \\\n",
       "Datetime                                                       \n",
       "2023-11-10 15:15:00     -0.666666  0.337707        -1.005911   \n",
       "2023-11-10 15:30:00     -0.999997 -0.220242        -0.994092   \n",
       "2023-11-10 15:45:00     -0.999997 -0.220242        -0.987138   \n",
       "2023-11-13 10:00:00     -1.333331 -0.582732        -1.075261   \n",
       "2023-11-13 10:15:00     -1.333331 -0.582732        -1.073580   \n",
       "\n",
       "                     Close_pct_change  day_of_week  SP500_High  SP500_Open  \\\n",
       "Datetime                                                                     \n",
       "2023-11-10 15:15:00          0.000000          1.0   -1.020989   -1.025298   \n",
       "2023-11-10 15:30:00         -0.009804          1.0   -1.009347   -0.995876   \n",
       "2023-11-10 15:45:00          0.000000          1.0   -0.991625   -0.984167   \n",
       "2023-11-13 10:00:00         -0.009901         -1.0   -1.090045   -1.080851   \n",
       "2023-11-13 10:15:00          0.000000         -1.0   -1.087466   -1.064871   \n",
       "\n",
       "                       EMA_15  Stoch_%K_10  Bollinger_hband_25  SET_Adj Close  \\\n",
       "Datetime                                                                        \n",
       "2023-11-10 15:15:00 -0.914775     0.494113           -0.867709      -0.049760   \n",
       "2023-11-10 15:30:00 -0.929731    -0.399996           -0.867709      -0.023178   \n",
       "2023-11-10 15:45:00 -0.942817    -0.399996           -0.867709      -0.203817   \n",
       "2023-11-13 10:00:00 -1.044214    -1.199996           -0.906987      -0.527948   \n",
       "2023-11-13 10:15:00 -1.083293    -1.199996           -0.933533      -0.597138   \n",
       "\n",
       "                       CCI_20    RSI_15   SET_Low  Bollinger_mavg_5    WPR_25  \\\n",
       "Datetime                                                                        \n",
       "2023-11-10 15:15:00  0.422200  0.263879 -0.028633         -0.733332  0.552629   \n",
       "2023-11-10 15:30:00  0.209864 -0.213607 -0.030010         -0.666665 -0.236840   \n",
       "2023-11-10 15:45:00  0.227894 -0.213607 -0.152123         -0.799998 -0.236840   \n",
       "2023-11-13 10:00:00 -0.393131 -0.552757 -0.514662         -1.133330 -1.026316   \n",
       "2023-11-13 10:15:00 -0.393131 -0.552757 -0.557090         -1.199997 -1.026316   \n",
       "\n",
       "                       SMA_10  SET50_Close     ATR_5  Bollinger_mavg_10  \\\n",
       "Datetime                                                                  \n",
       "2023-11-10 15:15:00 -0.903224    -0.040660  0.154376          -0.903224   \n",
       "2023-11-10 15:30:00 -0.870966     0.025922  0.055976          -0.870966   \n",
       "2023-11-10 15:45:00 -0.870966    -0.154510 -0.022744          -0.870966   \n",
       "2023-11-13 10:00:00 -0.935482    -0.452352 -0.176394          -0.935482   \n",
       "2023-11-13 10:15:00 -0.935482    -0.517917 -0.208635          -0.935482   \n",
       "\n",
       "                     Oil_Adj Close  Bollinger_hband_15  Bollinger_lband_15  \\\n",
       "Datetime                                                                     \n",
       "2023-11-10 15:15:00       0.834804           -0.827725           -1.089673   \n",
       "2023-11-10 15:30:00       0.839849           -0.828205           -1.047384   \n",
       "2023-11-10 15:45:00       0.829761           -0.828205           -1.047384   \n",
       "2023-11-13 10:00:00       0.970996           -0.832616           -1.000991   \n",
       "2023-11-13 10:15:00       0.993695           -0.832616           -1.000991   \n",
       "\n",
       "                     Gold_Volume  SET100_Close     SMA_5  SET50_Open  \\\n",
       "Datetime                                                               \n",
       "2023-11-10 15:15:00    -0.516538     -0.075388 -0.733332   -0.035614   \n",
       "2023-11-10 15:30:00    -0.394792     -0.034505 -0.666665   -0.052168   \n",
       "2023-11-10 15:45:00    -0.364884     -0.218835 -0.799998   -0.016053   \n",
       "2023-11-13 10:00:00     1.971851     -0.554179 -1.133330   -0.222222   \n",
       "2023-11-13 10:15:00     0.754046     -0.624129 -1.199997   -0.439429   \n",
       "\n",
       "                     Bollinger_lband_10  SET_High  SET50_Adj Close  \\\n",
       "Datetime                                                             \n",
       "2023-11-10 15:15:00           -1.067777 -0.055676        -0.040660   \n",
       "2023-11-10 15:30:00           -0.967777 -0.043697         0.025922   \n",
       "2023-11-10 15:45:00           -0.967777 -0.041580        -0.154510   \n",
       "2023-11-13 10:00:00           -1.154308 -0.305498        -0.452352   \n",
       "2023-11-13 10:15:00           -1.154308 -0.508106        -0.517917   \n",
       "\n",
       "                     DJI_Adj Close    ATR_15  Stoch_%D_20  SET50_Low  \\\n",
       "Datetime                                                               \n",
       "2023-11-10 15:15:00      -0.919037 -0.049562     0.617641  -0.009006   \n",
       "2023-11-10 15:30:00      -0.911503 -0.067852     0.323526  -0.019015   \n",
       "2023-11-10 15:45:00      -0.910026 -0.084923     0.029412  -0.104580   \n",
       "2023-11-13 10:00:00      -0.922874 -0.129600    -0.852936  -0.443833   \n",
       "2023-11-13 10:15:00      -0.931499 -0.142553    -0.852936  -0.467352   \n",
       "\n",
       "                       EMA_25    WPR_15  Stoch_%K_5  hour_of_day  Stoch_%D_10  \\\n",
       "Datetime                                                                        \n",
       "2023-11-10 15:15:00 -0.945674  0.617640    0.222224     1.000000     0.529406   \n",
       "2023-11-10 15:30:00 -0.955782 -0.264702   -0.666664     1.000000     0.210082   \n",
       "2023-11-10 15:45:00 -0.965113 -0.264702   -0.666664     1.000000    -0.109242   \n",
       "2023-11-13 10:00:00 -1.037601 -1.147052   -0.666664    -0.666667    -0.999993   \n",
       "2023-11-13 10:15:00 -1.066871 -1.499990   -0.666664    -0.666667    -0.999993   \n",
       "\n",
       "                     Gold_Low  Bollinger_hband_20     CCI_5     EMA_5  \\\n",
       "Datetime                                                                \n",
       "2023-11-10 15:15:00 -1.814248           -0.836984  0.144490 -0.714325   \n",
       "2023-11-10 15:30:00 -1.817854           -0.867216 -0.516036 -0.800414   \n",
       "2023-11-10 15:45:00 -1.817854           -0.867216 -0.433470 -0.857807   \n",
       "2023-11-13 10:00:00 -1.888188           -0.935766 -0.361227 -1.091203   \n",
       "2023-11-13 10:15:00 -1.834086           -0.935766 -0.240817 -1.157326   \n",
       "\n",
       "                          OBV     RSI_5    CCI_10  MACD_signal  ERW.BK_Open  \\\n",
       "Datetime                                                                      \n",
       "2023-11-10 15:15:00 -1.557053  0.378489  0.356885    -0.464162    -0.999997   \n",
       "2023-11-10 15:30:00 -1.557767 -0.243993  0.060823    -0.386506    -0.666666   \n",
       "2023-11-10 15:45:00 -1.551704 -0.243993  0.003378    -0.335272    -0.999997   \n",
       "2023-11-13 10:00:00 -1.559911 -0.461607 -0.508939    -0.357831    -0.999997   \n",
       "2023-11-13 10:15:00 -1.559508 -0.461607 -0.508939    -0.401695    -0.999997   \n",
       "\n",
       "                       SMA_20  ERW.BK_Low  Bollinger_lband_5  Oil_Volume  \\\n",
       "Datetime                                                                   \n",
       "2023-11-10 15:15:00 -0.983605   -0.999997          -0.987373   -0.560295   \n",
       "2023-11-10 15:30:00 -0.999999   -0.999997          -0.707784   -0.420929   \n",
       "2023-11-10 15:45:00 -0.999999   -0.999997          -0.743684   -0.546626   \n",
       "2023-11-13 10:00:00 -1.065572   -1.333331          -1.056186    0.538927   \n",
       "2023-11-13 10:15:00 -1.065572   -1.333331          -1.118687    0.415586   \n",
       "\n",
       "                     DJI_Open  DJI_Volume    SMA_15  Bollinger_lband_20  \\\n",
       "Datetime                                                                  \n",
       "2023-11-10 15:15:00 -0.936588    0.430106 -1.021738           -0.958114   \n",
       "2023-11-10 15:30:00 -0.913864    0.865141 -0.999999           -0.958572   \n",
       "2023-11-10 15:45:00 -0.906467    7.293764 -0.999999           -0.958572   \n",
       "2023-11-13 10:00:00 -0.920820    0.912056 -0.978259           -1.011770   \n",
       "2023-11-13 10:15:00 -0.918008    0.432906 -0.978259           -1.011770   \n",
       "\n",
       "                       WPR_10    ATR_25    CCI_15  DJI_High  Stoch_%K_15  \\\n",
       "Datetime                                                                   \n",
       "2023-11-10 15:15:00  0.494113 -0.081234  0.549450 -0.920554     0.617640   \n",
       "2023-11-10 15:30:00 -0.399996 -0.094826  0.279137 -0.909922    -0.264702   \n",
       "2023-11-10 15:45:00 -0.399996 -0.107874  0.242361 -0.900246    -0.264702   \n",
       "2023-11-13 10:00:00 -1.199996 -0.143966 -0.536075 -0.916463    -1.147052   \n",
       "2023-11-13 10:15:00 -1.199996 -0.155048 -0.550208 -0.924478    -1.499990   \n",
       "\n",
       "                       EMA_10  Gold_Close  SP500_Low  SET50_High  \\\n",
       "Datetime                                                           \n",
       "2023-11-10 15:15:00 -0.849909   -1.770800  -1.033988   -0.044376   \n",
       "2023-11-10 15:30:00 -0.878756   -1.772497  -1.000492   -0.041316   \n",
       "2023-11-10 15:45:00 -0.902359   -1.765708  -0.987359   -0.010713   \n",
       "2023-11-13 10:00:00 -1.047163   -1.770800  -1.094210   -0.269828   \n",
       "2023-11-13 10:15:00 -1.098111   -1.779289  -1.084665   -0.428464   \n",
       "\n",
       "                     Bollinger_mavg_25   DJI_Low  Stoch_%D_5  \\\n",
       "Datetime                                                       \n",
       "2023-11-10 15:15:00          -0.973332 -0.942608    0.379746   \n",
       "2023-11-10 15:30:00          -0.973332 -0.915137   -0.126582   \n",
       "2023-11-10 15:45:00          -0.973332 -0.910114   -0.632911   \n",
       "2023-11-13 10:00:00          -1.026665 -0.931641   -0.759490   \n",
       "2023-11-13 10:15:00          -1.053332 -0.934845   -0.759490   \n",
       "\n",
       "                     Bollinger_lband_25      MACD  Oil_Close  SET_Close  \\\n",
       "Datetime                                                                  \n",
       "2023-11-10 15:15:00           -0.898158 -0.031553   0.834804  -0.049760   \n",
       "2023-11-10 15:30:00           -0.898158 -0.094557   0.839849  -0.023178   \n",
       "2023-11-10 15:45:00           -0.898158 -0.143856   0.829761  -0.203817   \n",
       "2023-11-13 10:00:00           -0.956260 -0.448707   0.970996  -0.527948   \n",
       "2023-11-13 10:15:00           -0.978360 -0.548342   0.993695  -0.597138   \n",
       "\n",
       "                     Momentum    EMA_20  Oil_High  Bollinger_hband_10  \\\n",
       "Datetime                                                                \n",
       "2023-11-10 15:15:00  1.000005 -0.918881  0.791615           -0.680257   \n",
       "2023-11-10 15:30:00 -1.000005 -0.929962  0.781752           -0.711570   \n",
       "2023-11-10 15:45:00 -0.500000 -0.939988  0.791615           -0.711570   \n",
       "2023-11-13 10:00:00 -0.500005 -1.021933  0.971640           -0.661597   \n",
       "2023-11-13 10:15:00 -0.500005 -1.054676  0.969174           -0.661597   \n",
       "\n",
       "                     SET100_Open  MACD_diff  Oil_Open    CCI_25  \\\n",
       "Datetime                                                          \n",
       "2023-11-10 15:15:00    -0.054431   1.270606  0.808355  0.352058   \n",
       "2023-11-10 15:30:00    -0.086240   0.887726  0.805897  0.119089   \n",
       "2023-11-10 15:45:00    -0.062442   0.612911  0.810812  0.110172   \n",
       "2023-11-13 10:00:00    -0.277097  -0.204041  0.953319 -0.479612   \n",
       "2023-11-13 10:15:00    -0.534874  -0.376163  0.936120 -0.444135   \n",
       "\n",
       "                     Bollinger_mavg_20  SET100_Low  Bollinger_mavg_15  \\\n",
       "Datetime                                                                \n",
       "2023-11-10 15:15:00          -0.983605   -0.052632          -1.021738   \n",
       "2023-11-10 15:30:00          -0.999999   -0.050304          -0.999999   \n",
       "2023-11-10 15:45:00          -0.999999   -0.166745          -0.999999   \n",
       "2023-11-13 10:00:00          -1.065572   -0.535398          -0.978259   \n",
       "2023-11-13 10:15:00          -1.065572   -0.569168          -0.978259   \n",
       "\n",
       "                     Stoch_%D_25  Stoch_%K_20  ERW.BK_Volume  \\\n",
       "Datetime                                                       \n",
       "2023-11-10 15:15:00     0.617640     0.617640       1.675475   \n",
       "2023-11-10 15:30:00     0.323526    -0.264702      -0.168638   \n",
       "2023-11-10 15:45:00     0.029412    -0.264702       0.309089   \n",
       "2023-11-13 10:00:00    -0.852935    -1.147052       0.749866   \n",
       "2023-11-13 10:15:00    -0.852935    -1.147052      -0.196404   \n",
       "\n",
       "                     Bollinger_hband_5  y  \n",
       "Datetime                                   \n",
       "2023-11-10 15:15:00          -0.440437 -1  \n",
       "2023-11-10 15:30:00          -0.599148 -1  \n",
       "2023-11-10 15:45:00          -0.818958 -1  \n",
       "2023-11-13 10:00:00          -1.139789 -1  \n",
       "2023-11-13 10:15:00          -1.203956 -1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "จำนวนแถวที่ไม่มีค่า null ในทุกคอลัมน์: 531\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "def fit_robust_scaler(df, y_column):\n",
    "    \"\"\"\n",
    "    Fit a RobustScaler to the DataFrame (except for the target column) and return the scaler.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame used to fit the scaler.\n",
    "    y_column (str): The name of the target column.\n",
    "\n",
    "    Returns:\n",
    "    RobustScaler: The fitted scaler.\n",
    "    \"\"\"\n",
    "    # Copy the DataFrame to avoid modifying the original one\n",
    "    df_to_fit = df.drop(y_column, axis=1)\n",
    "\n",
    "    # Initialize and fit RobustScaler\n",
    "    scaler = RobustScaler()\n",
    "    scaler.fit(df_to_fit)\n",
    "\n",
    "    return scaler\n",
    "\n",
    "def transform_with_scaler(df, scaler, y_column):\n",
    "    \"\"\"\n",
    "    Transform a DataFrame using a provided RobustScaler.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame to be transformed.\n",
    "    scaler (RobustScaler): The scaler to use for transformation.\n",
    "    y_column (str): The name of the target column.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The scaled DataFrame with the target column.\n",
    "    \"\"\"\n",
    "    df_scaled = df.copy()\n",
    "    y = df_scaled.pop(y_column)\n",
    "    df_scaled = pd.DataFrame(scaler.transform(df_scaled), columns=df_scaled.columns, index=df_scaled.index)\n",
    "    df_scaled[y_column] = y\n",
    "\n",
    "    return df_scaled\n",
    "\n",
    "# Example usage\n",
    "scaler = fit_robust_scaler(df, 'y')\n",
    "robust_tf_df = transform_with_scaler(df, scaler, 'y')\n",
    "try:\n",
    "    robust_tf_test_df = transform_with_scaler(df_test, scaler, 'y')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "display(robust_tf_df.head())\n",
    "\n",
    "# จำนวนแถวที่ทุกคอลัมน์ไม่เป็น null\n",
    "num_rows_without_nulls = len(robust_tf_df.dropna())\n",
    "print(\"จำนวนแถวที่ไม่มีค่า null ในทุกคอลัมน์:\", num_rows_without_nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FtImpRF + MultiCorr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance using RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "1080 fits failed out of a total of 3240.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "783 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "297 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.59772727 0.63181818 0.55681818 0.57045455 0.58181818 0.54545455\n",
      " 0.61363636 0.59090909 0.58409091 0.53409091 0.575      0.59318182\n",
      " 0.68181818 0.66136364 0.67045455 0.72045455 0.61590909 0.69772727\n",
      " 0.68636364 0.71136364 0.72272727 0.74545455 0.70681818 0.72045455\n",
      " 0.74545455 0.75       0.76818182 0.76363636 0.76136364 0.76136364\n",
      " 0.77272727 0.77727273 0.71136364 0.76590909 0.775      0.77045455\n",
      " 0.76363636 0.62045455 0.62272727 0.60909091 0.69545455 0.62727273\n",
      " 0.6        0.63636364 0.575      0.61818182 0.6        0.59545455\n",
      " 0.58409091 0.74545455 0.68863636 0.70454545 0.67727273 0.67272727\n",
      " 0.75227273 0.72045455 0.69318182 0.73636364 0.72727273 0.73863636\n",
      " 0.71818182 0.74545455 0.78181818 0.77272727 0.71818182 0.76818182\n",
      " 0.76363636 0.77045455 0.73636364 0.77954545 0.75909091 0.78181818\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.67954545 0.59772727 0.59090909 0.64318182 0.70909091 0.60454545\n",
      " 0.62954545 0.59090909 0.58863636 0.58636364 0.60454545 0.61590909\n",
      " 0.68409091 0.66818182 0.66590909 0.7        0.64772727 0.74545455\n",
      " 0.68863636 0.70227273 0.55681818 0.72954545 0.71818182 0.73409091\n",
      " 0.74318182 0.74772727 0.74772727 0.77045455 0.72954545 0.76136364\n",
      " 0.75454545 0.76818182 0.73181818 0.77272727 0.76590909 0.76818182\n",
      " 0.70227273 0.65681818 0.62045455 0.625      0.61818182 0.56818182\n",
      " 0.625      0.61136364 0.61136364 0.67727273 0.66590909 0.63863636\n",
      " 0.67045455 0.70454545 0.75227273 0.70909091 0.70454545 0.72272727\n",
      " 0.75909091 0.71818182 0.60681818 0.73636364 0.74090909 0.75\n",
      " 0.76363636 0.75909091 0.77272727 0.77954545 0.72272727 0.75909091\n",
      " 0.77272727 0.78181818 0.725      0.775      0.76818182 0.77272727\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.57272727 0.71590909 0.61818182 0.58863636 0.57272727 0.58181818\n",
      " 0.59772727 0.60227273 0.61590909 0.59772727 0.60227273 0.59545455\n",
      " 0.68409091 0.71136364 0.69318182 0.67954545 0.62954545 0.69772727\n",
      " 0.64545455 0.68863636 0.70681818 0.7        0.73181818 0.70681818\n",
      " 0.74318182 0.76590909 0.77045455 0.77045455 0.77045455 0.75681818\n",
      " 0.76136364 0.77272727 0.7        0.76136364 0.76818182 0.775\n",
      " 0.64545455 0.58181818 0.64318182 0.6        0.59090909 0.59772727\n",
      " 0.59318182 0.61818182 0.63636364 0.66590909 0.54545455 0.62272727\n",
      " 0.7        0.71136364 0.72954545 0.70454545 0.675      0.73409091\n",
      " 0.71590909 0.70227273 0.66818182 0.73636364 0.70909091 0.75227273\n",
      " 0.72045455 0.77954545 0.77272727 0.77272727 0.75681818 0.78409091\n",
      " 0.76363636 0.77045455 0.71818182 0.77272727 0.78181818 0.78181818\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.63863636 0.61590909 0.58636364 0.59090909 0.68863636 0.56818182\n",
      " 0.55227273 0.59090909 0.59318182 0.56590909 0.59090909 0.56590909\n",
      " 0.61363636 0.71818182 0.65681818 0.70227273 0.68863636 0.7\n",
      " 0.68636364 0.68409091 0.69772727 0.69772727 0.7        0.72045455\n",
      " 0.75909091 0.73636364 0.76590909 0.76818182 0.75227273 0.76590909\n",
      " 0.77272727 0.76818182 0.71590909 0.76818182 0.77954545 0.76136364\n",
      " 0.67954545 0.64545455 0.6        0.62272727 0.54090909 0.65681818\n",
      " 0.60454545 0.61818182 0.62045455 0.65       0.64545455 0.63181818\n",
      " 0.64090909 0.68863636 0.70909091 0.725      0.72954545 0.725\n",
      " 0.69772727 0.74545455 0.71136364 0.72272727 0.70909091 0.74772727\n",
      " 0.73409091 0.78409091 0.78636364 0.77272727 0.71136364 0.77045455\n",
      " 0.76818182 0.77272727 0.75681818 0.77045455 0.75909091 0.77727273\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.67727273 0.60909091 0.58409091 0.58181818 0.57272727 0.55\n",
      " 0.58863636 0.58863636 0.55909091 0.62727273 0.625      0.58181818\n",
      " 0.71136364 0.65681818 0.69090909 0.67954545 0.66363636 0.70454545\n",
      " 0.72954545 0.7        0.67272727 0.7        0.69545455 0.71590909\n",
      " 0.74772727 0.77272727 0.76136364 0.76818182 0.675      0.77727273\n",
      " 0.76590909 0.77045455 0.74545455 0.77272727 0.76590909 0.77272727\n",
      " 0.64090909 0.66590909 0.64318182 0.6        0.64545455 0.60227273\n",
      " 0.59090909 0.6        0.53636364 0.56136364 0.62727273 0.61818182\n",
      " 0.59772727 0.71136364 0.66590909 0.71590909 0.66590909 0.71590909\n",
      " 0.70454545 0.75454545 0.68863636 0.73863636 0.72272727 0.73863636\n",
      " 0.76818182 0.775      0.775      0.775      0.75454545 0.77045455\n",
      " 0.77045455 0.77954545 0.75454545 0.75909091 0.77727273 0.77272727\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.61136364 0.61136364 0.60681818 0.58409091 0.6        0.56363636\n",
      " 0.57272727 0.55681818 0.60909091 0.60681818 0.56590909 0.58409091\n",
      " 0.68863636 0.67045455 0.73409091 0.70454545 0.68181818 0.67954545\n",
      " 0.73636364 0.69772727 0.66363636 0.68636364 0.72727273 0.71590909\n",
      " 0.72272727 0.77727273 0.76818182 0.77954545 0.75227273 0.75\n",
      " 0.75681818 0.76590909 0.69318182 0.76818182 0.775      0.77045455\n",
      " 0.59545455 0.625      0.63863636 0.61818182 0.60227273 0.66136364\n",
      " 0.60454545 0.62272727 0.61818182 0.54090909 0.59318182 0.64318182\n",
      " 0.57727273 0.68409091 0.68409091 0.70681818 0.64772727 0.66363636\n",
      " 0.725      0.73636364 0.66363636 0.71590909 0.725      0.75909091\n",
      " 0.74545455 0.78409091 0.77954545 0.77272727 0.67727273 0.77272727\n",
      " 0.77954545 0.76590909 0.73409091 0.775      0.77045455 0.77272727]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv score :  [0.70422535 0.6531746  0.66392694 0.59895833 0.67093106]\n",
      "cv avg score :  0.6582432576501285\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>RSI_5</td>\n",
       "      <td>0.024732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Close_pct_change</td>\n",
       "      <td>0.023276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>MACD_diff</td>\n",
       "      <td>0.019691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>DJI_Volume</td>\n",
       "      <td>0.019492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>RSI_15</td>\n",
       "      <td>0.019401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>ERW.BK_Low</td>\n",
       "      <td>0.002119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ERW.BK_High</td>\n",
       "      <td>0.001767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>ERW.BK_Open</td>\n",
       "      <td>0.001351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>day_of_week</td>\n",
       "      <td>0.001281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ERW.BK_Adj Close</td>\n",
       "      <td>0.000798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Feature  Importance\n",
       "66             RSI_5    0.024732\n",
       "25  Close_pct_change    0.023276\n",
       "99         MACD_diff    0.019691\n",
       "75        DJI_Volume    0.019492\n",
       "34            RSI_15    0.019401\n",
       "..               ...         ...\n",
       "71        ERW.BK_Low    0.002119\n",
       "12       ERW.BK_High    0.001767\n",
       "69       ERW.BK_Open    0.001351\n",
       "26       day_of_week    0.001281\n",
       "8   ERW.BK_Adj Close    0.000798\n",
       "\n",
       "[109 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average AUC-ROC Score: 0.6582432576501285\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, cross_val_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_feature_importance(df, y_column, n_splits):\n",
    "    # แยก DataFrame เป็น features และ target\n",
    "    X = df.drop(y_column, axis=1)\n",
    "    y = df[y_column]\n",
    "\n",
    "    # สร้าง TimeSeriesSplit ด้วย n_splits ที่ระบุ\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    # สร้างโมเดล Random Forest\n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    # ตั้งค่าพารามิเตอร์สำหรับ Grid Search\n",
    "    param_grid = {\n",
    "        'n_estimators': [10, 50, 100, 500],\n",
    "        'max_depth': [None, 10, 20, 30, 70, 100],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['auto', 'sqrt', 'log2']\n",
    "    }\n",
    "\n",
    "    # ใช้ Grid Search ด้วย TimeSeriesSplit เพื่อหาพารามิเตอร์ที่ดีที่สุด\n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=tscv, n_jobs=-1)\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # สร้างโมเดลด้วยพารามิเตอร์ที่ดีที่สุด\n",
    "    best_rf = grid_search.best_estimator_\n",
    "\n",
    "    # คำนวณ feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': best_rf.feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # ตรวจสอบประสิทธิภาพโมเดลด้วย cross-validation และ TimeSeriesSplit\n",
    "    cv_scores = cross_val_score(best_rf, X, y, cv=tscv, scoring='roc_auc')\n",
    "    avg_cv_score = np.mean(cv_scores)\n",
    "    print(\"cv score : \",cv_scores)\n",
    "    print(\"cv avg score : \",avg_cv_score)\n",
    "\n",
    "    return feature_importance, avg_cv_score\n",
    "\n",
    "\n",
    "feature_importance_df, avg_cv_score = calculate_feature_importance(robust_tf_df, 'y', 5)\n",
    "display(feature_importance_df)\n",
    "print(f'Average AUC-ROC Score: {avg_cv_score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance using LIGHT GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.model_selection import RandomizedSearchCV, cross_val_score, TimeSeriesSplit\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# def calculate_feature_importance_with_lgbm(df, y_column, n_splits):\n",
    "#     # แยก DataFrame เป็น features และ target\n",
    "#     X = df.drop(y_column, axis=1)\n",
    "#     y = df[y_column]\n",
    "\n",
    "#     # แปลงค่า target เป็น 0 และ 1\n",
    "#     y = y.replace(-1, 0)\n",
    "\n",
    "#     # สร้าง TimeSeriesSplit\n",
    "#     tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "#     # สร้างโมเดล LightGBM\n",
    "#     lgbm = LGBMClassifier(verbose=-1)\n",
    "\n",
    "#     # ตั้งค่าพารามิเตอร์สำหรับ Random Search\n",
    "#     param_dist = {\n",
    "#         'n_estimators': [100, 200, 500],\n",
    "#         'learning_rate': [0.01, 0.05, 0.1],\n",
    "#         'num_leaves': [31, 50, 100],\n",
    "#         'max_depth': [-1, 10, 20],\n",
    "#         'min_child_samples': [20, 30, 50]\n",
    "#     }\n",
    "\n",
    "#     # ใช้ Random Search ด้วย TimeSeriesSplit เพื่อหาพารามิเตอร์ที่ดีที่สุด\n",
    "#     random_search = RandomizedSearchCV(estimator=lgbm, param_distributions=param_dist, n_iter=100, cv=tscv, n_jobs=-1, random_state=42)\n",
    "#     random_search.fit(X, y)\n",
    "\n",
    "#     # สร้างโมเดลด้วยพารามิเตอร์ที่ดีที่สุด\n",
    "#     best_lgbm = random_search.best_estimator_\n",
    "\n",
    "#     # คำนวณ AUC-ROC score โดยใช้ cross validation กับ TimeSeriesSplit\n",
    "#     cv_scores = cross_val_score(best_lgbm, X, y, cv=tscv, scoring='roc_auc')\n",
    "#     avg_cv_score = np.mean(cv_scores)\n",
    "#     print('cv score :', cv_scores)\n",
    "#     print(f'Average AUC-ROC Score: {avg_cv_score}')\n",
    "\n",
    "#     # คำนวณ feature importance\n",
    "#     feature_importance = pd.DataFrame({\n",
    "#         'Feature': X.columns,\n",
    "#         'Importance': best_lgbm.feature_importances_\n",
    "#     }).sort_values(by='Importance', ascending=False)\n",
    "#     print(\"cv score : \",cv_scores)\n",
    "#     print(\"cv avg score : \",avg_cv_score)\n",
    "\n",
    "#     return feature_importance, avg_cv_score\n",
    "\n",
    "# # # การใช้งานฟังก์ชัน\n",
    "# # n_splits = 5  # กำหนดจำนวนการแบ่งข้อมูล\n",
    "# # feature_importance_df, avg_cv_score = calculate_feature_importance_with_lgbm(robust_tf_df, 'y', n_splits)\n",
    "# # display(feature_importance_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance using XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.model_selection import RandomizedSearchCV, cross_val_score, TimeSeriesSplit\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# def calculate_feature_importance_with_xgboost(df, y_column, n_splits):\n",
    "#     # แยก DataFrame เป็น features และ target\n",
    "#     X = df.drop(y_column, axis=1)\n",
    "#     y = df[y_column]\n",
    "\n",
    "#     # แปลงค่า target เป็น 0 และ 1\n",
    "#     y = y.replace(-1, 0)\n",
    "\n",
    "#     # สร้าง TimeSeriesSplit\n",
    "#     tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "#     # สร้างโมเดล XGBoost\n",
    "#     xgb = XGBClassifier(verbosity=0, use_label_encoder=False)\n",
    "\n",
    "#     # ตั้งค่าพารามิเตอร์สำหรับ Random Search\n",
    "#     param_dist = {\n",
    "#         'n_estimators': [100, 200, 500],\n",
    "#         'learning_rate': [0.01, 0.05, 0.1],\n",
    "#         'max_depth': [3, 6, 10],\n",
    "#         'min_child_weight': [1, 3, 5],\n",
    "#         'subsample': [0.7, 0.8, 0.9],\n",
    "#         'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "#     }\n",
    "\n",
    "#     # ใช้ Random Search ด้วย TimeSeriesSplit เพื่อหาพารามิเตอร์ที่ดีที่สุด\n",
    "#     random_search = RandomizedSearchCV(estimator=xgb, param_distributions=param_dist, n_iter=100, cv=tscv, n_jobs=-1)\n",
    "#     random_search.fit(X, y)\n",
    "\n",
    "#     # สร้างโมเดลด้วยพารามิเตอร์ที่ดีที่สุด\n",
    "#     best_xgb = random_search.best_estimator_\n",
    "\n",
    "#     # คำนวณ AUC-ROC score โดยใช้ cross validation กับ TimeSeriesSplit\n",
    "#     cv_scores = cross_val_score(best_xgb, X, y, cv=tscv, scoring='roc_auc')\n",
    "#     avg_cv_score = np.mean(cv_scores)\n",
    "#     print('cv score :',cv_scores)\n",
    "#     print(f'Average AUC-ROC Score: {avg_cv_score}')\n",
    "\n",
    "#     # คำนวณ feature importance\n",
    "#     feature_importance = pd.DataFrame({\n",
    "#         'Feature': X.columns,\n",
    "#         'Importance': best_xgb.feature_importances_\n",
    "#     }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "#     return feature_importance, avg_cv_score\n",
    "\n",
    "# # # การใช้งานฟังก์ชัน\n",
    "# # n_splits = 5  # กำหนดจำนวนการแบ่งข้อมูล\n",
    "# # feature_importance_df, avg_cv_score = calculate_feature_importance_with_xgboost(robust_tf_df, 'y', n_splits)\n",
    "# # display(feature_importance_df)\n",
    "# # print(f'Average AUC-ROC Score: {avg_cv_score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiCorr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/statsmodels/stats/outliers_influence.py:198: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ft1</th>\n",
       "      <th>ft2</th>\n",
       "      <th>absolute_correlation</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9159</th>\n",
       "      <td>Gold_Close</td>\n",
       "      <td>Gold_Adj Close</td>\n",
       "      <td>1.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4507</th>\n",
       "      <td>Bollinger_mavg_10</td>\n",
       "      <td>SMA_10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6294</th>\n",
       "      <td>WPR_15</td>\n",
       "      <td>Stoch_%K_15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10070</th>\n",
       "      <td>Oil_Close</td>\n",
       "      <td>Oil_Adj Close</td>\n",
       "      <td>1.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2312</th>\n",
       "      <td>SP500_Close</td>\n",
       "      <td>SP500_Adj Close</td>\n",
       "      <td>1.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3970</th>\n",
       "      <td>Bollinger_mavg_5</td>\n",
       "      <td>SMA_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3347</th>\n",
       "      <td>Stoch_%K_10</td>\n",
       "      <td>WPR_10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>Gold_Adj Close</td>\n",
       "      <td>Gold_Close</td>\n",
       "      <td>1.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2225</th>\n",
       "      <td>SET100_Adj Close</td>\n",
       "      <td>SET100_Close</td>\n",
       "      <td>1.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11568</th>\n",
       "      <td>Stoch_%K_20</td>\n",
       "      <td>WPR_20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ft1              ft2  absolute_correlation  VIF\n",
       "9159          Gold_Close   Gold_Adj Close                   1.0  inf\n",
       "4507   Bollinger_mavg_10           SMA_10                   1.0  inf\n",
       "6294              WPR_15      Stoch_%K_15                   1.0  inf\n",
       "10070          Oil_Close    Oil_Adj Close                   1.0  inf\n",
       "2312         SP500_Close  SP500_Adj Close                   1.0  inf\n",
       "3970    Bollinger_mavg_5            SMA_5                   1.0  inf\n",
       "3347         Stoch_%K_10           WPR_10                   1.0  inf\n",
       "410       Gold_Adj Close       Gold_Close                   1.0  inf\n",
       "2225    SET100_Adj Close     SET100_Close                   1.0  inf\n",
       "11568        Stoch_%K_20           WPR_20                   1.0  inf"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calculate_vif_and_correlation(df):\n",
    "    # คำนวณ VIF\n",
    "    def calculate_vif(dataframe):\n",
    "        vif_data = pd.DataFrame()\n",
    "        vif_data[\"feature\"] = dataframe.columns\n",
    "        vif_data[\"VIF\"] = [variance_inflation_factor(dataframe.values, i) for i in range(len(dataframe.columns))]\n",
    "        return vif_data\n",
    "\n",
    "    # สร้าง DataFrame ของค่า correlation แบบสัมบูรณ์\n",
    "    correlation_matrix = df.corr().abs()\n",
    "\n",
    "    # คำนวณ VIF\n",
    "    vif_data = calculate_vif(df)\n",
    "\n",
    "    # สร้าง DataFrame สำหรับค่า correlation และ VIF\n",
    "    correlation_pairs = correlation_matrix.unstack().reset_index()\n",
    "    correlation_pairs.columns = ['ft1', 'ft2', 'absolute_correlation']\n",
    "    correlation_pairs = correlation_pairs[correlation_pairs['ft1'] != correlation_pairs['ft2']]\n",
    "\n",
    "    # รวม VIF และ correlation\n",
    "    result_df = correlation_pairs.merge(vif_data, left_on='ft1', right_on='feature')\n",
    "    result_df = result_df.drop(columns=['feature']).sort_values(by='absolute_correlation', ascending=False)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# ใช้งานฟังก์ชัน\n",
    "# สมมติว่า df เป็น DataFrame ที่คุณต้องการวิเคราะห์\n",
    "multicorr_df = calculate_vif_and_correlation(robust_tf_df)\n",
    "\n",
    "display(multicorr_df.head(10) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features for Modeling: ['RSI_15', 'RSI_15', 'DJI_Volume', 'DJI_Volume', 'RSI_15', 'RSI_15', 'RSI_5', 'RSI_5', 'RSI_10', 'RSI_10', 'RSI_5', 'RSI_5', 'CCI_10', 'CCI_10', 'RSI_5', 'RSI_5', 'RSI_10', 'RSI_10', 'MACD_diff', 'MACD_diff', 'RSI_5', 'RSI_5', 'RSI_5', 'RSI_5', 'RSI_5', 'RSI_5', 'RSI_15', 'RSI_15', 'MACD_diff', 'MACD_diff', 'MACD_diff', 'MACD_diff', 'RSI_10', 'RSI_10', 'CCI_10', 'CCI_10', 'MACD_diff', 'MACD_diff', 'RSI_15', 'RSI_15', 'Close_pct_change', 'Close_pct_change', 'RSI_5', 'RSI_5']\n",
      "Num of Ft selected : 44\n"
     ]
    }
   ],
   "source": [
    "ft_imp_quantile_threshold = 0.9\n",
    "ft_imp_threshold = feature_importance_df['Importance'].quantile(ft_imp_quantile_threshold)\n",
    "ft_imp_selected_df = feature_importance_df[feature_importance_df['Importance'] >= ft_imp_threshold]\n",
    "# display(ft_imp_selected_df)\n",
    "ft_imp_selected_list = ft_imp_selected_df.iloc[:,0].values\n",
    "\n",
    "# กำหนดค่า Threshold สำหรับค่าสหสัมพันธ์\n",
    "correlation_threshold = 0.5\n",
    "\n",
    "# คัดเลือกเฉพาะคู่ของคุณสมบัติที่มีค่าสหสัมพันธ์สูงกว่า Threshold\n",
    "high_correlation_pairs = multicorr_df[\n",
    "    (multicorr_df['absolute_correlation'] > correlation_threshold) &\n",
    "    (multicorr_df['ft1'].isin(ft_imp_selected_list)) & \n",
    "    (multicorr_df['ft2'].isin(ft_imp_selected_list))\n",
    "]\n",
    "\n",
    "# ตัดสินใจเลือกคุณสมบัติที่มีค่า feature importance สูงที่สุด\n",
    "selected_features = []\n",
    "for _, row in high_correlation_pairs.iterrows():\n",
    "    ft1_importance = feature_importance_df.loc[feature_importance_df['Feature'] == row['ft1'], 'Importance'].values[0]\n",
    "    ft2_importance = feature_importance_df.loc[feature_importance_df['Feature'] == row['ft2'], 'Importance'].values[0]\n",
    "\n",
    "    if ft1_importance > ft2_importance:\n",
    "        selected_features.append(row['ft1'])\n",
    "    else:\n",
    "        selected_features.append(row['ft2'])\n",
    "\n",
    "# ลบคุณสมบัติที่ซ้ำกัน\n",
    "selected_features_by_ftimp_multicorr_list = list(set(selected_features))\n",
    "\n",
    "print(\"Selected Features for Modeling:\", selected_features)\n",
    "\n",
    "robust_tf_ftimp_mulcorr_df = robust_tf_df[selected_features_by_ftimp_multicorr_list + ['y']]\n",
    "print(\"Num of Ft selected :\" , len(selected_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:  Index(['SET_Open', 'RSI_25', 'Oil_Low', 'ATR_10', 'SP500_Volume', 'RSI_10',\n",
      "       'Close_pct_change', 'Bollinger_hband_25', 'ATR_5', 'Gold_Volume',\n",
      "       'CCI_5', 'RSI_5', 'CCI_10', 'Oil_Volume', 'DJI_Volume', 'ATR_25',\n",
      "       'Gold_Close', 'MACD', 'MACD_diff', 'ERW.BK_Volume'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# สร้างข้อมูลตัวอย่าง\n",
    "X = robust_tf_df.drop('y', axis=1)\n",
    "y = robust_tf_df['y'].values.ravel()  # ใช้ .values.ravel() เพื่อแปลงเป็น array 1D\n",
    "\n",
    "# สร้างโมเดล\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# RFE\n",
    "selector = RFE(model, n_features_to_select=20, step=1)\n",
    "selector = selector.fit(X, y)\n",
    "\n",
    "# คุณสมบัติที่เลือก\n",
    "selected_features = selector.support_\n",
    "print(\"Selected Features: \", X.columns[selected_features])\n",
    "\n",
    "selected_features_by_rfe_list = list(X.columns[selected_features])\n",
    "robust_tf_rfe_df = robust_tf_df[selected_features_by_rfe_list + ['y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def apply_pca(df, y_column, n_components=None):\n",
    "    X = df.drop(y_column, axis=1)\n",
    "\n",
    "    # ใช้ StandardScaler เพื่อทำ scaling ข้อมูล\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # ใช้ PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "    # สร้าง DataFrame ใหม่ที่มี components ของ PCA และ target\n",
    "    columns = [f'PC{i+1}' for i in range(X_pca.shape[1])]\n",
    "    df_pca = pd.DataFrame(X_pca, columns=columns)\n",
    "    df_pca[y_column] = df[y_column].values\n",
    "\n",
    "    return df_pca, pca\n",
    "\n",
    "def plot_scree(pca):\n",
    "    var_ratio = pca.explained_variance_ratio_\n",
    "    cum_var_ratio = var_ratio.cumsum()\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(range(1, len(var_ratio) + 1), var_ratio, marker='o', linestyle='-', alpha=0.5, label='Individual explained variance')\n",
    "    plt.plot(range(1, len(cum_var_ratio) + 1), cum_var_ratio, marker='o', color='red', linestyle='-', label='Cumulative explained variance')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.xlabel('Principal Component Index')\n",
    "    plt.xticks(range(1, len(var_ratio) + 1))\n",
    "    plt.yticks(np.arange(0, 1.1, 0.05))  # Adjusting y-axis ticks\n",
    "    plt.grid(True)  # Adding grid\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Scree Plot')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ตัวอย่างการใช้งาน\n",
    "df_pca_trial, pca_model_trial = apply_pca(robust_tf_df, 'y', n_components=30)\n",
    "plot_scree(pca_model_trial)\n",
    "\n",
    "df_pca_5_comp = df_pca_trial, pca_model_trial = apply_pca(robust_tf_df, 'y', n_components=5)\n",
    "df_pca_15_comp = df_pca_trial, pca_model_trial = apply_pca(robust_tf_df, 'y', n_components=5)\n",
    "df_pca_10_comp = df_pca_trial, pca_model_trial = apply_pca(robust_tf_df, 'y', n_components=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_5_comp , pca_model_trial = apply_pca(robust_tf_df, 'y', n_components=5)\n",
    "df_pca_10_comp , pca_model_trial = apply_pca(robust_tf_df, 'y', n_components=10)\n",
    "df_pca_15_comp , pca_model_trial = apply_pca(robust_tf_df, 'y', n_components=15)\n",
    "df_pca_20_comp , pca_model_trial = apply_pca(robust_tf_df, 'y', n_components=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepwise Ft Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Other imports remain the same\n",
    "\n",
    "def stepwise_selection(df, y_column):\n",
    "    X = df.drop(y_column, axis=1)\n",
    "    y = df[y_column]\n",
    "\n",
    "    models = {\n",
    "        'LGBM': LGBMClassifier(verbose=-1),\n",
    "        'GradientBoosting': GradientBoostingClassifier(),\n",
    "        'RandomForest': RandomForestClassifier(),\n",
    "        'LogisticRegression_L2': LogisticRegression(penalty='l2', solver='lbfgs'),\n",
    "        'LogisticRegression_L1': LogisticRegression(penalty='l1', solver='liblinear'),\n",
    "        'LogisticRegression_ElasticNet': LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5),\n",
    "        'GaussianNB': GaussianNB(),\n",
    "        'SVM': SVC()\n",
    "    }\n",
    "\n",
    "    selected_features = {}\n",
    "    model_scores = {}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        sfs = SFS(model, \n",
    "                  k_features=(3, len(X.columns)),  # Range of features to consider\n",
    "                  forward=True, \n",
    "                  floating=True,  # Enable both adding and removing features\n",
    "                  scoring='roc_auc',\n",
    "                  cv=5)\n",
    "\n",
    "        sfs.fit(X, y)\n",
    "        selected_features[model_name] = X.columns[list(sfs.k_feature_idx_)]\n",
    "        model_scores[model_name] = sfs.k_score_\n",
    "\n",
    "    return selected_features, model_scores\n",
    "\n",
    "model_ft_dict = dict()\n",
    "# Example usage\n",
    "selected_features_by_model, model_scores = stepwise_selection(robust_tf_df, 'y')\n",
    "for model, features in selected_features_by_model.items():\n",
    "    print(f\"Selected features for {model}: {features}\")\n",
    "    print(f\"Score for {model}: {model_scores[model]}\")\n",
    "    model_ft_dict[model] = list(features)\n",
    "    print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RSI_15', 'SET_Close', 'Close_pct_change', 'EMA_25']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model_ft_dict['LGBM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stepwise_lgbm_df\n",
      "stepwise_gradientboosting_df\n",
      "stepwise_randomforest_df\n",
      "stepwise_logisticregression_l2_df\n",
      "stepwise_logisticregression_l1_df\n",
      "stepwise_logisticregression_elasticnet_df\n",
      "stepwise_gaussiannb_df\n",
      "stepwise_svm_df\n"
     ]
    }
   ],
   "source": [
    "for k,v in model_ft_dict.items():\n",
    "    # print(k,v)\n",
    "    # print(f\"\"\"stepwise_{k.lower()}_df = robust_tf_df[model_ft_dict['{k}'] + ['y']] \"\"\")\n",
    "    print(f\"\"\"stepwise_{k.lower()}_df\"\"\")\n",
    "stepwise_lgbm_df = robust_tf_df[model_ft_dict['LGBM'] + ['y']]\n",
    "stepwise_lgbm_df = robust_tf_df[model_ft_dict['LGBM'] + ['y']] \n",
    "stepwise_gradientboosting_df = robust_tf_df[model_ft_dict['GradientBoosting'] + ['y']] \n",
    "stepwise_randomforest_df = robust_tf_df[model_ft_dict['RandomForest'] + ['y']] \n",
    "stepwise_logisticregression_l2_df = robust_tf_df[model_ft_dict['LogisticRegression_L2'] + ['y']] \n",
    "stepwise_logisticregression_l1_df = robust_tf_df[model_ft_dict['LogisticRegression_L1'] + ['y']] \n",
    "stepwise_logisticregression_elasticnet_df = robust_tf_df[model_ft_dict['LogisticRegression_ElasticNet'] + ['y']] \n",
    "stepwise_gaussiannb_df = robust_tf_df[model_ft_dict['GaussianNB'] + ['y']] \n",
    "stepwise_svm_df = robust_tf_df[model_ft_dict['SVM'] + ['y']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Knowledge Ft selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)\n",
    "# display(robust_tf_df.columns.to_frame())\n",
    "# pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>RSI_15</th>\n",
       "      <th>SET_Close</th>\n",
       "      <th>MACD</th>\n",
       "      <th>Close_pct_change</th>\n",
       "      <th>EMA_25</th>\n",
       "      <th>ERW.BK_Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-11-10 15:15:00</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.263879</td>\n",
       "      <td>-0.049760</td>\n",
       "      <td>-0.031553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.945674</td>\n",
       "      <td>-0.666666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-10 15:30:00</th>\n",
       "      <td>-1</td>\n",
       "      <td>-0.213607</td>\n",
       "      <td>-0.023178</td>\n",
       "      <td>-0.094557</td>\n",
       "      <td>-0.009804</td>\n",
       "      <td>-0.955782</td>\n",
       "      <td>-0.999997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-10 15:45:00</th>\n",
       "      <td>-1</td>\n",
       "      <td>-0.213607</td>\n",
       "      <td>-0.203817</td>\n",
       "      <td>-0.143856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.965113</td>\n",
       "      <td>-0.999997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     y    RSI_15  SET_Close      MACD  Close_pct_change  \\\n",
       "Datetime                                                                  \n",
       "2023-11-10 15:15:00 -1  0.263879  -0.049760 -0.031553          0.000000   \n",
       "2023-11-10 15:30:00 -1 -0.213607  -0.023178 -0.094557         -0.009804   \n",
       "2023-11-10 15:45:00 -1 -0.213607  -0.203817 -0.143856          0.000000   \n",
       "\n",
       "                       EMA_25  ERW.BK_Close  \n",
       "Datetime                                     \n",
       "2023-11-10 15:15:00 -0.945674     -0.666666  \n",
       "2023-11-10 15:30:00 -0.955782     -0.999997  \n",
       "2023-11-10 15:45:00 -0.965113     -0.999997  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm_cols_selected = ['y'\n",
    "                    ,'RSI_15'\n",
    ",'SET_Close'\n",
    ",'MACD'\n",
    ",'Close_pct_change'\n",
    ",'EMA_25'\n",
    ",'ERW.BK_Close'\n",
    "]\n",
    "\n",
    "manual_domain_selection_df = robust_tf_df[dm_cols_selected ]\n",
    "manual_domain_selection_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise SystemExit(\"Stopping notebook execution\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'subsample': 0.8, 'num_leaves': 30, 'n_estimators': 200, 'min_child_weight': 0.1, 'max_depth': 10, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "Best ROC-AUC score found:  0.676869115220619\n",
      "ROC-AUC on test set:  0.5551219512195122\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best parameters found:  {'subsample': 0.8, 'num_leaves': 100, 'n_estimators': 50, 'min_child_weight': 0.01, 'max_depth': 20, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n",
      "Best ROC-AUC score found:  0.6467515979396975\n",
      "ROC-AUC on test set:  0.6609756097560976\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best parameters found:  {'subsample': 0.9, 'num_leaves': 30, 'n_estimators': 50, 'min_child_weight': 0.01, 'max_depth': 20, 'learning_rate': 0.2, 'colsample_bytree': 0.8}\n",
      "Best ROC-AUC score found:  0.6279182319711228\n",
      "ROC-AUC on test set:  0.5570731707317074\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best parameters found:  {'subsample': 0.6, 'num_leaves': 20, 'n_estimators': 500, 'min_child_weight': 0.001, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "Best ROC-AUC score found:  0.67228472435006\n",
      "ROC-AUC on test set:  0.6624390243902439\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best parameters found:  {'subsample': 0.6, 'num_leaves': 20, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 30, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\n",
      "Best ROC-AUC score found:  0.6370720272727014\n",
      "ROC-AUC on test set:  0.6653658536585366\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best parameters found:  {'subsample': 1.0, 'num_leaves': 70, 'n_estimators': 50, 'min_child_weight': 0.01, 'max_depth': 10, 'learning_rate': 0.001, 'colsample_bytree': 0.6}\n",
      "Best ROC-AUC score found:  0.6814404273455349\n",
      "ROC-AUC on test set:  0.6690243902439024\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best parameters found:  {'subsample': 0.7, 'num_leaves': 100, 'n_estimators': 300, 'min_child_weight': 0.1, 'max_depth': 15, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "Best ROC-AUC score found:  0.6940341577444248\n",
      "ROC-AUC on test set:  0.6399999999999999\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best parameters found:  {'subsample': 1.0, 'num_leaves': 70, 'n_estimators': 300, 'min_child_weight': 1, 'max_depth': 20, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "Best ROC-AUC score found:  0.6897520705031724\n",
      "ROC-AUC on test set:  0.6465853658536586\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best parameters found:  {'subsample': 1.0, 'num_leaves': 70, 'n_estimators': 300, 'min_child_weight': 1, 'max_depth': 20, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "Best ROC-AUC score found:  0.6897520705031724\n",
      "ROC-AUC on test set:  0.6465853658536586\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best parameters found:  {'subsample': 0.6, 'num_leaves': 50, 'n_estimators': 300, 'min_child_weight': 0.01, 'max_depth': 10, 'learning_rate': 0.001, 'colsample_bytree': 1.0}\n",
      "Best ROC-AUC score found:  0.6743946813060763\n",
      "ROC-AUC on test set:  0.6080487804878049\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best parameters found:  {'subsample': 1.0, 'num_leaves': 70, 'n_estimators': 50, 'min_child_weight': 0.01, 'max_depth': 20, 'learning_rate': 0.1, 'colsample_bytree': 0.6}\n",
      "Best ROC-AUC score found:  0.6619534589513849\n",
      "ROC-AUC on test set:  0.6682926829268293\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best parameters found:  {'subsample': 0.7, 'num_leaves': 50, 'n_estimators': 200, 'min_child_weight': 0.1, 'max_depth': 20, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "Best ROC-AUC score found:  0.6287924636723573\n",
      "ROC-AUC on test set:  0.6480487804878049\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best parameters found:  {'subsample': 0.7, 'num_leaves': 50, 'n_estimators': 200, 'min_child_weight': 0.1, 'max_depth': 20, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "Best ROC-AUC score found:  0.6287924636723573\n",
      "ROC-AUC on test set:  0.6480487804878049\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best parameters found:  {'subsample': 0.6, 'num_leaves': 100, 'n_estimators': 100, 'min_child_weight': 0.01, 'max_depth': 5, 'learning_rate': 0.001, 'colsample_bytree': 0.7}\n",
      "Best ROC-AUC score found:  0.6916265047501111\n",
      "ROC-AUC on test set:  0.6707317073170732\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best parameters found:  {'subsample': 0.6, 'num_leaves': 50, 'n_estimators': 300, 'min_child_weight': 0.01, 'max_depth': 10, 'learning_rate': 0.001, 'colsample_bytree': 1.0}\n",
      "Best ROC-AUC score found:  0.6743946813060763\n",
      "ROC-AUC on test set:  0.6080487804878049\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "\n",
    "def lgbm_random_search_cv(df, y_column):\n",
    "    # Data preparation\n",
    "    X = df.drop(y_column, axis=1)\n",
    "    y = df[y_column].replace(-1, 0)\n",
    "    \n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    split_index = int(0.8 * len(df))\n",
    "    X_train = df.iloc[:split_index].drop(y_column, axis=1)\n",
    "    y_train = df.iloc[:split_index][y_column]\n",
    "    X_test = df.iloc[split_index:].drop(y_column, axis=1)\n",
    "    y_test = df.iloc[split_index:][y_column]\n",
    "\n",
    "# df_test = df.iloc[split_index:]\n",
    "# df = df.iloc[:split_index]\n",
    "# print(len(df) , len(df_test))\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    # Define hyperparameters for random search\n",
    "    param_dist = {\n",
    "        'num_leaves': [20, 30, 50, 70, 100],\n",
    "        'max_depth': [5, 10, 15, 20, 30],\n",
    "        'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2],\n",
    "        'n_estimators': [50, 100, 200, 300, 500],\n",
    "        'min_child_weight': [0.001, 0.01, 0.1, 1, 10],\n",
    "        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    }\n",
    "\n",
    "    # LightGBM model\n",
    "    lgbm = LGBMClassifier(objective='binary', random_state=42,verbosity=-1)\n",
    "\n",
    "    # RandomizedSearchCV\n",
    "    rscv = RandomizedSearchCV(lgbm, param_distributions=param_dist, n_iter=100, scoring='roc_auc', cv=tscv, verbose=1, random_state=42, n_jobs=-1)\n",
    "    rscv.fit(X_train, y_train)\n",
    "\n",
    "    # Best parameters and best score\n",
    "    print(\"Best parameters found: \", rscv.best_params_)\n",
    "    print(\"Best ROC-AUC score found: \", rscv.best_score_)\n",
    "    \n",
    "    # Optionally, evaluate on the test set\n",
    "    test_score = rscv.score(X_test, y_test)\n",
    "    print(\"ROC-AUC on test set: \", test_score)\n",
    "    print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "    \n",
    "\n",
    "\n",
    "lgbm_random_search_cv(df_pca_5_comp, 'y')\n",
    "lgbm_random_search_cv(df_pca_15_comp, 'y')\n",
    "lgbm_random_search_cv(df_pca_20_comp, 'y')\n",
    "lgbm_random_search_cv(robust_tf_df, 'y')\n",
    "lgbm_random_search_cv(robust_tf_rfe_df, 'y')\n",
    "lgbm_random_search_cv(robust_tf_ftimp_mulcorr_df, 'y')\n",
    "lgbm_random_search_cv(manual_domain_selection_df, 'y')\n",
    "print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "lgbm_random_search_cv(stepwise_lgbm_df, 'y')\n",
    "lgbm_random_search_cv(stepwise_gradientboosting_df, 'y')\n",
    "lgbm_random_search_cv(stepwise_randomforest_df, 'y')\n",
    "lgbm_random_search_cv(stepwise_logisticregression_l2_df, 'y')\n",
    "lgbm_random_search_cv(stepwise_logisticregression_l1_df, 'y')\n",
    "lgbm_random_search_cv(stepwise_logisticregression_elasticnet_df, 'y')\n",
    "lgbm_random_search_cv(stepwise_gaussiannb_df, 'y')\n",
    "lgbm_random_search_cv(stepwise_svm_df, 'y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:  [0.5965203  0.69563492 0.59908676 0.60720486 0.60163468]\n",
      "Average CV Score:  0.6200163043442568\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/narischanpaiboonkit/Documents/Nida_DADS/6003_ML/erw_ml_final_pj.ipynb Cell 51\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/narischanpaiboonkit/Documents/Nida_DADS/6003_ML/erw_ml_final_pj.ipynb#X63sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m best_model\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/narischanpaiboonkit/Documents/Nida_DADS/6003_ML/erw_ml_final_pj.ipynb#X63sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m xgb_bayesian_optimization(df_pca_5_comp, \u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/narischanpaiboonkit/Documents/Nida_DADS/6003_ML/erw_ml_final_pj.ipynb#X63sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m xgb_bayesian_optimization(df_pca_15_comp, \u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/narischanpaiboonkit/Documents/Nida_DADS/6003_ML/erw_ml_final_pj.ipynb#X63sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m xgb_bayesian_optimization(df_pca_20_comp, \u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/narischanpaiboonkit/Documents/Nida_DADS/6003_ML/erw_ml_final_pj.ipynb#X63sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m xgb_bayesian_optimization(robust_tf_df, \u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/narischanpaiboonkit/Documents/Nida_DADS/6003_ML/erw_ml_final_pj.ipynb Cell 51\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/narischanpaiboonkit/Documents/Nida_DADS/6003_ML/erw_ml_final_pj.ipynb#X63sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# Bayesian Optimization\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/narischanpaiboonkit/Documents/Nida_DADS/6003_ML/erw_ml_final_pj.ipynb#X63sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m xgb_bo \u001b[39m=\u001b[39m BayesianOptimization(xgb_evaluate, params_bounds, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, verbose \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/narischanpaiboonkit/Documents/Nida_DADS/6003_ML/erw_ml_final_pj.ipynb#X63sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m xgb_bo\u001b[39m.\u001b[39mmaximize(init_points\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, n_iter\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/narischanpaiboonkit/Documents/Nida_DADS/6003_ML/erw_ml_final_pj.ipynb#X63sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# ใช้โมเดลที่ดีที่สุดบนชุดทดสอบ\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/narischanpaiboonkit/Documents/Nida_DADS/6003_ML/erw_ml_final_pj.ipynb#X63sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m best_params \u001b[39m=\u001b[39m xgb_bo\u001b[39m.\u001b[39mmax[\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/bayes_opt/bayesian_optimization.py:310\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[0;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    308\u001b[0m     x_probe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuggest(util)\n\u001b[1;32m    309\u001b[0m     iteration \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 310\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprobe(x_probe, lazy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    312\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bounds_transformer \u001b[39mand\u001b[39;00m iteration \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    313\u001b[0m     \u001b[39m# The bounds transformer should only modify the bounds after\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39m# the init_points points (only for the true iterations)\u001b[39;00m\n\u001b[1;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_bounds(\n\u001b[1;32m    316\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bounds_transformer\u001b[39m.\u001b[39mtransform(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_space))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/bayes_opt/bayesian_optimization.py:208\u001b[0m, in \u001b[0;36mBayesianOptimization.probe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_queue\u001b[39m.\u001b[39madd(params)\n\u001b[1;32m    207\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_space\u001b[39m.\u001b[39mprobe(params)\n\u001b[1;32m    209\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch(Events\u001b[39m.\u001b[39mOPTIMIZATION_STEP)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/bayes_opt/target_space.py:236\u001b[0m, in \u001b[0;36mTargetSpace.probe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    234\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_as_array(params)\n\u001b[1;32m    235\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_keys, x))\n\u001b[0;32m--> 236\u001b[0m target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constraint \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister(x, target)\n",
      "\u001b[1;32m/Users/narischanpaiboonkit/Documents/Nida_DADS/6003_ML/erw_ml_final_pj.ipynb Cell 51\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/narischanpaiboonkit/Documents/Nida_DADS/6003_ML/erw_ml_final_pj.ipynb#X63sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mint\u001b[39m(max_depth),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/narischanpaiboonkit/Documents/Nida_DADS/6003_ML/erw_ml_final_pj.ipynb#X63sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m: learning_rate,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/narischanpaiboonkit/Documents/Nida_DADS/6003_ML/erw_ml_final_pj.ipynb#X63sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mn_estimators\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mint\u001b[39m(n_estimators),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/narischanpaiboonkit/Documents/Nida_DADS/6003_ML/erw_ml_final_pj.ipynb#X63sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mcolsample_bytree\u001b[39m\u001b[39m'\u001b[39m: colsample_bytree,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/narischanpaiboonkit/Documents/Nida_DADS/6003_ML/erw_ml_final_pj.ipynb#X63sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mbinary:logistic\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/narischanpaiboonkit/Documents/Nida_DADS/6003_ML/erw_ml_final_pj.ipynb#X63sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m xgb \u001b[39m=\u001b[39m XGBClassifier(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/narischanpaiboonkit/Documents/Nida_DADS/6003_ML/erw_ml_final_pj.ipynb#X63sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m cv_scores \u001b[39m=\u001b[39m cross_val_score(xgb, X_train, y_train, cv\u001b[39m=\u001b[39mtscv, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mroc_auc\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/narischanpaiboonkit/Documents/Nida_DADS/6003_ML/erw_ml_final_pj.ipynb#X63sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(cv_scores)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:562\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    560\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[0;32m--> 562\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[1;32m    563\u001b[0m     estimator\u001b[39m=\u001b[39mestimator,\n\u001b[1;32m    564\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[1;32m    565\u001b[0m     y\u001b[39m=\u001b[39my,\n\u001b[1;32m    566\u001b[0m     groups\u001b[39m=\u001b[39mgroups,\n\u001b[1;32m    567\u001b[0m     scoring\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m: scorer},\n\u001b[1;32m    568\u001b[0m     cv\u001b[39m=\u001b[39mcv,\n\u001b[1;32m    569\u001b[0m     n_jobs\u001b[39m=\u001b[39mn_jobs,\n\u001b[1;32m    570\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m    571\u001b[0m     fit_params\u001b[39m=\u001b[39mfit_params,\n\u001b[1;32m    572\u001b[0m     pre_dispatch\u001b[39m=\u001b[39mpre_dispatch,\n\u001b[1;32m    573\u001b[0m     error_score\u001b[39m=\u001b[39merror_score,\n\u001b[1;32m    574\u001b[0m )\n\u001b[1;32m    575\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:309\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 309\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    310\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    311\u001b[0m         clone(estimator),\n\u001b[1;32m    312\u001b[0m         X,\n\u001b[1;32m    313\u001b[0m         y,\n\u001b[1;32m    314\u001b[0m         scorers,\n\u001b[1;32m    315\u001b[0m         train,\n\u001b[1;32m    316\u001b[0m         test,\n\u001b[1;32m    317\u001b[0m         verbose,\n\u001b[1;32m    318\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    319\u001b[0m         fit_params,\n\u001b[1;32m    320\u001b[0m         return_train_score\u001b[39m=\u001b[39mreturn_train_score,\n\u001b[1;32m    321\u001b[0m         return_times\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    322\u001b[0m         return_estimator\u001b[39m=\u001b[39mreturn_estimator,\n\u001b[1;32m    323\u001b[0m         error_score\u001b[39m=\u001b[39merror_score,\n\u001b[1;32m    324\u001b[0m     )\n\u001b[1;32m    325\u001b[0m     \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m indices\n\u001b[1;32m    326\u001b[0m )\n\u001b[1;32m    328\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    330\u001b[0m \u001b[39m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mapply_async(batch, callback\u001b[39m=\u001b[39mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:732\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    730\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    731\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 732\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    734\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    735\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    736\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    729\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 730\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/sklearn.py:1519\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1491\u001b[0m (\n\u001b[1;32m   1492\u001b[0m     model,\n\u001b[1;32m   1493\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1498\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1499\u001b[0m )\n\u001b[1;32m   1500\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1501\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[1;32m   1502\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     feature_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types,\n\u001b[1;32m   1517\u001b[0m )\n\u001b[0;32m-> 1519\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1520\u001b[0m     params,\n\u001b[1;32m   1521\u001b[0m     train_dmatrix,\n\u001b[1;32m   1522\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_num_boosting_rounds(),\n\u001b[1;32m   1523\u001b[0m     evals\u001b[39m=\u001b[39mevals,\n\u001b[1;32m   1524\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39mearly_stopping_rounds,\n\u001b[1;32m   1525\u001b[0m     evals_result\u001b[39m=\u001b[39mevals_result,\n\u001b[1;32m   1526\u001b[0m     obj\u001b[39m=\u001b[39mobj,\n\u001b[1;32m   1527\u001b[0m     custom_metric\u001b[39m=\u001b[39mmetric,\n\u001b[1;32m   1528\u001b[0m     verbose_eval\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m   1529\u001b[0m     xgb_model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m   1530\u001b[0m     callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[1;32m   1531\u001b[0m )\n\u001b[1;32m   1533\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[1;32m   1534\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    729\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 730\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m bst\u001b[39m.\u001b[39mupdate(dtrain, i, obj)\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/core.py:2047\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(dtrain, DMatrix):\n\u001b[1;32m   2046\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minvalid training matrix: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(dtrain)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2047\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2049\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2050\u001b[0m     _check_call(\n\u001b[1;32m   2051\u001b[0m         _LIB\u001b[39m.\u001b[39mXGBoosterUpdateOneIter(\n\u001b[1;32m   2052\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle, ctypes\u001b[39m.\u001b[39mc_int(iteration), dtrain\u001b[39m.\u001b[39mhandle\n\u001b[1;32m   2053\u001b[0m         )\n\u001b[1;32m   2054\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/core.py:2927\u001b[0m, in \u001b[0;36mBooster._assign_dmatrix_features\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   2924\u001b[0m \u001b[39mif\u001b[39;00m data\u001b[39m.\u001b[39mnum_row() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   2925\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m-> 2927\u001b[0m fn \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mfeature_names\n\u001b[1;32m   2928\u001b[0m ft \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mfeature_types\n\u001b[1;32m   2930\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/core.py:1281\u001b[0m, in \u001b[0;36mDMatrix.feature_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1276\u001b[0m length \u001b[39m=\u001b[39m c_bst_ulong()\n\u001b[1;32m   1277\u001b[0m sarr \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mPOINTER(ctypes\u001b[39m.\u001b[39mc_char_p)()\n\u001b[1;32m   1278\u001b[0m _check_call(\n\u001b[1;32m   1279\u001b[0m     _LIB\u001b[39m.\u001b[39mXGDMatrixGetStrFeatureInfo(\n\u001b[1;32m   1280\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m-> 1281\u001b[0m         c_str(\u001b[39m\"\u001b[39m\u001b[39mfeature_name\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1282\u001b[0m         ctypes\u001b[39m.\u001b[39mbyref(length),\n\u001b[1;32m   1283\u001b[0m         ctypes\u001b[39m.\u001b[39mbyref(sarr),\n\u001b[1;32m   1284\u001b[0m     )\n\u001b[1;32m   1285\u001b[0m )\n\u001b[1;32m   1286\u001b[0m feature_names \u001b[39m=\u001b[39m from_cstr_to_pystr(sarr, length)\n\u001b[1;32m   1287\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m feature_names:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/core.py:421\u001b[0m, in \u001b[0;36mc_str\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mc_str\u001b[39m(string: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ctypes\u001b[39m.\u001b[39mc_char_p:\n\u001b[1;32m    420\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Convert a python string to cstring.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m     \u001b[39mreturn\u001b[39;00m ctypes\u001b[39m.\u001b[39mc_char_p(string\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "def xgb_bayesian_optimization(df, y_column, vb = 0):\n",
    "    df[y_column] = df[y_column].replace(-1, 0)\n",
    "    # แยกข้อมูลเป็น features และ target\n",
    "    X = df.drop(y_column, axis=1)\n",
    "    y = df[y_column].replace(-1, 0)\n",
    "\n",
    "    # แยกชุดฝึกและชุดทดสอบ\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    split_index = int(0.8 * len(df))\n",
    "    X_train = df.iloc[:split_index].drop(y_column, axis=1)\n",
    "    y_train = df.iloc[:split_index][y_column]\n",
    "    X_test = df.iloc[split_index:].drop(y_column, axis=1)\n",
    "    y_test = df.iloc[split_index:][y_column]\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    # ฟังก์ชันสำหรับ Cross-Validation และการคำนวณ ROC-AUC\n",
    "    def xgb_evaluate(max_depth, learning_rate, n_estimators, gamma, min_child_weight, subsample, colsample_bytree):\n",
    "        params = {'max_depth': int(max_depth),\n",
    "                  'learning_rate': learning_rate,\n",
    "                  'n_estimators': int(n_estimators),\n",
    "                  'gamma': gamma,\n",
    "                  'min_child_weight': min_child_weight,\n",
    "                  'subsample': subsample,\n",
    "                  'colsample_bytree': colsample_bytree,\n",
    "                  'objective': 'binary:logistic'}\n",
    "        xgb = XGBClassifier(**params)\n",
    "        cv_scores = cross_val_score(xgb, X_train, y_train, cv=tscv, scoring='roc_auc')\n",
    "        return np.mean(cv_scores)\n",
    "\n",
    "    # กำหนดช่วงของ Hyperparameters\n",
    "    params_bounds = {\n",
    "    'max_depth': (3, 150),  # ขยายเป็น 3 ถึง 150\n",
    "    'learning_rate': (0.001, 0.5),  # ขยายเป็น 0.001 ถึง 0.5\n",
    "    'n_estimators': (50, 500),  # ขยายเป็น 50 ถึง 500\n",
    "    'gamma': (0, 10),  # ขยายเป็น 0 ถึง 10\n",
    "    'min_child_weight': (0, 10),  # ขยายเป็น 0 ถึง 10\n",
    "    'subsample': (0.5, 1.0),  # ขยายช่วง subsample\n",
    "    'colsample_bytree': (0.5, 1.0)  # ขยายช่วง colsample_bytree\n",
    "    }\n",
    "\n",
    "    # Bayesian Optimization\n",
    "    xgb_bo = BayesianOptimization(xgb_evaluate, params_bounds, random_state=42, verbose = 0)\n",
    "    xgb_bo.maximize(init_points=5, n_iter=100 )\n",
    "\n",
    "    # ใช้โมเดลที่ดีที่สุดบนชุดทดสอบ\n",
    "    best_params = xgb_bo.max['params']\n",
    "    best_params['max_depth'] = int(best_params['max_depth'])\n",
    "    best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "    best_model = XGBClassifier(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # ทำ Cross-Validation และแสดงคะแนนสำหรับทุกรอบ\n",
    "    cv_scores = cross_val_score(best_model, X, y, cv=tscv, scoring='roc_auc')\n",
    "    print(\"Cross-Validation Scores: \", cv_scores)\n",
    "    print(\"Average CV Score: \", np.mean(cv_scores))\n",
    "    print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "    return best_model\n",
    "\n",
    "xgb_bayesian_optimization(df_pca_5_comp, 'y')\n",
    "xgb_bayesian_optimization(df_pca_15_comp, 'y')\n",
    "xgb_bayesian_optimization(df_pca_20_comp, 'y')\n",
    "xgb_bayesian_optimization(robust_tf_df, 'y')\n",
    "xgb_bayesian_optimization(robust_tf_rfe_df, 'y')\n",
    "xgb_bayesian_optimization(robust_tf_ftimp_mulcorr_df, 'y')\n",
    "xgb_bayesian_optimization(manual_domain_selection_df, 'y')\n",
    "print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "xgb_bayesian_optimization(stepwise_lgbm_df, 'y')\n",
    "xgb_bayesian_optimization(stepwise_gradientboosting_df, 'y')\n",
    "xgb_bayesian_optimization(stepwise_randomforest_df, 'y')\n",
    "xgb_bayesian_optimization(stepwise_logisticregression_l2_df, 'y')\n",
    "xgb_bayesian_optimization(stepwise_logisticregression_l1_df, 'y')\n",
    "xgb_bayesian_optimization(stepwise_logisticregression_elasticnet_df, 'y')\n",
    "xgb_bayesian_optimization(stepwise_gaussiannb_df, 'y')\n",
    "xgb_bayesian_optimization(stepwise_svm_df, 'y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:  [0.62386081 0.76031746 0.65388128 0.609375   0.63823738]\n",
      "Average CV Score:  0.6571343870585441\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Cross-Validation Scores:  [0.6097763  0.69285714 0.70136986 0.61458333 0.69509595]\n",
      "Average CV Score:  0.6627365185839238\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Cross-Validation Scores:  [0.65202983 0.73095238 0.62374429 0.57291667 0.56858564]\n",
      "Average CV Score:  0.6296457618167824\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Cross-Validation Scores:  [0.69676885 0.69365079 0.67031963 0.64322917 0.66879886]\n",
      "Average CV Score:  0.6745534612467589\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Cross-Validation Scores:  [0.68434134 0.68492063 0.73881279 0.55121528 0.73347548]\n",
      "Average CV Score:  0.6785531040002695\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Cross-Validation Scores:  [0.65327258 0.56746032 0.71506849 0.484375   0.56858564]\n",
      "Average CV Score:  0.5977524060919599\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Cross-Validation Scores:  [0.71416736 0.7547619  0.66484018 0.67621528 0.60767591]\n",
      "Average CV Score:  0.6835321256910263\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=28, min_samples_leaf=3, min_samples_split=5,\n",
       "                       n_estimators=491)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=28, min_samples_leaf=3, min_samples_split=5,\n",
       "                       n_estimators=491)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=28, min_samples_leaf=3, min_samples_split=5,\n",
       "                       n_estimators=491)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "def rf_bayesian_optimization(df, y_column, vb=0):\n",
    "    # แยกข้อมูลเป็น features และ target\n",
    "    df[y_column] = df[y_column].replace(-1, 0)\n",
    "\n",
    "    X = df.drop(y_column, axis=1)\n",
    "    y = df[y_column].replace(-1, 0)\n",
    "\n",
    "    # แยกชุดฝึกและชุดทดสอบ\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    split_index = int(0.8 * len(df))\n",
    "    X_train = df.iloc[:split_index].drop(y_column, axis=1)\n",
    "    y_train = df.iloc[:split_index][y_column]\n",
    "    X_test = df.iloc[split_index:].drop(y_column, axis=1)\n",
    "    y_test = df.iloc[split_index:][y_column]\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    # ฟังก์ชันสำหรับ Cross-Validation และการคำนวณ ROC-AUC สำหรับ RandomForest\n",
    "    def rf_evaluate(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n",
    "        params = {\n",
    "            'n_estimators': int(n_estimators),\n",
    "            'max_depth': int(max_depth),\n",
    "            'min_samples_split': int(min_samples_split),\n",
    "            'min_samples_leaf': int(min_samples_leaf),\n",
    "            'random_state': 42\n",
    "        }\n",
    "        rf = RandomForestClassifier(**params)\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        cv_scores = cross_val_score(rf, X_train, y_train, cv=tscv, scoring='roc_auc')\n",
    "        return np.mean(cv_scores)\n",
    "\n",
    "    # กำหนดช่วงของ Hyperparameters สำหรับ RandomForest\n",
    "    params_bounds = {\n",
    "        'n_estimators': (100, 500),\n",
    "        'max_depth': (10, 50),\n",
    "        'min_samples_split': (2, 10),\n",
    "        'min_samples_leaf': (1, 4)\n",
    "    }\n",
    "\n",
    "    # Bayesian Optimization\n",
    "    rf_bo = BayesianOptimization(rf_evaluate, params_bounds, random_state=42, verbose=0)\n",
    "    rf_bo.maximize(init_points=5, n_iter=100 )\n",
    "\n",
    "    # ใช้โมเดลที่ดีที่สุดบนชุดทดสอบ\n",
    "    best_params = rf_bo.max['params']\n",
    "    best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "    best_params['max_depth'] = int(best_params['max_depth'])\n",
    "    best_params['min_samples_split'] = int(best_params['min_samples_split'])\n",
    "    best_params['min_samples_leaf'] = int(best_params['min_samples_leaf'])\n",
    "    best_model = RandomForestClassifier(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # ทำ Cross-Validation และแสดงคะแนนสำหรับทุกรอบ\n",
    "    cv_scores = cross_val_score(best_model, X, y, cv=tscv, scoring='roc_auc')\n",
    "    print(\"Cross-Validation Scores: \", cv_scores)\n",
    "    print(\"Average CV Score: \", np.mean(cv_scores))\n",
    "    print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "    return best_model\n",
    "\n",
    "rf_bayesian_optimization(df_pca_5_comp, 'y')\n",
    "rf_bayesian_optimization(df_pca_15_comp, 'y')\n",
    "rf_bayesian_optimization(df_pca_20_comp, 'y')\n",
    "rf_bayesian_optimization(robust_tf_df, 'y')\n",
    "rf_bayesian_optimization(robust_tf_rfe_df, 'y')\n",
    "rf_bayesian_optimization(robust_tf_ftimp_mulcorr_df, 'y')\n",
    "rf_bayesian_optimization(manual_domain_selection_df, 'y')\n",
    "print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "rf_bayesian_optimization(stepwise_lgbm_df, 'y')\n",
    "rf_bayesian_optimization(stepwise_gradientboosting_df, 'y')\n",
    "rf_bayesian_optimization(stepwise_randomforest_df, 'y')\n",
    "rf_bayesian_optimization(stepwise_logisticregression_l2_df, 'y')\n",
    "rf_bayesian_optimization(stepwise_logisticregression_l1_df, 'y')\n",
    "rf_bayesian_optimization(stepwise_logisticregression_elasticnet_df, 'y')\n",
    "rf_bayesian_optimization(stepwise_gaussiannb_df, 'y')\n",
    "rf_bayesian_optimization(stepwise_svm_df, 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_5_comp , pca_model_trial = apply_pca(robust_tf_df, 'y', n_components=5)\n",
    "df_pca_10_comp , pca_model_trial = apply_pca(robust_tf_df, 'y', n_components=10)\n",
    "df_pca_15_comp , pca_model_trial = apply_pca(robust_tf_df, 'y', n_components=15)\n",
    "df_pca_20_comp , pca_model_trial = apply_pca(robust_tf_df, 'y', n_components=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:  [0.58666667 0.76248313 0.69132653 0.61538462 0.61063218]\n",
      "Average CV Score:  0.6532986254951513\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:  [0.62787879 0.76248313 0.76658163 0.67206478 0.61781609]\n",
      "Average CV Score:  0.6893648841435982\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:  [0.65212121 0.7611336  0.76658163 0.65047233 0.55890805]\n",
      "Average CV Score:  0.6778433657346025\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:  [0.67636364 0.73954116 0.77806122 0.6977058  0.55172414]\n",
      "Average CV Score:  0.6886791924694439\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:  [0.53454545 0.81106613 0.74234694 0.67746289 0.62068966]\n",
      "Average CV Score:  0.6772222126676366\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:  [0.56121212 0.74224022 0.62755102 0.61538462 0.57471264]\n",
      "Average CV Score:  0.6242201233214975\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Cross-Validation Scores:  [0.53454545 0.83535762 0.74107143 0.62618084 0.59482759]\n",
      "Average CV Score:  0.6663965861724483\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=3745.4018139335058, penalty=&#x27;l1&#x27;, solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=3745.4018139335058, penalty=&#x27;l1&#x27;, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=3745.4018139335058, penalty='l1', solver='saga')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, TimeSeriesSplit\n",
    "from bayes_opt import BayesianOptimization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def logistic_regression_bayesian_optimization_l1(df, y_column):\n",
    "    df[y_column] = df[y_column].replace(-1, 0)\n",
    "\n",
    "    # แยกข้อมูลเป็น features และ target\n",
    "    X = df.drop(y_column, axis=1)\n",
    "    y = df[y_column].replace(-1, 0)\n",
    "\n",
    "    # แยกชุดฝึกและชุดทดสอบ\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    split_index = int(0.8 * len(df))\n",
    "    X_train = df.iloc[:split_index].drop(y_column, axis=1)\n",
    "    y_train = df.iloc[:split_index][y_column]\n",
    "    X_test = df.iloc[split_index:].drop(y_column, axis=1)\n",
    "    y_test = df.iloc[split_index:][y_column]\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    def logistic_evaluate(C):\n",
    "        params = {\n",
    "            'penalty': 'l1',\n",
    "            'C': C,\n",
    "            'solver': 'saga',\n",
    "            'max_iter': 1000000,\n",
    "            'random_state': 42\n",
    "        }\n",
    "        logistic = LogisticRegression(**params)\n",
    "        cv_scores = cross_val_score(logistic, X_train, y_train, cv=tscv, scoring='roc_auc')\n",
    "        return np.mean(cv_scores)\n",
    "\n",
    "    # กำหนดช่วงของ Hyperparameters\n",
    "    params_bounds = {\n",
    "        'C': (0.001, 10000)\n",
    "    }\n",
    "\n",
    "    # Bayesian Optimization\n",
    "    logistic_bo = BayesianOptimization(logistic_evaluate, params_bounds, random_state=42, allow_duplicate_points=True, verbose=0)\n",
    "    logistic_bo.maximize(init_points=5, n_iter=100)\n",
    "\n",
    "    # ใช้โมเดลที่ดีที่สุดบนชุดทดสอบ\n",
    "    best_params = logistic_bo.max['params']\n",
    "    best_params['penalty'] = 'l1'\n",
    "    best_params['solver'] = 'saga'\n",
    "    best_model = LogisticRegression(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # ทำ Cross-Validation และแสดงคะแนนสำหรับทุกรอบ\n",
    "    cv_scores = cross_val_score(best_model, X_train, y_train, cv=tscv, scoring='roc_auc')\n",
    "    print(\"Cross-Validation Scores: \", cv_scores)\n",
    "    print(\"Average CV Score: \", np.mean(cv_scores))\n",
    "    print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "    return best_model\n",
    "\n",
    "logistic_regression_bayesian_optimization_l1(df_pca_5_comp, 'y')\n",
    "logistic_regression_bayesian_optimization_l1(df_pca_15_comp, 'y')\n",
    "logistic_regression_bayesian_optimization_l1(df_pca_20_comp, 'y')\n",
    "logistic_regression_bayesian_optimization_l1(robust_tf_df, 'y')\n",
    "logistic_regression_bayesian_optimization_l1(robust_tf_rfe_df, 'y')\n",
    "logistic_regression_bayesian_optimization_l1(robust_tf_ftimp_mulcorr_df, 'y')\n",
    "logistic_regression_bayesian_optimization_l1(manual_domain_selection_df, 'y')\n",
    "print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "logistic_regression_bayesian_optimization_l1(stepwise_lgbm_df, 'y')\n",
    "logistic_regression_bayesian_optimization_l1(stepwise_gradientboosting_df, 'y')\n",
    "logistic_regression_bayesian_optimization_l1(stepwise_randomforest_df, 'y')\n",
    "logistic_regression_bayesian_optimization_l1(stepwise_logisticregression_l2_df, 'y')\n",
    "logistic_regression_bayesian_optimization_l1(stepwise_logisticregression_l1_df, 'y')\n",
    "logistic_regression_bayesian_optimization_l1(stepwise_logisticregression_elasticnet_df, 'y')\n",
    "logistic_regression_bayesian_optimization_l1(stepwise_gaussiannb_df, 'y')\n",
    "logistic_regression_bayesian_optimization_l1(stepwise_svm_df, 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:  [0.62909091 0.78002699 0.67091837 0.64777328 0.59913793]\n",
      "Average CV Score:  0.6653894954755727\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Cross-Validation Scores:  [0.64484848 0.80026991 0.78316327 0.69230769 0.60632184]\n",
      "Average CV Score:  0.7053822374151645\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Cross-Validation Scores:  [0.65212121 0.80431849 0.7869898  0.6977058  0.58764368]\n",
      "Average CV Score:  0.7057557955396948\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Cross-Validation Scores:  [0.64       0.76518219 0.79081633 0.73144399 0.5933908 ]\n",
      "Average CV Score:  0.7041666623930041\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:  [0.54424242 0.82591093 0.75382653 0.68825911 0.62068966]\n",
      "Average CV Score:  0.6865857301025826\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Cross-Validation Scores:  [0.57090909 0.74224022 0.62755102 0.61538462 0.57471264]\n",
      "Average CV Score:  0.6261595172608914\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Cross-Validation Scores:  [0.62787879 0.82186235 0.70535714 0.63967611 0.60775862]\n",
      "Average CV Score:  0.6805066025928095\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.3124702711773379)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.3124702711773379)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.3124702711773379)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, TimeSeriesSplit\n",
    "from bayes_opt import BayesianOptimization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def logistic_regression_bayesian_optimization_l2(df, y_column):\n",
    "    df[y_column] = df[y_column].replace(-1, 0)\n",
    "\n",
    "    # แยกข้อมูลเป็น features และ target\n",
    "    X = df.drop(y_column, axis=1)\n",
    "    y = df[y_column].replace(-1, 0)\n",
    "\n",
    "    # แยกชุดฝึกและชุดทดสอบ\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    split_index = int(0.8 * len(df))\n",
    "    X_train = df.iloc[:split_index].drop(y_column, axis=1)\n",
    "    y_train = df.iloc[:split_index][y_column]\n",
    "    X_test = df.iloc[split_index:].drop(y_column, axis=1)\n",
    "    y_test = df.iloc[split_index:][y_column]\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    def logistic_evaluate(C):\n",
    "        params = {\n",
    "            'penalty': 'l2',\n",
    "            'C': C,\n",
    "            'solver': 'lbfgs',\n",
    "            'max_iter': 1000000,\n",
    "            'random_state': 42\n",
    "        }\n",
    "        logistic = LogisticRegression(**params)\n",
    "        cv_scores = cross_val_score(logistic, X_train, y_train, cv=tscv, scoring='roc_auc')\n",
    "        return np.mean(cv_scores)\n",
    "\n",
    "    # กำหนดช่วงของ Hyperparameters\n",
    "    params_bounds = {\n",
    "        'C': (0.001, 10000)\n",
    "    }\n",
    "\n",
    "    # Bayesian Optimization\n",
    "    logistic_bo = BayesianOptimization(logistic_evaluate, params_bounds, random_state=42, allow_duplicate_points=True, verbose=0)\n",
    "    logistic_bo.maximize(init_points=5, n_iter=100)\n",
    "\n",
    "    # ใช้โมเดลที่ดีที่สุดบนชุดทดสอบ\n",
    "    best_params = logistic_bo.max['params']\n",
    "    best_params['penalty'] = 'l2'\n",
    "    best_params['solver'] = 'lbfgs'\n",
    "    best_model = LogisticRegression(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # ทำ Cross-Validation และแสดงคะแนนสำหรับทุกรอบ\n",
    "    cv_scores = cross_val_score(best_model, X_train, y_train, cv=tscv, scoring='roc_auc')\n",
    "    print(\"Cross-Validation Scores: \", cv_scores)\n",
    "    print(\"Average CV Score: \", np.mean(cv_scores))\n",
    "    print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "    return best_model\n",
    "\n",
    "# ตัวอย่างการใช้งาน\n",
    "logistic_regression_bayesian_optimization_l2(df_pca_5_comp, 'y')\n",
    "logistic_regression_bayesian_optimization_l2(df_pca_15_comp, 'y')\n",
    "logistic_regression_bayesian_optimization_l2(df_pca_20_comp, 'y')\n",
    "logistic_regression_bayesian_optimization_l2(robust_tf_df, 'y')\n",
    "logistic_regression_bayesian_optimization_l2(robust_tf_rfe_df, 'y')\n",
    "logistic_regression_bayesian_optimization_l2(robust_tf_ftimp_mulcorr_df, 'y')\n",
    "logistic_regression_bayesian_optimization_l2(manual_domain_selection_df, 'y')\n",
    "print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "logistic_regression_bayesian_optimization_l2(stepwise_lgbm_df, 'y')\n",
    "logistic_regression_bayesian_optimization_l2(stepwise_gradientboosting_df, 'y')\n",
    "logistic_regression_bayesian_optimization_l2(stepwise_randomforest_df, 'y')\n",
    "logistic_regression_bayesian_optimization_l2(stepwise_logisticregression_l2_df, 'y')\n",
    "logistic_regression_bayesian_optimization_l2(stepwise_logisticregression_l1_df, 'y')\n",
    "logistic_regression_bayesian_optimization_l2(stepwise_logisticregression_elasticnet_df, 'y')\n",
    "logistic_regression_bayesian_optimization_l2(stepwise_gaussiannb_df, 'y')\n",
    "logistic_regression_bayesian_optimization_l2(stepwise_svm_df, 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:  [0.66666667 0.7611336  0.67984694 0.64102564 0.59195402]\n",
      "Average CV Score:  0.668125374539038\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:  [0.63030303 0.79352227 0.78443878 0.68960864 0.61206897]\n",
      "Average CV Score:  0.7019883351028022\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:  [0.65939394 0.76788124 0.77040816 0.65182186 0.56034483]\n",
      "Average CV Score:  0.6819700068318164\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:  [0.67515152 0.73954116 0.77806122 0.70175439 0.55028736]\n",
      "Average CV Score:  0.6889591285043709\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:  [0.5369697  0.81106613 0.74107143 0.67881242 0.62068966]\n",
      "Average CV Score:  0.6777218646447322\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:  [0.56121212 0.74224022 0.62755102 0.61538462 0.57471264]\n",
      "Average CV Score:  0.6242201233214975\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Cross-Validation Scores:  [0.53575758 0.83805668 0.74107143 0.62887989 0.59626437]\n",
      "Average CV Score:  0.6680059888689652\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/narischanpaiboonkit/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=7318.2209718323875, l1_ratio=0.9883606531880464,\n",
       "                   penalty=&#x27;elasticnet&#x27;, solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=7318.2209718323875, l1_ratio=0.9883606531880464,\n",
       "                   penalty=&#x27;elasticnet&#x27;, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=7318.2209718323875, l1_ratio=0.9883606531880464,\n",
       "                   penalty='elasticnet', solver='saga')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logistic_regression_bayesian_optimization_elasticnet(df, y_column):\n",
    "    df[y_column] = df[y_column].replace(-1, 0)\n",
    "\n",
    "    # แยกข้อมูลเป็น features และ target\n",
    "    X = df.drop(y_column, axis=1)\n",
    "    y = df[y_column].replace(-1, 0)\n",
    "\n",
    "    # แยกชุดฝึกและชุดทดสอบ\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    split_index = int(0.8 * len(df))\n",
    "    X_train = df.iloc[:split_index].drop(y_column, axis=1)\n",
    "    y_train = df.iloc[:split_index][y_column]\n",
    "    X_test = df.iloc[split_index:].drop(y_column, axis=1)\n",
    "    y_test = df.iloc[split_index:][y_column]\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    def logistic_evaluate(C, l1_ratio):\n",
    "        params = {\n",
    "            'penalty': 'elasticnet',\n",
    "            'C': C,\n",
    "            'solver': 'saga',  # Solver ที่รองรับ elasticnet\n",
    "            'l1_ratio': l1_ratio,\n",
    "            'max_iter': 1000000,\n",
    "            'random_state': 42\n",
    "        }\n",
    "        logistic = LogisticRegression(**params)\n",
    "        cv_scores = cross_val_score(logistic, X_train, y_train, cv=tscv, scoring='roc_auc')\n",
    "        return np.mean(cv_scores)\n",
    "\n",
    "    # กำหนดช่วงของ Hyperparameters\n",
    "    params_bounds = {\n",
    "        'C': (0.001, 10000),\n",
    "        'l1_ratio': (0, 1)  # ช่วงของ l1_ratio สำหรับ elasticnet\n",
    "    }\n",
    "\n",
    "    # Bayesian Optimization\n",
    "    logistic_bo = BayesianOptimization(logistic_evaluate, params_bounds, random_state=42, allow_duplicate_points=True, verbose=0)\n",
    "    logistic_bo.maximize(init_points=5, n_iter=100)\n",
    "\n",
    "    # ใช้โมเดลที่ดีที่สุดบนชุดทดสอบ\n",
    "    best_params = logistic_bo.max['params']\n",
    "    best_params['penalty'] = 'elasticnet'\n",
    "    best_params['solver'] = 'saga'\n",
    "    best_model = LogisticRegression(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # ทำ Cross-Validation และแสดงคะแนนสำหรับทุกรอบ\n",
    "    cv_scores = cross_val_score(best_model, X_train, y_train, cv=tscv, scoring='roc_auc')\n",
    "    print(\"Cross-Validation Scores: \", cv_scores)\n",
    "    print(\"Average CV Score: \", np.mean(cv_scores))\n",
    "    print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "    return best_model\n",
    "\n",
    "# ตัวอย่างการใช้งาน\n",
    "logistic_regression_bayesian_optimization_elasticnet(df_pca_5_comp, 'y')\n",
    "logistic_regression_bayesian_optimization_elasticnet(df_pca_15_comp, 'y')\n",
    "logistic_regression_bayesian_optimization_elasticnet(df_pca_20_comp, 'y')\n",
    "logistic_regression_bayesian_optimization_elasticnet(robust_tf_df, 'y')\n",
    "logistic_regression_bayesian_optimization_elasticnet(robust_tf_rfe_df, 'y')\n",
    "logistic_regression_bayesian_optimization_elasticnet(robust_tf_ftimp_mulcorr_df, 'y')\n",
    "logistic_regression_bayesian_optimization_elasticnet(manual_domain_selection_df, 'y')\n",
    "print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "logistic_regression_bayesian_optimization_elasticnet(stepwise_lgbm_df, 'y')\n",
    "logistic_regression_bayesian_optimization_elasticnet(stepwise_gradientboosting_df, 'y')\n",
    "logistic_regression_bayesian_optimization_elasticnet(stepwise_randomforest_df, 'y')\n",
    "logistic_regression_bayesian_optimization_elasticnet(stepwise_logisticregression_l2_df, 'y')\n",
    "logistic_regression_bayesian_optimization_elasticnet(stepwise_logisticregression_l1_df, 'y')\n",
    "logistic_regression_bayesian_optimization_elasticnet(stepwise_logisticregression_elasticnet_df, 'y')\n",
    "logistic_regression_bayesian_optimization_elasticnet(stepwise_gaussiannb_df, 'y')\n",
    "logistic_regression_bayesian_optimization_elasticnet(stepwise_svm_df, 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:  [0.65120133 0.79444444 0.38173516 0.64670139 0.59488273]\n",
      "Average CV Score:  0.6137930095924871\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Cross-Validation Scores:  [0.68351284 0.71269841 0.67214612 0.63368056 0.58137882]\n",
      "Average CV Score:  0.6566833497833281\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Cross-Validation Scores:  [0.71582436 0.75238095 0.65388128 0.6640625  0.62046908]\n",
      "Average CV Score:  0.681323634397519\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Cross-Validation Scores:  [0.67771334 0.70793651 0.72511416 0.64930556 0.58493248]\n",
      "Average CV Score:  0.6690004076109486\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Cross-Validation Scores:  [0.67357084 0.73809524 0.72054795 0.64930556 0.40156361]\n",
      "Average CV Score:  0.6366166372321053\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Cross-Validation Scores:  [0.66031483 0.61111111 0.63105023 0.47829861 0.56289979]\n",
      "Average CV Score:  0.5887349134941047\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Cross-Validation Scores:  [0.71665286 0.76269841 0.6456621  0.63888889 0.5920398 ]\n",
      "Average CV Score:  0.6711884122730754\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=14.019505481047146, gamma=0.12544979393743136, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=14.019505481047146, gamma=0.12544979393743136, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=14.019505481047146, gamma=0.12544979393743136, random_state=42)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from bayes_opt import BayesianOptimization\n",
    "import numpy as np\n",
    "\n",
    "def svm_bayesian_optimization(df, y_column):\n",
    "    df[y_column] = df[y_column].replace(-1, 0)\n",
    "\n",
    "    # แยกข้อมูลเป็น features และ target\n",
    "    X = df.drop(y_column, axis=1)\n",
    "    y = df[y_column].replace(-1, 0)\n",
    "\n",
    "    # แยกชุดฝึกและชุดทดสอบ\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    split_index = int(0.8 * len(df))\n",
    "    X_train = df.iloc[:split_index].drop(y_column, axis=1)\n",
    "    y_train = df.iloc[:split_index][y_column]\n",
    "    X_test = df.iloc[split_index:].drop(y_column, axis=1)\n",
    "    y_test = df.iloc[split_index:][y_column]\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    def svm_evaluate(C, gamma):\n",
    "        params = {\n",
    "            'C': C,\n",
    "            'gamma': gamma,\n",
    "            'kernel': 'rbf',  # ใช้ Radial Basis Function (RBF) kernel\n",
    "            'random_state': 42\n",
    "        }\n",
    "        svm = SVC(**params)\n",
    "        cv_scores = cross_val_score(svm, X_train, y_train, cv=tscv, scoring='roc_auc')\n",
    "        return np.mean(cv_scores)\n",
    "\n",
    "    # กำหนดช่วงของ Hyperparameters สำหรับ SVM\n",
    "    params_bounds = {\n",
    "        'C': (0.1, 100),\n",
    "        'gamma': (0.001, 1)\n",
    "    }\n",
    "\n",
    "    # Bayesian Optimization\n",
    "    svm_bo = BayesianOptimization(svm_evaluate, params_bounds, random_state=42, allow_duplicate_points=True, verbose=0)\n",
    "    svm_bo.maximize(init_points=5, n_iter=100)\n",
    "\n",
    "    # ใช้โมเดลที่ดีที่สุดบนชุดทดสอบ\n",
    "    best_params = svm_bo.max['params']\n",
    "    best_model = SVC(**best_params, kernel='rbf', random_state=42)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # ทำ Cross-Validation และแสดงคะแนนสำหรับทุกรอบ\n",
    "    cv_scores = cross_val_score(best_model, X, y, cv=tscv, scoring='roc_auc')\n",
    "    print(\"Cross-Validation Scores: \", cv_scores)\n",
    "    print(\"Average CV Score: \", np.mean(cv_scores))\n",
    "    print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "    return best_model\n",
    "\n",
    "# ใช้งานฟังก์ชัน\n",
    "svm_bayesian_optimization(df_pca_5_comp, 'y')\n",
    "svm_bayesian_optimization(df_pca_15_comp, 'y')\n",
    "svm_bayesian_optimization(df_pca_20_comp, 'y')\n",
    "svm_bayesian_optimization(robust_tf_df, 'y')\n",
    "svm_bayesian_optimization(robust_tf_rfe_df, 'y')\n",
    "svm_bayesian_optimization(robust_tf_ftimp_mulcorr_df, 'y')\n",
    "svm_bayesian_optimization(manual_domain_selection_df, 'y')\n",
    "print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "svm_bayesian_optimization(stepwise_lgbm_df, 'y')\n",
    "svm_bayesian_optimization(stepwise_gradientboosting_df, 'y')\n",
    "svm_bayesian_optimization(stepwise_randomforest_df, 'y')\n",
    "svm_bayesian_optimization(stepwise_logisticregression_l2_df, 'y')\n",
    "svm_bayesian_optimization(stepwise_logisticregression_l1_df, 'y')\n",
    "svm_bayesian_optimization(stepwise_logisticregression_elasticnet_df, 'y')\n",
    "svm_bayesian_optimization(stepwise_gaussiannb_df, 'y')\n",
    "svm_bayesian_optimization(stepwise_svm_df, 'y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:  [0.67757576 0.79757085 0.71811224 0.62753036 0.56465517]\n",
      "Average CV Score:  0.6770888778924818\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Cross-Validation Scores:  [0.58060606 0.73144399 0.67984694 0.67206478 0.5933908 ]\n",
      "Average CV Score:  0.6514705151818193\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Cross-Validation Scores:  [0.6169697  0.69905533 0.69515306 0.68690958 0.51293103]\n",
      "Average CV Score:  0.6422037409915294\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Cross-Validation Scores:  [0.70424242 0.71524966 0.68239796 0.67746289 0.57471264]\n",
      "Average CV Score:  0.6708131155423093\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Cross-Validation Scores:  [0.67636364 0.73414305 0.64285714 0.73144399 0.55172414]\n",
      "Average CV Score:  0.6673063923372453\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Cross-Validation Scores:  [0.51030303 0.7462888  0.61607143 0.7705803  0.54454023]\n",
      "Average CV Score:  0.6375567569151961\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Cross-Validation Scores:  [0.74666667 0.85425101 0.75637755 0.6437247  0.59195402]\n",
      "Average CV Score:  0.7185947898355209\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, TimeSeriesSplit\n",
    "\n",
    "def gaussian_naive_bayes(df, y_column):\n",
    "    df[y_column] = df[y_column].replace(-1, 0)\n",
    "\n",
    "    # Split data into features and target\n",
    "    X = df.drop(y_column, axis=1)\n",
    "    y = df[y_column]\n",
    "\n",
    "    # Split the dataset\n",
    "    split_index = int(0.8 * len(df))\n",
    "    X_train = df.iloc[:split_index].drop(y_column, axis=1)\n",
    "    y_train = df.iloc[:split_index][y_column]\n",
    "    X_test = df.iloc[split_index:].drop(y_column, axis=1)\n",
    "    y_test = df.iloc[split_index:][y_column]\n",
    "\n",
    "    # Initialize TimeSeriesSplit\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    # Create Gaussian Naive Bayes model and adjust var_smoothing\n",
    "    gnb = GaussianNB(var_smoothing=1e-9)\n",
    "    gnb.fit(X_train, y_train)\n",
    "\n",
    "    # Perform Cross-Validation and print scores\n",
    "    cv_scores = cross_val_score(gnb, X_train, y_train, cv=tscv, scoring='roc_auc')\n",
    "    print(\"Cross-Validation Scores: \", cv_scores)\n",
    "    print(\"Average CV Score: \", np.mean(cv_scores))\n",
    "    print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "\n",
    "    return gnb\n",
    "\n",
    "\n",
    "# ใช้งานฟังก์ชัน\n",
    "gaussian_naive_bayes(df_pca_5_comp, 'y')\n",
    "gaussian_naive_bayes(df_pca_15_comp, 'y')\n",
    "gaussian_naive_bayes(df_pca_20_comp, 'y')\n",
    "gaussian_naive_bayes(robust_tf_df, 'y')\n",
    "gaussian_naive_bayes(robust_tf_rfe_df, 'y')\n",
    "gaussian_naive_bayes(robust_tf_ftimp_mulcorr_df, 'y')\n",
    "gaussian_naive_bayes(manual_domain_selection_df, 'y')\n",
    "print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "gaussian_naive_bayes(stepwise_lgbm_df, 'y')\n",
    "gaussian_naive_bayes(stepwise_gradientboosting_df, 'y')\n",
    "gaussian_naive_bayes(stepwise_randomforest_df, 'y')\n",
    "gaussian_naive_bayes(stepwise_logisticregression_l2_df, 'y')\n",
    "gaussian_naive_bayes(stepwise_logisticregression_l1_df, 'y')\n",
    "gaussian_naive_bayes(stepwise_logisticregression_elasticnet_df, 'y')\n",
    "gaussian_naive_bayes(stepwise_gaussiannb_df, 'y')\n",
    "gaussian_naive_bayes(stepwise_svm_df, 'y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
